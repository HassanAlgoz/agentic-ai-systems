[
  {
    "objectID": "questions.html",
    "href": "questions.html",
    "title": "Questions",
    "section": "",
    "text": "Q: What drives the most value and is the most useful for developers in building real production systems? create_agent, create_deep_agent, or LangGraph?\nA: LangGraph drives the most production value‚Äîit‚Äôs the low-level foundation for reliable, scalable agent systems with native LangSmith deployment.\n\n\n\n\n\n\n\n\n\n\nTool\nPrototyping Speed\nProduction Readiness\nKey Strengths\nLimitations\n\n\n\n\ncreate_agent\n‚ö° Fastest (1 line)\n‚≠ê‚≠ê (Prototypes)\nSimple tool-calling loops\nNon-deterministic, no persistence, loops fail silently\n\n\ncreate_deep_agent\n‚≠ê‚≠ê‚≠ê (Planning + subagents)\n‚≠ê‚≠ê‚≠ê (Experimental)\nAdaptive planning, context mgmt (todos/files)\nPre-1.0 (API changes), LLM-dependent reliability, high token costs\n\n\nLangGraph\n‚≠ê‚≠ê (Build graphs)\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Enterprise)\nDeterministic flows, HITL, memory, streaming, scalable\nSteeper curve (define nodes/edges)\n\n\n\n\n\nProduction Checklist (LangGraph ‚úÖ):\ngraph = StateGraph(State)  # Typed state schema\n.add_node(\"tool\", tool_node)\n.add_conditional_edges(\"router\", precise_fn)  # Exact branching\n.set_entry_point(\"start\")\n.compile(checkpointer=MemorySaver())  # Persistence + threads\n\nReliability: Functions &gt; LLM decisions (no hallucinations/loops)\nObservability: LangSmith traces every node/edge (Klarna/Replit scale)\nDeployment: LangSmith Deployment (stateful hosting, auto-scale)\nMaturity: LTS 1.0, semver, used by Elastic/Klarna\nFeatures: Streaming, memory (short/long-term), HITL interrupts\n\nReal-World Evidence: - LangSmith Deployment docs: ‚Äúpurpose-built for stateful, long-running agents‚Äù - Companies: Klarna (agents@scale), Replit (code agents), Elastic (search pipelines)\n\n\n\n\ncreate_agent: MVP/prototyping (95% simple cases)\nagent = create_agent(model, tools)  # 5min hello-world\ncreate_deep_agent: Research/experimental (open-ended tasks)\ndeep = create_deep_agent(model, tools=[search])  # Adaptive multi-step\nPre-1.0 (release policy warns API flux), great for dynamic planning.\n\n\n\n\nPrototyping ‚Üí Production\ncreate_agent ‚îÄ‚îÄ create_deep_agent ‚îÄ‚îÄ LangGraph (+ LangSmith deploy)\n    ‚îÇ                ‚îÇ                    ‚îÇ\nSimple loops    Adaptive agents    Reliable pipelines\nHigh variance   Expensive runs     Predictable + scalable\nValue Equation: Reliability √ó Scale √ó Observability = Production Success\nLangGraph maximizes all three. Start with create_agent ‚Üí migrate to LangGraph for prod.\nRelevant docs: - LangGraph Overview (Production) - LangSmith Deployment (LangGraph) - Release Policy (DeepAgents pre-1.0) - Agents (create_agent)"
  },
  {
    "objectID": "questions.html#why-langgraph-wins-production",
    "href": "questions.html#why-langgraph-wins-production",
    "title": "Questions",
    "section": "",
    "text": "Production Checklist (LangGraph ‚úÖ):\ngraph = StateGraph(State)  # Typed state schema\n.add_node(\"tool\", tool_node)\n.add_conditional_edges(\"router\", precise_fn)  # Exact branching\n.set_entry_point(\"start\")\n.compile(checkpointer=MemorySaver())  # Persistence + threads\n\nReliability: Functions &gt; LLM decisions (no hallucinations/loops)\nObservability: LangSmith traces every node/edge (Klarna/Replit scale)\nDeployment: LangSmith Deployment (stateful hosting, auto-scale)\nMaturity: LTS 1.0, semver, used by Elastic/Klarna\nFeatures: Streaming, memory (short/long-term), HITL interrupts\n\nReal-World Evidence: - LangSmith Deployment docs: ‚Äúpurpose-built for stateful, long-running agents‚Äù - Companies: Klarna (agents@scale), Replit (code agents), Elastic (search pipelines)"
  },
  {
    "objectID": "questions.html#higher-level-tools-role",
    "href": "questions.html#higher-level-tools-role",
    "title": "Questions",
    "section": "",
    "text": "create_agent: MVP/prototyping (95% simple cases)\nagent = create_agent(model, tools)  # 5min hello-world\ncreate_deep_agent: Research/experimental (open-ended tasks)\ndeep = create_deep_agent(model, tools=[search])  # Adaptive multi-step\nPre-1.0 (release policy warns API flux), great for dynamic planning."
  },
  {
    "objectID": "questions.html#production-maturity-spectrum",
    "href": "questions.html#production-maturity-spectrum",
    "title": "Questions",
    "section": "",
    "text": "Prototyping ‚Üí Production\ncreate_agent ‚îÄ‚îÄ create_deep_agent ‚îÄ‚îÄ LangGraph (+ LangSmith deploy)\n    ‚îÇ                ‚îÇ                    ‚îÇ\nSimple loops    Adaptive agents    Reliable pipelines\nHigh variance   Expensive runs     Predictable + scalable\nValue Equation: Reliability √ó Scale √ó Observability = Production Success\nLangGraph maximizes all three. Start with create_agent ‚Üí migrate to LangGraph for prod.\nRelevant docs: - LangGraph Overview (Production) - LangSmith Deployment (LangGraph) - Release Policy (DeepAgents pre-1.0) - Agents (create_agent)"
  },
  {
    "objectID": "L01/06_components.html",
    "href": "L01/06_components.html",
    "title": "Component architecture",
    "section": "",
    "text": "Source.\nLangChain‚Äôs power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components."
  },
  {
    "objectID": "L01/06_components.html#core-component-ecosystem",
    "href": "L01/06_components.html#core-component-ecosystem",
    "title": "Component architecture",
    "section": "Core component ecosystem",
    "text": "Core component ecosystem\nThe diagram below shows how LangChain‚Äôs major components connect to form complete AI applications:\n\n\n\n\n\ngraph TD\n    %% Input processing\n    subgraph \"üì• Input processing\"\n        A[Text input] --&gt; B[Document loaders]\n        B --&gt; C[Text splitters]\n        C --&gt; D[Documents]\n    end\n\n    %% Embedding & storage\n    subgraph \"üî¢ Embedding & storage\"\n        D --&gt; E[Embedding models]\n        E --&gt; F[Vectors]\n        F --&gt; G[(Vector stores)]\n    end\n\n    %% Retrieval\n    subgraph \"üîç Retrieval\"\n        H[User Query] --&gt; I[Embedding models]\n        I --&gt; J[Query vector]\n        J --&gt; K[Retrievers]\n        K --&gt; G\n        G --&gt; L[Relevant context]\n    end\n\n    %% Generation\n    subgraph \"ü§ñ Generation\"\n        M[Chat models] --&gt; N[Tools]\n        N --&gt; O[Tool results]\n        O --&gt; M\n        L --&gt; M\n        M --&gt; P[AI response]\n    end\n\n    %% Orchestration\n    subgraph \"üéØ Orchestration\"\n        Q[Agents] --&gt; M\n        Q --&gt; N\n        Q --&gt; K\n        Q --&gt; R[Memory]\n    end\n\n\n\n\n\n\n\nHow components connect\nEach component layer builds on the previous ones:\n\nInput processing ‚Äì Transform raw data into structured documents\nEmbedding & storage ‚Äì Convert text into searchable vector representations\nRetrieval ‚Äì Find relevant information based on user queries\nGeneration ‚Äì Use AI models to create responses, optionally with tools\nOrchestration ‚Äì Coordinate everything through agents and memory systems"
  },
  {
    "objectID": "L01/06_components.html#component-categories",
    "href": "L01/06_components.html#component-categories",
    "title": "Component architecture",
    "section": "Component categories",
    "text": "Component categories\nLangChain organizes components into these main categories:\n\n\n\nCategory\nPurpose\nKey Components\nUse Cases\n\n\n\n\nModels\nAI reasoning and generation\nChat models, LLMs, Embedding models\nText generation, reasoning, semantic understanding\n\n\nTools\nExternal capabilities\nAPIs, databases, etc.\nWeb search, data access, computations\n\n\nAgents\nOrchestration and reasoning\nReAct agents, tool calling agents\nNondeterministic workflows, decision making\n\n\nMemory\nContext preservation\nMessage history, custom state\nConversations, stateful interactions\n\n\nRetrievers\nInformation access\nVector retrievers, web retrievers\nRAG, knowledge base search\n\n\nDocument processing\nData ingestion\nLoaders, splitters, transformers\nPDF processing, web scraping\n\n\nVector Stores\nSemantic search\nChroma, Pinecone, FAISS\nSimilarity search, embeddings storage"
  },
  {
    "objectID": "L01/06_components.html#common-patterns",
    "href": "L01/06_components.html#common-patterns",
    "title": "Component architecture",
    "section": "Common patterns",
    "text": "Common patterns\n\nRAG (Retrieval-Augmented generation)\n\n\n\n\n\ngraph LR\n    A[User question] --&gt; B[Retriever]\n    B --&gt; C[Relevant docs]\n    C --&gt; D[Chat model]\n    A --&gt; D\n    D --&gt; E[Informed response]\n\n\n\n\n\n\n\n\nAgent with tools\n\n\n\n\n\ngraph LR\n    A[User request] --&gt; B[Agent]\n    B --&gt; C{Need tool?}\n    C --&gt;|Yes| D[Call tool]\n    D --&gt; E[Tool result]\n    E --&gt; B\n    C --&gt;|No| F[Final answer]\n\n\n\n\n\n\n\n\nMulti-agent system\n\n\n\n\n\ngraph LR\n    A[Complex Task] --&gt; B[Supervisor agent]\n    B --&gt; C[Specialist agent 1]\n    B --&gt; D[Specialist agent 2]\n    C --&gt; E[Results]\n    D --&gt; E\n    E --&gt; B\n    B --&gt; F[Coordinated response]"
  },
  {
    "objectID": "L01/04_semantic-search.html",
    "href": "L01/04_semantic-search.html",
    "title": "Build a semantic search engine with LangChain",
    "section": "",
    "text": "This tutorial will familiarize you with LangChain‚Äôs document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data‚Äì from (vector) databases and other sources ‚Äì for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG.\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.\n\n\nThis guide focuses on retrieval of text data. We will cover the following concepts:\n\nDocuments and document loaders;\nText splitters;\nEmbeddings;\nVector stores and retrievers."
  },
  {
    "objectID": "L01/04_semantic-search.html#overview",
    "href": "L01/04_semantic-search.html#overview",
    "title": "Build a semantic search engine with LangChain",
    "section": "",
    "text": "This tutorial will familiarize you with LangChain‚Äôs document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data‚Äì from (vector) databases and other sources ‚Äì for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG.\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.\n\n\nThis guide focuses on retrieval of text data. We will cover the following concepts:\n\nDocuments and document loaders;\nText splitters;\nEmbeddings;\nVector stores and retrievers."
  },
  {
    "objectID": "L01/04_semantic-search.html#setup",
    "href": "L01/04_semantic-search.html#setup",
    "title": "Build a semantic search engine with LangChain",
    "section": "Setup",
    "text": "Setup\n\nInstallation\nThis tutorial requires the langchain-community and pypdf packages. Using uv:\n!uv add langchain-community pypdf\nFor more details, see our Installation guide."
  },
  {
    "objectID": "L01/04_semantic-search.html#documents-and-document-loaders",
    "href": "L01/04_semantic-search.html#documents-and-document-loaders",
    "title": "Build a semantic search engine with LangChain",
    "section": "1. Documents and document loaders",
    "text": "1. Documents and document loaders\nLangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n\npage_content: a string representing the content;\nmetadata: a dict containing arbitrary metadata;\nid: (optional) a string identifier for the document.\n\nThe metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\nWe can generate sample documents when desired:\n\nfrom langchain_core.documents import Document\n\ndocuments = [\n    Document(\n        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n        metadata={\"source\": \"mammal-pets-doc\"},\n    ),\n    Document(\n        page_content=\"Cats are independent pets that often enjoy their own space.\",\n        metadata={\"source\": \"mammal-pets-doc\"},\n    ),\n]\n\nHowever, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application.\n\nLoading documents\nLet‚Äôs load a PDF into a sequence of Document objects. First, fetch a PDF from arXiv (e.g.¬†the ‚ÄúAttention Is All You Need‚Äù paper) using curl:\ncurl -L -o paper.pdf \"https://arxiv.org/pdf/1706.03762.pdf\"\nThen load it with the PyPDFLoader:\n\nfrom langchain_community.document_loaders import PyPDFLoader\n\nfile_path = \"paper.pdf\"\nloader = PyPDFLoader(file_path)\n\ndocs = loader.load()\n\nprint(len(docs))\n\n15\n\n\nPyPDFLoader loads one Document object per PDF page. For each, we can easily access:\n\nThe string content of the page;\nMetadata containing the file name and page number.\n\n\nprint(f\"{docs[0].page_content[:200]}\\n\")\nprint(docs[0].metadata)\n\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\n\n\n{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n\n\n\n\nSplitting\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not ‚Äúwashed out‚Äù by surrounding text.\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nWe set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute ‚Äústart_index‚Äù.\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=200, add_start_index=True\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(len(all_splits))\n\n52"
  },
  {
    "objectID": "L01/04_semantic-search.html#embeddings",
    "href": "L01/04_semantic-search.html#embeddings",
    "title": "Build a semantic search engine with LangChain",
    "section": "2. Embeddings",
    "text": "2. Embeddings\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\nLangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. We use HuggingFace with the all-mpnet-base-v2 sentence transformer:\nuv add langchain-huggingface sentence-transformers\n\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nvector_1 = embeddings.embed_query(all_splits[0].page_content)\nvector_2 = embeddings.embed_query(all_splits[1].page_content)\n\nassert len(vector_1) == len(vector_2)\nprint(f\"Generated vectors of length {len(vector_1)}\\n\")\nprint(vector_1[:10])\n\nGenerated vectors of length 768\n\n[0.00369695364497602, 0.017323777079582214, -0.01369203720241785, 0.00036987385828979313, -0.05261596664786339, -0.0010653170756995678, 0.002715452341362834, -0.02163930982351303, -0.06304091215133667, -0.0036578048020601273]\n\n\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search."
  },
  {
    "objectID": "L01/04_semantic-search.html#vector-stores",
    "href": "L01/04_semantic-search.html#vector-stores",
    "title": "Build a semantic search engine with LangChain",
    "section": "3. Vector stores",
    "text": "3. Vector stores\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\nLangChain includes a suite of integrations with different vector store technologies. For this tutorial we use the in-memory vector store, which is lightweight and requires no extra infrastructure:\n\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nvector_store = InMemoryVectorStore(embeddings)\n\nHaving instantiated our vector store, we can now index the documents.\n\nids = vector_store.add_documents(documents=all_splits)\n\nNote that most vector store implementations will allow you to connect to an existing vector store‚Äì e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail.\nOnce we‚Äôve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\n\nSynchronously and asynchronously;\nBy string query and by vector;\nWith and without returning similarity scores;\nBy similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results).\n\nThe methods will generally include a list of Document objects in their outputs.\nUsage\nEmbeddings typically represent text as a ‚Äúdense‚Äù vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\nReturn documents based on similarity to a string query:\n\nresults = vector_store.similarity_search(\n    \"What is the main architecture proposed in the paper?\"\n)\n\nprint(results[0])\n\npage_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 770}\n\n\nAsync query:\n\nresults = await vector_store.asimilarity_search(\"What is self-attention?\")\n\nprint(results[0])\n\npage_content='3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 1611}\n\n\nReturn scores:\n\n# Note that providers implement different scores; the score here\n# is a distance metric that varies inversely with similarity.\n\nresults = vector_store.similarity_search_with_score(\"How does the Transformer encoder work?\")\ndoc, score = results[0]\nprint(f\"Score: {score}\\n\")\nprint(doc)\n\nScore: 0.668251152922073\n\npage_content='Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 0}\n\n\nReturn documents based on similarity to an embedded query:\n\nembedding = embeddings.embed_query(\"What are the advantages of the Transformer over RNNs?\")\n\nresults = vector_store.similarity_search_by_vector(embedding)\nprint(results[0])\n\npage_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop œµls\ntrain PPL BLEU params\nsteps (dev) (dev) √ó106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'start_index': 0}\n\n\nLearn more:\n\nAPI Reference\nIntegration-specific docs"
  },
  {
    "objectID": "L01/04_semantic-search.html#retrievers",
    "href": "L01/04_semantic-search.html#retrievers",
    "title": "Build a semantic search engine with LangChain",
    "section": "4. Retrievers",
    "text": "4. Retrievers\nLangChain VectorStore objects do not subclass Runnable. LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\nWe can create a simple version of this ourselves, without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method:\n\nfrom typing import List\n\nfrom langchain_core.documents import Document\nfrom langchain_core.runnables import chain\n\n\n@chain\ndef retriever(query: str) -&gt; List[Document]:\n    return vector_store.similarity_search(query, k=1)\n\n\nretriever.batch(\n    [\n        \"What is the main architecture proposed in the paper?\",\n        \"What is self-attention?\",\n    ],\n)\n\n[[Document(id='3a9a01f8-683c-471c-9dda-ba075d260afd', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 770}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,')],\n [Document(id='417ae00c-a9d5-49ee-b5f3-2a8ecb5cd8ed', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 1611}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3')]]\n\n\nVectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\n\nretriever = vector_store.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 1},\n)\n\nretriever.batch(\n    [\n        \"What is the main architecture proposed in the paper?\",\n        \"What is self-attention?\",\n    ],\n)\n\n[[Document(id='3a9a01f8-683c-471c-9dda-ba075d260afd', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 770}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,')],\n [Document(id='417ae00c-a9d5-49ee-b5f3-2a8ecb5cd8ed', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 1611}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3')]]\n\n\nVectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and \"similarity_score_threshold\". We can use the latter to threshold documents output by the retriever by similarity score.\nRetrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial."
  },
  {
    "objectID": "L01/04_semantic-search.html#next-steps",
    "href": "L01/04_semantic-search.html#next-steps",
    "title": "Build a semantic search engine with LangChain",
    "section": "Next steps",
    "text": "Next steps\nYou‚Äôve now seen how to build a semantic search engine over a PDF document.\nFor more on document loaders:\n\nOverview\nAvailable integrations\n\nFor more on embeddings:\n\nOverview\nAvailable integrations\n\nFor more on vector stores:\n\nOverview\nAvailable integrations\n\nFor more on RAG, see:\n\nBuild a Retrieval Augmented Generation (RAG) App"
  },
  {
    "objectID": "L01/04_semantic-search.html#activity",
    "href": "L01/04_semantic-search.html#activity",
    "title": "Build a semantic search engine with LangChain",
    "section": "Activity",
    "text": "Activity\nOver to you: Apply the Load, Embed, Store, and Retrieve, on another document."
  },
  {
    "objectID": "L01/subagents_hil.html",
    "href": "L01/subagents_hil.html",
    "title": "6. Add human-in-the-loop review",
    "section": "",
    "text": "It can be prudent to incorporate human-in-the-loop review of sensitive actions. LangChain includes built-in middleware to review tool calls, in this case the tools invoked by sub-agents.\nLet‚Äôs add human-in-the-loop review to both sub-agents:\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\ncalendar_agent = create_agent(\n    model,\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\"create_calendar_event\": True},\n            description_prefix=\"Calendar event pending approval\",\n        ),\n    ],\n)\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\"send_email\": True},\n            description_prefix=\"Outbound email pending approval\",\n        ),\n    ],\n)\nsupervisor_agent = create_agent(\n    model,\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n    checkpointer=InMemorySaver(),\n)\nLet‚Äôs repeat the query. Note that we gather interrupt events into a list to access downstream:\nquery = (\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n    \"and send them an email reminder about reviewing the new mockups.\"\n)\ninput_ = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n\n# thread_id is needed to later resume the interrupted conversation\nconfig = {\"configurable\": {\"thread_id\": \"6\"}}\n\ninterrupts = []\nfor step in supervisor_agent.stream(input_, config):\n    for update in step.values():\n        if isinstance(update, dict):\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n        else:\n            interrupt_ = update[0]\n            interrupts.append(interrupt_)\n            print(f\"\\nINTERRUPTED: {interrupt_.id}\")\n\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  schedule_event (call_JsP0Ij54jT1X0TcxYafwRcMa)\n\n Call ID: call_JsP0Ij54jT1X0TcxYafwRcMa\n\n  Args:\n\n    request: Schedule a meeting with the design team next Tuesday at 2pm for 1 hour.\n\n\n\nINTERRUPTED: c5cc54c21b9258043abf580bec6fe356\nThis time we‚Äôve interrupted execution. Let‚Äôs inspect the interrupt events:\nfor interrupt_ in interrupts:\n    for request in interrupt_.value[\"action_requests\"]:\n        print(f\"INTERRUPTED: {interrupt_.id}\")\n        print(f\"{request['description']}\\n\")\n\nINTERRUPTED: c5cc54c21b9258043abf580bec6fe356\nCalendar event pending approval\n\nTool: create_calendar_event\nArgs: {'title': 'Design Team Meeting', 'start_time': '2026-02-24T14:00:00', 'end_time': '2026-02-24T15:00:00', 'attendees': ['design-team@example.com'], 'location': ''}\nWe can specify decisions for each interrupt by referring to its ID using a Command. Refer to the human-in-the-loop guide for additional details. For demonstration purposes, here we will:\nas follows:\nfrom langgraph.types import Command\n\nresume = {}\nfor interrupt_ in interrupts:\n    if interrupt_.id == \"c5cc54c21b9258043abf580bec6fe356\":\n        # Edit email\n        edited_action = interrupt_.value[\"action_requests\"][0].copy()\n        edited_action[\"args\"][\"subject\"] = \"Mockups reminder\"\n        resume[interrupt_.id] = {\n            \"decisions\": [{\"type\": \"edit\", \"edited_action\": edited_action}]\n        }\n    else:\n        resume[interrupt_.id] = {\"decisions\": [{\"type\": \"approve\"}]}\n\ninterrupts = []\nfor step in supervisor_agent.stream(\n    Command(resume=resume),\n    config,\n):\n    for update in step.values():\n        if isinstance(update, dict):\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n        else:\n            interrupt_ = update[0]\n            interrupts.append(interrupt_)\n            print(f\"\\nINTERRUPTED: {interrupt_.id}\")\n\n\n================================= Tool Message =================================\n\nName: schedule_event\n\n\n\nAll set. Here‚Äôs what I scheduled:\n\n\n\n- Event: Design Team Meeting\n\n- Date: Tuesday, February 24, 2026\n\n- Time: 2:00 PM ‚Äì 3:00 PM\n\n- Attendees: design-team@example.com\n\n- Location: not specified\n\n\n\nWould you like me to add a location, a video conferencing link, or an agenda?\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  manage_email (call_gLCOpI4HNDDyLPtRWPnUzlBX)\n\n Call ID: call_gLCOpI4HNDDyLPtRWPnUzlBX\n\n  Args:\n\n    request: Send a reminder email to the design team to review the new mockups before the meeting on Tuesday at 2 PM.\n\n================================= Tool Message =================================\n\nName: manage_email\n\n\n\nI‚Äôm ready to send this, but I need two details:\n\n1) The recipient email addresses (or confirm using your Design Team distribution list, e.g., design-team@yourdomain.com).\n\n2) The exact date for ‚ÄúTuesday‚Äù (the date, if you‚Äôd like me to include it in the body).\n\n\n\nDraft email ready to send (you can approve or adjust):\n\n\n\nSubject: Reminder: Review new mockups before Tuesday at 2:00 PM meeting\n\n\n\nBody:\n\nHello Design Team,\n\n\n\nThis is a quick reminder to review the new mockups before the meeting on Tuesday at 2:00 PM. Please share any feedback or questions in advance so we can address them during the discussion.\n\n\n\nThe latest mockups are available here: [location/link to mockups]\n\n\n\nThank you,\n\n[Your Name]\n\n[Your Role/Team]\n\n[Contact information]\n\n\n\nPlease provide:\n\n- The recipient email(s) to send to (or confirm the distribution list).\n\n- The exact date for Tuesday (so I can include the date in the email if you‚Äôd like).\n\n- Any link or location to include for the mockups.\n\n- Your name or signature details for the closing.\n\n\n\nOnce you confirm, I‚Äôll send the email and report back with the sent details.\n\n================================== Ai Message ==================================\n\n\n\nI‚Äôve got the meeting scheduled for Tuesday, February 24, 2026, 2:00‚Äì3:00 PM with the design team (design-team@example.com). Location isn‚Äôt set yet.\n\n\n\nFor the reminder email, I can send it as soon as I have a couple of details. Here‚Äôs a ready-to-send draft with the date included. You can approve or adjust, and I‚Äôll send it right away.\n\n\n\nProposed email draft\n\n- Subject: Reminder: Review new mockups before Tue Feb 24, 2026 at 2:00 PM\n\n- Body:\n\n  Hello Design Team,\n\n  \n\n  This is a quick reminder to review the new mockups before the meeting at 2:00 PM on Tuesday, February 24, 2026. Please share any feedback or questions in advance so we can address them during the discussion.\n\n  \n\n  The latest mockups are available here: [location/link to mockups]\n\n  \n\n  Thank you,\n\n  [Your Name]\n\n  [Your Role/Team]\n\n  [Contact information]\n\n\n\nWhat I need from you to send this:\n\n- Recipient(s): Confirm using the design-team distribution list (design-team@example.com) or provide specific addresses.\n\n- Mockups link/location: Provide the URL or file path to include.\n\n- Your signature: Name, role, and contact info to put at the bottom (e.g., ‚ÄúAlex Smith, Product Designer, alex@example.com‚Äù).\n\n- Timezone: Is 2:00 PM in your local timezone? Should I specify a timezone in the body?\n\n\n\nWould you like me to proceed with the default recipients (design-team@example.com) and use the date shown above, once you provide the mockups link and signature? If you have different recipients or a preferred wording, tell me and I‚Äôll tailor it.\nThe run proceeds with our input."
  },
  {
    "objectID": "L01/subagents_hil.html#advanced-control-information-flow",
    "href": "L01/subagents_hil.html#advanced-control-information-flow",
    "title": "6. Add human-in-the-loop review",
    "section": "7. Advanced: Control information flow",
    "text": "7. Advanced: Control information flow\nBy default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.\n\n\n\nAdvanced: Control information flow\n\n\n\nPass additional conversational context to sub-agents\n\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef schedule_event(\n    request: str,\n    runtime: ToolRuntime # Runtime context automatically injected into tools.\n) -&gt; str:\n    \"\"\"Schedule calendar events using natural language.\"\"\"\n    # Customize context received by sub-agent\n    original_user_message = next(\n        message for message in runtime.state[\"messages\"]\n        if message.type == \"human\"\n    )\n    prompt = (\n        \"You are assisting with the following user inquiry:\\n\\n\"\n        f\"{original_user_message.text}\\n\\n\"\n        \"You are tasked with the following sub-request:\\n\\n\"\n        f\"{request}\"\n    )\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n    })\n    return result[\"messages\"][-1].text\n\nThis allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like ‚Äúschedule it for the same time tomorrow‚Äù (referencing a previous conversation).\n\n\n\n\n\n\nTip\n\n\n\nYou can see the full context received by the sub agent in the chat model call of the LangSmith trace.\n\n\n\n\nControl what supervisor receives\nYou can also customize what information flows back to the supervisor:\n\nimport json\n\n@tool\ndef schedule_event(request: str) -&gt; str:\n    \"\"\"Schedule calendar events using natural language.\"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n\n    # Option 1: Return just the confirmation message\n    return result[\"messages\"][-1].text\n\n    # Option 2: Return structured data\n    # return json.dumps({\n    #     \"status\": \"success\",\n    #     \"event_id\": \"evt_123\",\n    #     \"summary\": result[\"messages\"][-1].text\n    # })\n\nImportant: Make sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but don‚Äôt include the results in their final response."
  },
  {
    "objectID": "L01/02_subagents.html",
    "href": "L01/02_subagents.html",
    "title": "Build a personal assistant with subagents",
    "section": "",
    "text": "The supervisor pattern is a multi-agent architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.\nIn this tutorial, you‚Äôll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:\n\nA calendar agent that handles scheduling, availability checking, and event management.\nAn email agent that manages communication, drafts messages, and sends notifications.\n\n\n\nMulti-agent architectures allow you to partition tools across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).\n\n\n\n\n\n\nArchitecture\n\n\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.\nThis separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.\n\n\n\nWe will need to select a chat model from LangChain‚Äôs suite of integrations:\n\n\nüëâ Read the OpenAI chat model integration docs\nuv add \"langchain[openai]\"\n\nimport os\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# We use OpenRouter for the agent ‚Äî set OPENROUTER_API_KEY in .env\n# Get your key at https://openrouter.ai/keys\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    raise RuntimeError(\n        \"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:\\n\"\n        \"OPENROUTER_API_KEY=your-openrouter-api-key\"\n    )\n\n\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")"
  },
  {
    "objectID": "L01/02_subagents.html#overview",
    "href": "L01/02_subagents.html#overview",
    "title": "Build a personal assistant with subagents",
    "section": "",
    "text": "The supervisor pattern is a multi-agent architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.\nIn this tutorial, you‚Äôll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:\n\nA calendar agent that handles scheduling, availability checking, and event management.\nAn email agent that manages communication, drafts messages, and sends notifications.\n\n\n\nMulti-agent architectures allow you to partition tools across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).\n\n\n\n\n\n\nArchitecture\n\n\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.\nThis separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.\n\n\n\nWe will need to select a chat model from LangChain‚Äôs suite of integrations:\n\n\nüëâ Read the OpenAI chat model integration docs\nuv add \"langchain[openai]\"\n\nimport os\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# We use OpenRouter for the agent ‚Äî set OPENROUTER_API_KEY in .env\n# Get your key at https://openrouter.ai/keys\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    raise RuntimeError(\n        \"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:\\n\"\n        \"OPENROUTER_API_KEY=your-openrouter-api-key\"\n    )\n\n\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")"
  },
  {
    "objectID": "L01/02_subagents.html#define-tools",
    "href": "L01/02_subagents.html#define-tools",
    "title": "Build a personal assistant with subagents",
    "section": "1. Define tools",
    "text": "1. Define tools\n\nStart by defining the tools that require structured inputs.\nIn real applications, these would call actual APIs (Google Calendar, SendGrid, etc.).\nFor this tutorial, you‚Äôll use stubs to demonstrate the pattern.\n\n\nfrom langchain.tools import tool\n\n\n@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,       # ISO format: \"2024-01-15T14:00:00\"\n    end_time: str,         # ISO format: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # email addresses\n    location: str = \"\"\n) -&gt; str:\n    \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\n    # Stub: In practice, this would call Google Calendar API, Outlook API, etc.\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\n\n\n@tool\ndef send_email(\n    to: list[str],  # email addresses\n    subject: str,\n    body: str,\n    cc: list[str] = []\n) -&gt; str:\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\n    # Stub: In practice, this would call SendGrid, Gmail API, etc.\n    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO format: \"2024-01-15\"\n    duration_minutes: int\n) -&gt; list[str]:\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\n    # Stub: In practice, this would query calendar APIs\n    return [\"09:00\", \"14:00\", \"16:00\"]"
  },
  {
    "objectID": "L01/02_subagents.html#create-specialized-sub-agents",
    "href": "L01/02_subagents.html#create-specialized-sub-agents",
    "title": "Build a personal assistant with subagents",
    "section": "2. Create specialized sub-agents",
    "text": "2. Create specialized sub-agents\nNext, we‚Äôll create specialized sub-agents that handle each domain.\n\nCreate a calendar agent\n\nThe calendar agent understands natural language scheduling requests and translates them into precise API calls.\nIt handles date parsing, availability checking, and event creation.\n\n\n\nfrom langchain.agents import create_agent\n\nCALENDAR_AGENT_PROMPT = (\n    \"You are a calendar scheduling assistant. \"\n    \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \"\n    \"into proper ISO datetime formats. \"\n    \"Use get_available_time_slots to check availability when needed. \"\n    \"Use create_calendar_event to schedule events. \"\n    \"Always confirm what was scheduled in your final response.\"\n)\n\ncalendar_agent = create_agent(\n    model,\n    tools=[\n        create_calendar_event,\n        get_available_time_slots\n    ],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n)\n\nTest the calendar agent to see how it handles natural language scheduling:\n\nquery = \"Schedule a team meeting next Tuesday at 2pm for 1 hour\"\ninput_ = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n\nfor step in calendar_agent.stream(input_):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  create_calendar_event (call_VPij7Wlqm5MTct1VYogczdmM)\n\n Call ID: call_VPij7Wlqm5MTct1VYogczdmM\n\n  Args:\n\n    title: Team meeting\n\n    start_time: 2026-02-24T14:00:00\n\n    end_time: 2026-02-24T15:00:00\n\n    attendees: []\n\n    location:\n\n================================= Tool Message =================================\n\nName: create_calendar_event\n\n\n\nEvent created: Team meeting from 2026-02-24T14:00:00 to 2026-02-24T15:00:00 with 0 attendees\n\n================================== Ai Message ==================================\n\n\n\nAll set. I‚Äôve scheduled the Team meeting for Tuesday, February 24, 2026, from 2:00 PM to 3:00 PM (local time). Attendees: none. Location: none.\n\n\n\nWould you like me to add attendees and/or a location (or conferencing details) and send invites?\n\n\n\n\nThe agent parses ‚Äúnext Tuesday at 2pm‚Äù into ISO format (‚Äú2024-01-16T14:00:00‚Äù), calculates the end time, calls create_calendar_event, and returns a natural language confirmation.\n\n\nCreate an email agent\n\nThe email agent handles message composition and sending.\nIt focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication.\n\n\nEMAIL_AGENT_PROMPT = (\n    \"You are an email assistant. \"\n    \"Compose professional emails based on natural language requests. \"\n    \"Extract recipient information and craft appropriate subject lines and body text. \"\n    \"Use send_email to send the message. \"\n    \"Always confirm what was sent in your final response.\"\n)\n\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n)\n\nTest the email agent with a natural language request:\n\nquery = \"Send the design team a reminder about reviewing the new mockups\"\ninput_ = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n\nfor step in email_agent.stream(input_):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\n\n================================== Ai Message ==================================\n\n\n\nHere‚Äôs a ready-to-send draft. I‚Äôve assumed a Design Team distribution list. If you want a different recipient, let me know.\n\n\n\nRecipients: design-team@yourcompany.com\n\nSubject: Reminder: Please review the new mockups\n\nBody:\n\nHi Design Team,\n\n\n\nThis is a friendly reminder to review the new mockups for the [Project/Feature]. Your feedback is important to ensure we stay aligned with requirements and timelines.\n\n\n\nPlease share your comments by [Due Date] or let me know if you need more time. You can access the mockups here: [Link to mockups].\n\n\n\nIf you‚Äôve already provided feedback, thank you and please disregard this message.\n\n\n\nBest regards,\n\n[Your Name]\n\n[Your Title]\n\n[Department]\n\n\n\nWould you like me to send this now as-is, or would you like me to replace the placeholders with the actual project name, due date, link, and your signature? If you confirm, I‚Äôll send it.\n\n\n\n\n\nThe agent infers the recipient from the informal request, crafts a professional subject line and body, calls send_email, and returns a confirmation.\nEach sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task."
  },
  {
    "objectID": "L01/02_subagents.html#wrap-sub-agents-as-tools",
    "href": "L01/02_subagents.html#wrap-sub-agents-as-tools",
    "title": "Build a personal assistant with subagents",
    "section": "3. Wrap sub-agents as tools",
    "text": "3. Wrap sub-agents as tools\n\nNow wrap each sub-agent as a tool that the supervisor can invoke.\nThis is the key architectural step that creates the layered system.\nThe supervisor will see high-level tools like \"schedule_event\", not low-level tools like \"create_calendar_event\".\n\n\n@tool\ndef schedule_event(request: str) -&gt; str:\n    \"\"\"Schedule calendar events using natural language.\n\n    Use this when the user wants to create, modify, or check calendar appointments.\n    Handles date/time parsing, availability checking, and event creation.\n\n    Input: Natural language scheduling request (e.g., 'meeting with design team\n    next Tuesday at 2pm')\n    \"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\n\n@tool\ndef manage_email(request: str) -&gt; str:\n    \"\"\"Send emails using natural language.\n\n    Use this when the user wants to send notifications, reminders, or any email\n    communication. Handles recipient extraction, subject generation, and email\n    composition.\n\n    Input: Natural language email request (e.g., 'send them a reminder about\n    the meeting')\n    \"\"\"\n    result = email_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\n\nThe tool descriptions (docstring) help the supervisor decide when to use each tool, so make them clear and specific.\nWe return only the sub-agent‚Äôs final response, as the supervisor doesn‚Äôt need to see intermediate reasoning or tool calls."
  },
  {
    "objectID": "L01/02_subagents.html#create-the-supervisor-agent",
    "href": "L01/02_subagents.html#create-the-supervisor-agent",
    "title": "Build a personal assistant with subagents",
    "section": "4. Create the supervisor agent",
    "text": "4. Create the supervisor agent\n\nNow create the supervisor that orchestrates the sub-agents.\nThe supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level.\n\n\nSUPERVISOR_PROMPT = (\n    \"You are a helpful personal assistant. \"\n    \"You can schedule calendar events and send emails. \"\n    \"Break down user requests into appropriate tool calls and coordinate the results. \"\n    \"When a request involves multiple actions, use multiple tools in sequence.\"\n)\n\nsupervisor_agent = create_agent(\n    model,\n    tools=[\n        schedule_event,\n        manage_email\n    ],\n    system_prompt=SUPERVISOR_PROMPT,\n)"
  },
  {
    "objectID": "L01/02_subagents.html#use-the-supervisor",
    "href": "L01/02_subagents.html#use-the-supervisor",
    "title": "Build a personal assistant with subagents",
    "section": "5. Use the supervisor",
    "text": "5. Use the supervisor\nNow test your complete system with complex requests that require coordination across multiple domains:\n\nExample 1: Simple single-domain request\n\nquery = \"Schedule a team standup for tomorrow at 9am\"\ninput_ = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n\nfor step in supervisor_agent.stream(input_):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  schedule_event (call_gtoo8MGcOP1cOyGS8hm1rkRM)\n\n Call ID: call_gtoo8MGcOP1cOyGS8hm1rkRM\n\n  Args:\n\n    request: Schedule a team standup tomorrow at 9am\n\n================================= Tool Message =================================\n\nName: schedule_event\n\n\n\nI can schedule that. A couple of details to confirm:\n\n\n\n- Date: Tomorrow is 2026-02-19. Is that correct?\n\n- Duration: What duration should I use? (default 15 minutes)\n\n- Attendees: Who should attend the standup? (e.g., list of names, or simply \"Team\")\n\n- Location/ conferencing: Should this be virtual (and which platform) or in-person? If virtual, do you want me to create a meeting link?\n\n- Timezone: Should I use your calendar‚Äôs default timezone, or specify a different one?\n\n\n\nIf you‚Äôd like, I can proceed with defaults (9:00‚Äì9:15 local time, attendees: Team, virtual meeting link to be determined) once you confirm.\n\n================================== Ai Message ==================================\n\n\n\nI can schedule that. I can proceed with defaults unless you want changes.\n\n\n\nProposed defaults:\n\n- Date/time: Tomorrow, 2026-02-19, 9:00‚Äì9:15 local time\n\n- Attendees: Team\n\n- Location: Virtual meeting link (link will be created)\n\n- Timezone: Your calendar‚Äôs default\n\n\n\nPlease confirm or tell me the adjustments (e.g., different duration, specific attendees, in-person vs virtual, or a preferred conferencing platform). If you‚Äôre okay with the defaults, I‚Äôll schedule it right away.\n\n\n\n\nThe supervisor identifies this as a calendar task, calls schedule_event, and the calendar agent handles date parsing and event creation.\n\n\n\n\n\n\nTip\n\n\n\nFor full transparency into the information flow, including prompts and responses for each chat model call, check out the LangSmith trace for the above run.\n\n\n\n\nExample 2: Complex multi-domain request\n\nquery = (\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n    \"and send them an email reminder about reviewing the new mockups.\"\n)\ninput_ = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n\nfor step in supervisor_agent.stream(input_):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\n\nThe supervisor recognizes this requires both calendar and email actions, calls schedule_event for the meeting, then calls manage_email for the reminder.\nEach sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.\n\n\n\n\n\n\n\nTip\n\n\n\nRefer to the LangSmith trace to see the detailed information flow for the above run, including individual chat model prompts and responses."
  },
  {
    "objectID": "L01/02_subagents.html#key-takeaways",
    "href": "L01/02_subagents.html#key-takeaways",
    "title": "Build a personal assistant with subagents",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nThe supervisor pattern creates layers of abstraction where each layer has a clear responsibility.\nWhen designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts.\nWrite clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.\n\nUse the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents don‚Äôt need to converse directly with users.\nFor simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use handoffs instead. For peer-to-peer collaboration between agents, consider other multi-agent patterns."
  },
  {
    "objectID": "L01/02_subagents.html#activity",
    "href": "L01/02_subagents.html#activity",
    "title": "Build a personal assistant with subagents",
    "section": "Activity",
    "text": "Activity\nOver to you: recreate the supervisor pattern on a different problem domain (other than calendar & emails). You may stub API calls or, better yet, use actual ones!"
  },
  {
    "objectID": "L01/checkpoint.html",
    "href": "L01/checkpoint.html",
    "title": "Short-term memory",
    "section": "",
    "text": "Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\nShort term memory lets your application remember previous interactions within a single thread or conversation.\n\n\n\n\n\n\nNote\n\n\n\nA thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\n\n\nConversation history is the most common form of short-term memory. Long conversations pose a challenge to today‚Äôs LLMs; a full history may not fit inside an LLM‚Äôs context window, resulting in an context loss or errors.\nEven if your model supports the full context length, most LLMs still perform poorly over long contexts. They get ‚Äúdistracted‚Äù by stale or off-topic content, all while suffering from slower response times and higher costs.\nChat models accept context using messages, which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or ‚Äúforget‚Äù stale information."
  },
  {
    "objectID": "L01/checkpoint.html#overview",
    "href": "L01/checkpoint.html#overview",
    "title": "Short-term memory",
    "section": "",
    "text": "Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\nShort term memory lets your application remember previous interactions within a single thread or conversation.\n\n\n\n\n\n\nNote\n\n\n\nA thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\n\n\nConversation history is the most common form of short-term memory. Long conversations pose a challenge to today‚Äôs LLMs; a full history may not fit inside an LLM‚Äôs context window, resulting in an context loss or errors.\nEven if your model supports the full context length, most LLMs still perform poorly over long contexts. They get ‚Äúdistracted‚Äù by stale or off-topic content, all while suffering from slower response times and higher costs.\nChat models accept context using messages, which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or ‚Äúforget‚Äù stale information."
  },
  {
    "objectID": "L01/checkpoint.html#usage",
    "href": "L01/checkpoint.html#usage",
    "title": "Short-term memory",
    "section": "Usage",
    "text": "Usage\nTo add short-term memory (thread-level persistence) to an agent, you need to specify a checkpointer when creating an agent.\n\n\n\n\n\n\nNote\n\n\n\nLangChain‚Äôs agent manages short-term memory as a part of your agent‚Äôs state.\nBy storing these in the graph‚Äôs state, the agent can access the full context for a given conversation while maintaining separation between different threads.\nState is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.\nShort-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.\n\n\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nagent = create_agent(\n    \"gpt-5-nano\",\n    tools=[get_user_info],\n    checkpointer=InMemorySaver(),\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n    {\"configurable\": {\"thread_id\": \"1\"}},\n)\n\ncheckpointer=InMemorySaver(),\n{\"configurable\": {\"thread_id\": \"1\"}},\n\n\nIn production\nIn production, use a checkpointer backed by a database:\nuv add langgraph-checkpoint-postgres\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    checkpointer.setup() # auto create tables in PostgresSql\n    agent = create_agent(\n        \"gpt-5\",\n        tools=[get_user_info],\n        checkpointer=checkpointer,\n    )\n\n\n\n\n\n\nNote\n\n\n\nFor more checkpointer options including SQLite, Postgres, and Azure Cosmos DB, see the list of checkpointer libraries in the Persistence documentation."
  },
  {
    "objectID": "L01/checkpoint.html#customizing-agent-memory",
    "href": "L01/checkpoint.html#customizing-agent-memory",
    "title": "Short-term memory",
    "section": "Customizing agent memory",
    "text": "Customizing agent memory\nBy default, agents use AgentState to manage short term memory, specifically the conversation history via a messages key.\nYou can extend AgentState to add additional fields. Custom state schemas are passed to create_agent using the state_schema parameter.\n\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass CustomAgentState(AgentState):\n    user_id: str\n    preferences: dict\n\nagent = create_agent(\n    \"gpt-5\",\n    tools=[get_user_info],\n    state_schema=CustomAgentState,  # [!code highlight]\n    checkpointer=InMemorySaver(),\n)\n\n# Custom state can be passed in invoke\nresult = agent.invoke(\n    {\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n        \"user_id\": \"user_123\",\n        \"preferences\": {\"theme\": \"dark\"}\n    },\n    {\"configurable\": {\"thread_id\": \"1\"}})"
  },
  {
    "objectID": "L01/checkpoint.html#common-patterns",
    "href": "L01/checkpoint.html#common-patterns",
    "title": "Short-term memory",
    "section": "Common patterns",
    "text": "Common patterns\nWith short-term memory enabled, long conversations can exceed the LLM‚Äôs context window. Common solutions are:\n\nTrim messages: Remove first or last N messages (before calling LLM)\nDelete messages: Remove messages from LangGraph state permanently\nSummarize messages: Summarize earlier messages in the history and replace them with a summary\nCustom strategies: Custom strategies (e.g., message filtering, etc.)\n\nThis allows the agent to keep track of the conversation without exceeding the LLM‚Äôs context window.\nCheckout: Common patterns for more details."
  },
  {
    "objectID": "L01/checkpoint.html#access-memory",
    "href": "L01/checkpoint.html#access-memory",
    "title": "Short-term memory",
    "section": "Access memory",
    "text": "Access memory\nYou can access and modify the short-term memory (state) of an agent in several ways:\n\nTools\n\nRead short-term memory in a tool\nAccess short term memory (state) in a tool using the runtime parameter (typed as ToolRuntime).\nThe runtime parameter is hidden from the tool signature (so the model doesn‚Äôt see it), but the tool can access the state through it.\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.tools import tool, ToolRuntime\n\n\nclass CustomState(AgentState):\n    user_id: str\n\n@tool\ndef get_user_info(\n    runtime: ToolRuntime\n) -&gt; str:\n    \"\"\"Look up user info.\"\"\"\n    user_id = runtime.state[\"user_id\"]\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_user_info],\n    state_schema=CustomState,\n)\n\nresult = agent.invoke({\n    \"messages\": \"look up user information\",\n    \"user_id\": \"user_123\"\n})\nprint(result[\"messages\"][-1].content)\n# &gt; User is John Smith.\n\n\nWrite short-term memory from tools\nTo modify the agent‚Äôs short-term memory (state) during execution, you can return state updates directly from the tools.\nThis is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.messages import ToolMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.types import Command\nfrom pydantic import BaseModel\n\n\nclass CustomState(AgentState):  # [!code highlight]\n    user_name: str\n\nclass CustomContext(BaseModel):\n    user_id: str\n\n@tool\ndef update_user_info(\n    runtime: ToolRuntime[CustomContext, CustomState],\n) -&gt; Command:\n    \"\"\"Look up and update user info.\"\"\"\n    user_id = runtime.context.user_id\n    name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n    return Command(update={  # [!code highlight]\n        \"user_name\": name,\n        # update the message history\n        \"messages\": [\n            ToolMessage(\n                \"Successfully looked up user information\",\n                tool_call_id=runtime.tool_call_id\n            )\n        ]\n    })\n\n@tool\ndef greet(\n    runtime: ToolRuntime[CustomContext, CustomState]\n) -&gt; str | Command:\n    \"\"\"Use this to greet the user once you found their info.\"\"\"\n    user_name = runtime.state.get(\"user_name\", None)\n    if user_name is None:\n       return Command(update={\n            \"messages\": [\n                ToolMessage(\n                    \"Please call the 'update_user_info' tool it will get and update the user's name.\",\n                    tool_call_id=runtime.tool_call_id\n                )\n            ]\n        })\n    return f\"Hello {user_name}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[update_user_info, greet],\n    state_schema=CustomState, # [!code highlight]\n    context_schema=CustomContext,\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]},\n    context=CustomContext(user_id=\"user_123\"),\n)"
  },
  {
    "objectID": "L01/00_intro_comparison.html",
    "href": "L01/00_intro_comparison.html",
    "title": "Introduction: What is Agentic AI?",
    "section": "",
    "text": "Artificial Intelligence is a field of Computer Science, studying how to automate decision making.\nAgentic AI is where autonomy of the system is at the level of dealing not just with structured tabular data, but with unstructured data such as natural language, speech, visual to inform their decision, in a growingly less supervised manner; and hence, autonomous.\nSpecifically, today‚Äôs Agentic AI systems are driven by Large Language Models (LLMs)."
  },
  {
    "objectID": "L01/00_intro_comparison.html#case-studies",
    "href": "L01/00_intro_comparison.html#case-studies",
    "title": "Introduction: What is Agentic AI?",
    "section": "Case Studies",
    "text": "Case Studies\nHomework: read 3 Case Studies and make a presentation about one."
  },
  {
    "objectID": "L01/00_intro_comparison.html#agent-framework",
    "href": "L01/00_intro_comparison.html#agent-framework",
    "title": "Introduction: What is Agentic AI?",
    "section": "Agent Framework",
    "text": "Agent Framework\nIn programming, frameworks are a way of thinking about a logical system. In two-folds:\nFirslty, Abstractions: which are the conceptual objects that are defined with specific properties and rules of interaction with other objects. Langchain has:\n\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\n\nSecondly, Integrations: which are ‚Äúexternal‚Äù objects that interact with our internal framework of objects. Langchain has:\n\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\n\nExamples of Agentic frameworks are:\n\nLangChain\nVercel‚Äôs AI SDK\nCrewAI\nOpenAI Agents SDK\nGoogle ADK\nLlamaIndex"
  },
  {
    "objectID": "L01/00_intro_comparison.html#agent-runtime",
    "href": "L01/00_intro_comparison.html#agent-runtime",
    "title": "Introduction: What is Agentic AI?",
    "section": "Agent Runtime",
    "text": "Agent Runtime\nRuntimes manage state, and state transitions (orchestration). In other words, building, managing, and deploying long-running, stateful agents. Concretely, things like:\n\nControl-flow: Step by step instructions, conditional execution, and loops.\nPersistence: Thread-level and cross-thread persistence for state management.\nDurable execution: Agents persist through failures and can run for extended periods, resuming from where they left off.\nStreaming: Support for streaming workflows and responses.\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state.\n\nExamples:\n\nLangGraph\nTemporal\nInngest\n\nNote: for the best DX (Developer Experience), we will be using the Functional API."
  },
  {
    "objectID": "L01/00_intro_comparison.html#agent-platform",
    "href": "L01/00_intro_comparison.html#agent-platform",
    "title": "Introduction: What is Agentic AI?",
    "section": "Agent Platform",
    "text": "Agent Platform\nLangSmith:\n\nDeployment: localhost -&gt; production server\nObservability: tracing, real-time monitoring, alerting and usage.\nEvaluation: testing versions and providing feedback on traces."
  },
  {
    "objectID": "L01/00_intro_comparison.html#key-takeways",
    "href": "L01/00_intro_comparison.html#key-takeways",
    "title": "Introduction: What is Agentic AI?",
    "section": "Key Takeways",
    "text": "Key Takeways\n\nLangChain is the framework.\nLangGraph is the runtime.\nLangSmith is the platform.\n\n\n\nRead More.\nHelp: Chat with langchain docs"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent.html",
    "href": "L04_Advanced/Voice_Agent.html",
    "title": "Build a voice agent with LangChain",
    "section": "",
    "text": "Chat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.\nVoice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.\n\n\nVoice agents are agents that can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.\nThey‚Äôre suited for a variety of use cases, including:\n\nCustomer support\nPersonal assistants\nHands-free interfaces\nCoaching and training\n\n\n\n\nAt a high level, every voice agent needs to handle three tasks:\n\nListen - capture audio and transcribe it\nThink - interpret intent, reason, plan\nSpeak - generate audio and stream it back to the user\n\nThe difference lies in how these steps are sequenced and coupled. In practice, production agents follow one of two main architectures:\n\n\nThe Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).\n\n\n\n\n\n\n\n\n\n\n\nPros:\n\nFull control over each component (swap STT/TTS providers as needed)\nAccess to latest capabilities from modern text-modality models\nTransparent behavior with clear boundaries between components\n\nCons:\n\nRequires orchestrating multiple services\nAdditional complexity in managing the pipeline\nConversion from speech to text loses information (e.g., tone, emotion)\n\n\n\n\nSpeech-to-speech uses a multimodal model that processes audio input and generates audio output natively.\n\n\n\n\n\n\n\n\n\n\n\nPros:\n\nSimpler architecture with fewer moving parts\nTypically lower latency for simple interactions\nDirect audio processing captures tone and other nuances of speech\n\nCons:\n\nLimited model options, greater risk of provider lock-in\nFeatures may lag behind text-modality models\nLess transparency in how audio is processed\nReduced controllability and customization options\n\nThis guide demonstrates the sandwich architecture to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.\n\n\n\n\nWe‚Äôll walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using AssemblyAI for STT and Cartesia for TTS (although adapters can be built for most providers).\nAn end-to-end reference application is available in the voice-sandwich-demo repository. We will walk through that application here.\nThe demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe demo implements a streaming pipeline where each stage processes data asynchronously:\nClient (Browser)\n\nCaptures microphone audio and encodes it as PCM\nEstablishes WebSocket connection to the backend server\nStreams audio chunks to the server in real-time\nReceives and plays back synthesized speech audio\n\nServer (Python)\n\nAccepts WebSocket connections from clients\nOrchestrates the three-step pipeline:\n\nSpeech-to-text (STT): Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events\nAgent: Processes transcripts with LangChain agent, streams response tokens\nText-to-speech (TTS): Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks\n\nReturns synthesized audio to the client for playback\n\nThe pipeline uses async generators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\nEach stage processes events independently and concurrently:\n\naudio transcription begins as soon as audio arrives,\nthe agent starts reasoning as soon as a transcript is available,\nand speech synthesis begins as soon as agent text is generated.\n\nThis architecture can achieve sub-700ms latency to support natural conversation."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent.html#overview",
    "href": "L04_Advanced/Voice_Agent.html#overview",
    "title": "Build a voice agent with LangChain",
    "section": "",
    "text": "Chat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.\nVoice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.\n\n\nVoice agents are agents that can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.\nThey‚Äôre suited for a variety of use cases, including:\n\nCustomer support\nPersonal assistants\nHands-free interfaces\nCoaching and training\n\n\n\n\nAt a high level, every voice agent needs to handle three tasks:\n\nListen - capture audio and transcribe it\nThink - interpret intent, reason, plan\nSpeak - generate audio and stream it back to the user\n\nThe difference lies in how these steps are sequenced and coupled. In practice, production agents follow one of two main architectures:\n\n\nThe Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).\n\n\n\n\n\n\n\n\n\n\n\nPros:\n\nFull control over each component (swap STT/TTS providers as needed)\nAccess to latest capabilities from modern text-modality models\nTransparent behavior with clear boundaries between components\n\nCons:\n\nRequires orchestrating multiple services\nAdditional complexity in managing the pipeline\nConversion from speech to text loses information (e.g., tone, emotion)\n\n\n\n\nSpeech-to-speech uses a multimodal model that processes audio input and generates audio output natively.\n\n\n\n\n\n\n\n\n\n\n\nPros:\n\nSimpler architecture with fewer moving parts\nTypically lower latency for simple interactions\nDirect audio processing captures tone and other nuances of speech\n\nCons:\n\nLimited model options, greater risk of provider lock-in\nFeatures may lag behind text-modality models\nLess transparency in how audio is processed\nReduced controllability and customization options\n\nThis guide demonstrates the sandwich architecture to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.\n\n\n\n\nWe‚Äôll walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using AssemblyAI for STT and Cartesia for TTS (although adapters can be built for most providers).\nAn end-to-end reference application is available in the voice-sandwich-demo repository. We will walk through that application here.\nThe demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe demo implements a streaming pipeline where each stage processes data asynchronously:\nClient (Browser)\n\nCaptures microphone audio and encodes it as PCM\nEstablishes WebSocket connection to the backend server\nStreams audio chunks to the server in real-time\nReceives and plays back synthesized speech audio\n\nServer (Python)\n\nAccepts WebSocket connections from clients\nOrchestrates the three-step pipeline:\n\nSpeech-to-text (STT): Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events\nAgent: Processes transcripts with LangChain agent, streams response tokens\nText-to-speech (TTS): Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks\n\nReturns synthesized audio to the client for playback\n\nThe pipeline uses async generators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\nEach stage processes events independently and concurrently:\n\naudio transcription begins as soon as audio arrives,\nthe agent starts reasoning as soon as a transcript is available,\nand speech synthesis begins as soon as agent text is generated.\n\nThis architecture can achieve sub-700ms latency to support natural conversation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Index",
    "section": "",
    "text": "Lesson\nDescription\nMaterials\n\n\n\n\n0\nIntroduction: What is Agentic AI? Agents‚Äô Use Cases, Frameworks, Runtimes, and Platforms.\n[lesson]\n\n\n1\nAgentsModel, Tools, System Prompt, Search and Structured Output.\n[notebook]\n\n\n2\nSub Agents Build a personal assistant with agents as tools.\n[notebook]\n\n\n3\nLLM AugmentationsHow Agents Work? - breakdown the agent loop that enables LLMs to ‚Äúcall‚Äù tools.\n[notebook]\n\n\n4\nSemantic SearchLoad, Embed, Store, Retrieve text based on meaning.\n[notebook]\n\n\n5\nRetrieval Augmented Generation (RAG)Answer questions about specific source of information.\n[notebook]\n\n\n6\nComponents Agentic systems components and common patterns.\n[lesson]\n\n\n7\nWorkflows\n[notebook]\n\n\n8\nWorkflow and Agent Patterns\n[notebook]\n\n\n9\nLangSmith\n[notebook]\n\n\n10\nProject\n[lesson]\n\n\n\n\nHelp: Chat with docs"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#overview",
    "href": "L04_Advanced/Voice_Agent_slides.html#overview",
    "title": "Build a voice agent with LangChain",
    "section": "Overview",
    "text": "Overview\nChat interfaces have dominated how we interact with AI. Multimodal AI and text-to-speech (TTS) now make it possible to build agents that feel like conversational partners.\nVoice agents: interact by spoken words instead of keyboard and mouse ‚Äî more natural and engaging."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#what-are-voice-agents",
    "href": "L04_Advanced/Voice_Agent_slides.html#what-are-voice-agents",
    "title": "Build a voice agent with LangChain",
    "section": "What are voice agents?",
    "text": "What are voice agents?\nAgents that engage in natural spoken conversations: speech recognition + NLP + generative AI + TTS.\nUse cases:\n\nCustomer support\nPersonal assistants\nHands-free interfaces\nCoaching and training"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#how-do-voice-agents-work",
    "href": "L04_Advanced/Voice_Agent_slides.html#how-do-voice-agents-work",
    "title": "Build a voice agent with LangChain",
    "section": "How do voice agents work?",
    "text": "How do voice agents work?\nEvery voice agent handles three tasks:\n\nListen ‚Äî capture audio and transcribe it\nThink ‚Äî interpret intent, reason, plan\nSpeak ‚Äî generate audio and stream it back\n\nDifference: how these steps are sequenced and coupled. Two main architectures:"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#stt-agent-tts-sandwich",
    "href": "L04_Advanced/Voice_Agent_slides.html#stt-agent-tts-sandwich",
    "title": "Build a voice agent with LangChain",
    "section": "1. STT ‚Üí Agent ‚Üí TTS (‚ÄúSandwich‚Äù)",
    "text": "1. STT ‚Üí Agent ‚Üí TTS (‚ÄúSandwich‚Äù)\nThree components: STT ‚Üí LangChain Agent ‚Üí TTS.\n\n\n\n\n\n\n\n\n\n\n\nPros: Full control, swap providers, latest text models, transparent.\nCons: More orchestration, pipeline complexity, speech‚Üítext loses tone/emotion."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#speech-to-speech-s2s",
    "href": "L04_Advanced/Voice_Agent_slides.html#speech-to-speech-s2s",
    "title": "Build a voice agent with LangChain",
    "section": "2. Speech-to-Speech (S2S)",
    "text": "2. Speech-to-Speech (S2S)\nOne multimodal model: audio in ‚Üí audio out.\n\n\n\n\n\n\n\n\n\n\n\nPros: Simpler, lower latency, preserves tone.\nCons: Fewer models, lock-in risk, less transparency and control."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#why-the-sandwich-in-this-guide",
    "href": "L04_Advanced/Voice_Agent_slides.html#why-the-sandwich-in-this-guide",
    "title": "Build a voice agent with LangChain",
    "section": "Why the sandwich in this guide?",
    "text": "Why the sandwich in this guide?\nSandwich architecture balances performance, controllability, and access to modern models.\n\nSub-700ms latency with some STT/TTS providers\nModular components you can swap and control"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#demo-application-overview",
    "href": "L04_Advanced/Voice_Agent_slides.html#demo-application-overview",
    "title": "Build a voice agent with LangChain",
    "section": "Demo application overview",
    "text": "Demo application overview\n\nVoice agent with sandwich architecture: orders for a sandwich shop\nAssemblyAI for STT, Cartesia for TTS (adapters possible for most providers)\nReference app: voice-sandwich-demo\nWebSockets for real-time browser ‚ÜîÔ∏é server; adaptable to Twilio, Vonage, WebRTC"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#architecture",
    "href": "L04_Advanced/Voice_Agent_slides.html#architecture",
    "title": "Build a voice agent with LangChain",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#pipeline-client-server",
    "href": "L04_Advanced/Voice_Agent_slides.html#pipeline-client-server",
    "title": "Build a voice agent with LangChain",
    "section": "Pipeline: client & server",
    "text": "Pipeline: client & server\nClient (browser): Mic ‚Üí PCM ‚Üí WebSocket ‚Üí server; receives and plays synthesized audio.\nServer (Python): WebSocket in ‚Üí STT (e.g.¬†AssemblyAI) ‚Üí Agent (LangChain) ‚Üí TTS (e.g.¬†Cartesia) ‚Üí WebSocket out.\nStreaming at each stage with async generators: downstream can start before upstream finishes ‚Üí lower latency."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#pipeline-concurrency",
    "href": "L04_Advanced/Voice_Agent_slides.html#pipeline-concurrency",
    "title": "Build a voice agent with LangChain",
    "section": "Pipeline: concurrency",
    "text": "Pipeline: concurrency\nStages run independently and concurrently:\n\nTranscription starts as soon as audio arrives\nAgent reasons as soon as a transcript is available\nSpeech synthesis starts as soon as agent text is generated\n\n‚Üí Sub-700ms latency for natural conversation."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#summary",
    "href": "L04_Advanced/Voice_Agent_slides.html#summary",
    "title": "Build a voice agent with LangChain",
    "section": "Summary",
    "text": "Summary\n\nVoice agents: Listen ‚Üí Think ‚Üí Speak; natural spoken interaction with AI\nSandwich architecture: STT ‚Üí LangChain Agent ‚Üí TTS\nDemo: voice-sandwich-demo ‚Äî WebSockets, AssemblyAI STT, Cartesia TTS, streaming pipeline"
  },
  {
    "objectID": "L01/01_agents.html",
    "href": "L01/01_agents.html",
    "title": "Agents in LangChain",
    "section": "",
    "text": "This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes."
  },
  {
    "objectID": "L01/01_agents.html#questions",
    "href": "L01/01_agents.html#questions",
    "title": "Agents in LangChain",
    "section": "Questions",
    "text": "Questions\n\nWhat is an Agent in LangChain and how to make one?\nWhat‚Äôs the relationship between an LLM and an Agent?\nWhat can agents do?\nCan I run an agent locally without a provider?"
  },
  {
    "objectID": "L01/01_agents.html#setup-virtual-environment",
    "href": "L01/01_agents.html#setup-virtual-environment",
    "title": "Agents in LangChain",
    "section": "Setup Virtual Environment",
    "text": "Setup Virtual Environment\nuv init\nuv venv -p 3.12\n\nActivate the virtual environment\n\nWindowsMacOS / Linux\n\n\n.venv\\Scripts\\activate.bat\n\n\nsource .venv/bin/activate"
  },
  {
    "objectID": "L01/01_agents.html#what-is-an-api-key",
    "href": "L01/01_agents.html#what-is-an-api-key",
    "title": "Agents in LangChain",
    "section": "What is an API Key?",
    "text": "What is an API Key?\nThink of an API Key as a hotel key card.\n\nThe Hotel (Server): Has resources (rooms) but keeps them locked.\nThe Guest (Client): Wants access.\nThe Key Card (API Key): Identifies you and proves you are allowed to enter specific rooms.\n\n\n\nWhat & Why\nAn API key is a unique string of characters used to identify the calling program.\n\nIdentification: Keys ‚Äúauthenticate the calling project,‚Äù allowing the server to recognize who is asking for data.\nControl: This lets the server track usage for billing and enforce limits (quotas) so one user doesn‚Äôt crash the system.\n\n\n\n\nSecurity Risks\nIf you lose your key, it is like dropping your credit card.\n\nTheft: Attackers can use your key to make requests on your behalf.\nConsequences: You suffer financial loss (paying for their usage) or service denial (they use up your available quota).\n\n\nRule: Never post keys on public sites like GitHub.\n\n\n\nHow to Set Your API Key?\nThis project uses OpenRouter (The Unified Interface For LLMs), via LiteLLM to access the DeepSeek model, which requires an API key. If you don‚Äôt already have an OpenRouter API key, you can create one for free at: OpenRouter.\nWrite your API key into an .env file as an environment variable, as follows:\nOPENROUTER_API_KEY=...\n\nNote: make sure to add it to .gitignore to avoid committing it to the repository.\nNote: this is different than the .venv file used for the virtual environment.\n\nIf we use the OpenAI API, we‚Äôll have to add:\nOPENAI_API_BASE=\"https://openrouter.ai/api/v1\"\n.. such that the model uses OpenRouter instead of the default OpenAI API.\n\n\nSign up and Set LangSmith API (Free)\n\nCost:\nSign up for LangSmith here, find out more about LangSmith and how to use it within your workflow here.\nSet LANGSMITH_API_KEY, LANGSMITH_TRACING_V2=\"true\" LANGSMITH_PROJECT=\"langchain-academy\"in your environment\nIf you are on the EU instance also set LANGSMITH_ENDPOINT=‚Äúhttps://eu.api.smith.langchain.com‚Äù as well.\n\n\n\nSet up Tavily API for web search (Free)\n\nTavily Search API is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results.\nYou can sign up for an API key here. It‚Äôs easy to sign up and offers a very generous free tier. Some lessons (in Module 4) will use Tavily.\nSet TAVILY_API_KEY in your environment.\n\n\n\nInstall dependencies\nuv add langchain tavily-python langchain_openai langchain_community \n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# We use OpenRouter for the agent ‚Äî set OPENROUTER_API_KEY in .env\n# Get your key at https://openrouter.ai/keys\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    raise RuntimeError(\n        \"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:\\n\"\n        \"OPENROUTER_API_KEY=your-openrouter-api-key\"\n    )"
  },
  {
    "objectID": "L01/01_agents.html#basic-usage",
    "href": "L01/01_agents.html#basic-usage",
    "title": "Agents in LangChain",
    "section": "Basic usage",
    "text": "Basic usage\nModels can be utilized in two ways:\n\nWith agents - Models can be dynamically specified when creating an agent.\nStandalone - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.\n\nHere is a useful how-to for all the things that you can do with chat models, but we‚Äôll show a few highlights below.\nThere are a few standard parameters that we can set with chat models. Two of the most common are:\n\nmodel: the name of the model\ntemperature: the sampling temperature\nmax_tokens: the maximum number of tokens to generate\n\nTemperature controls the randomness or creativity of the model‚Äôs output where:\n\nLow temperature (close to 0) is more deterministic and focused outputs. This is good for tasks requiring accuracy or factual responses.\nHigh temperature (close to 1) is good for creative tasks or generating varied responses.\n\nmax_tokens limits the total number of tokens in the response, effectively controlling how long the output can be.\nLangChain supports many models via third-party integrations. By default, the course will use ChatOpenAI because it is both popular and performant.\n\nfrom langchain_openai import ChatOpenAI\n\n# https://openrouter.ai/openai/gpt-5-nano\nmodel_gpt5_nano = ChatOpenAI(\n    model=\"openai/gpt-5-nano\",\n    temperature=0,\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n)\n\n# https://openrouter.ai/nvidia/nemotron-3-nano-30b-a3b:free\nmodel_nemotron3_nano = ChatOpenAI(\n    model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n    temperature=0,\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n)\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nRunning a model locally\nLangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.\nOllama is one of the easiest ways to run chat and embedding models locally."
  },
  {
    "objectID": "L01/01_agents.html#key-methods",
    "href": "L01/01_agents.html#key-methods",
    "title": "Agents in LangChain",
    "section": "Key Methods",
    "text": "Key Methods\n\nInvoke\nStream\nBatch\n\n\n1. Invoke\nThe most straightforward way to call a model is to use invoke() with a single message or a list of messages:\n\nmessage = model_nemotron3_nano.invoke(\"why do parrots talk?\")\n\n.. this returns an AIMessage object:\n\nmessage\n\nAIMessage(content='Parrots ‚Äútalk‚Äù because they‚Äôre one of the few animals that can **learn and reproduce complex vocalizations**‚Äîa skill called vocal learning. Here‚Äôs a quick rundown of why and how they do it:\\n\\n| Reason | What it means for parrots |\\n|--------|--------------------------|\\n| **Social bonding** | In the wild, parrots use calls to stay in contact with flock members, coordinate movement, and reinforce pair bonds. Mimicking the sounds of their companions (including human speech) helps them stay socially connected. |\\n| **Territory & status** | Some species use distinctive vocalizations to claim space or signal dominance. A parrot that can produce a clear, attention‚Äëgrabbing ‚Äúspeech‚Äù may gain more social leverage. |\\n| **Mental stimulation** | Parrots are highly intelligent; they need cognitive challenges. Learning new sounds is a form of problem‚Äësolving that keeps their brains active and reduces boredom‚Äërelated behaviors (like feather‚Äëplucking). |\\n| **Mimicry as a survival tool** | In the wild, many parrots imitate the calls of other species (e.g., hawks, other birds) to deter predators or to blend into mixed‚Äëspecies flocks. This ability extends to human speech when they live with people. |\\n| **Neural specialization** | Parrots possess a brain region called the **song system** that is analogous to the vocal‚Äëlearning circuits of songbirds and humans. This system allows them to store, modify, and reproduce complex sound patterns with high fidelity. |\\n\\n### How they actually ‚Äútalk‚Äù\\n\\n1. **Listening & Encoding** ‚Äì A parrot constantly hears sounds in its environment and stores a mental ‚Äútemplate‚Äù of each one.\\n2. **Practice & Refinement** ‚Äì Using its syrinx (the bird equivalent of vocal cords), it tries out different variations, gradually shaping the output to match the stored template.\\n3. **Feedback Loop** ‚Äì The bird hears its own attempt, compares it to the template, and tweaks pitch, rhythm, or timing until it‚Äôs close enough.\\n4. **Reinforcement** ‚Äì Positive attention from humans (treats, praise, interaction) reinforces the behavior, encouraging the parrot to repeat the sound.\\n\\n### Why some parrots sound more ‚Äúhuman‚Äëlike‚Äù\\n\\n- **Species differences** ‚Äì African Grey, Amazon, and some Cockatoos have especially flexible vocal repertoires.\\n- **Individual exposure** ‚Äì The more varied and frequent the human speech they hear, the richer their mimicry can become.\\n- **Training & interaction** ‚Äì Repetition, consistent cues, and reward-based training help solidify specific words or phrases.\\n\\n### Bottom line\\n\\nParrots ‚Äútalk‚Äù not because they understand language the way we do, but because they‚Äôre **social, intelligent mimics** equipped with a unique vocal‚Äëlearning brain circuit. By copying the sounds that matter most to them‚Äîwhether it‚Äôs a flock mate‚Äôs call, a predator‚Äôs alarm, or a human‚Äôs voice‚Äîthey enhance their social world and keep their minds sharp. \\n\\nIf you‚Äôre curious about getting a talking parrot, remember that the ability to mimic speech varies widely among individuals and species, and it thrives best with plenty of social interaction, mental enrichment, and consistent, positive training.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 731, 'prompt_tokens': 22, 'total_tokens': 753, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 58, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771586508-ixswTWdhOMhqmt9bxk5a', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c7ac8-d0ab-75d1-aeeb-5bcfce7f31bb-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 22, 'output_tokens': 731, 'total_tokens': 753, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 58}})\n\n\n.. which has a content property, which includes the generated response text:\n\nprint(message.content)\n\nParrots ‚Äútalk‚Äù because they‚Äôre one of the few animals that can **learn and reproduce complex vocalizations**‚Äîa skill called vocal learning. Here‚Äôs a quick rundown of why and how they do it:\n\n| Reason | What it means for parrots |\n|--------|--------------------------|\n| **Social bonding** | In the wild, parrots use calls to stay in contact with flock members, coordinate movement, and reinforce pair bonds. Mimicking the sounds of their companions (including human speech) helps them stay socially connected. |\n| **Territory & status** | Some species use distinctive vocalizations to claim space or signal dominance. A parrot that can produce a clear, attention‚Äëgrabbing ‚Äúspeech‚Äù may gain more social leverage. |\n| **Mental stimulation** | Parrots are highly intelligent; they need cognitive challenges. Learning new sounds is a form of problem‚Äësolving that keeps their brains active and reduces boredom‚Äërelated behaviors (like feather‚Äëplucking). |\n| **Mimicry as a survival tool** | In the wild, many parrots imitate the calls of other species (e.g., hawks, other birds) to deter predators or to blend into mixed‚Äëspecies flocks. This ability extends to human speech when they live with people. |\n| **Neural specialization** | Parrots possess a brain region called the **song system** that is analogous to the vocal‚Äëlearning circuits of songbirds and humans. This system allows them to store, modify, and reproduce complex sound patterns with high fidelity. |\n\n### How they actually ‚Äútalk‚Äù\n\n1. **Listening & Encoding** ‚Äì A parrot constantly hears sounds in its environment and stores a mental ‚Äútemplate‚Äù of each one.\n2. **Practice & Refinement** ‚Äì Using its syrinx (the bird equivalent of vocal cords), it tries out different variations, gradually shaping the output to match the stored template.\n3. **Feedback Loop** ‚Äì The bird hears its own attempt, compares it to the template, and tweaks pitch, rhythm, or timing until it‚Äôs close enough.\n4. **Reinforcement** ‚Äì Positive attention from humans (treats, praise, interaction) reinforces the behavior, encouraging the parrot to repeat the sound.\n\n### Why some parrots sound more ‚Äúhuman‚Äëlike‚Äù\n\n- **Species differences** ‚Äì African Grey, Amazon, and some Cockatoos have especially flexible vocal repertoires.\n- **Individual exposure** ‚Äì The more varied and frequent the human speech they hear, the richer their mimicry can become.\n- **Training & interaction** ‚Äì Repetition, consistent cues, and reward-based training help solidify specific words or phrases.\n\n### Bottom line\n\nParrots ‚Äútalk‚Äù not because they understand language the way we do, but because they‚Äôre **social, intelligent mimics** equipped with a unique vocal‚Äëlearning brain circuit. By copying the sounds that matter most to them‚Äîwhether it‚Äôs a flock mate‚Äôs call, a predator‚Äôs alarm, or a human‚Äôs voice‚Äîthey enhance their social world and keep their minds sharp. \n\nIf you‚Äôre curious about getting a talking parrot, remember that the ability to mimic speech varies widely among individuals and species, and it thrives best with plenty of social interaction, mental enrichment, and consistent, positive training.\n\n\nA list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.\n\nfrom langchain.messages import SystemMessage, HumanMessage, AIMessage\n\nconversation = [    \n    SystemMessage(content=\"You are a helpful assistant that translates English to Arabic.\"),\n    HumanMessage(content=\"Translate: I love programming.\"),\n    AIMessage(content=\"ÿ£ÿ≠ÿ® ÿßŸÑÿ®ÿ±ŸÖÿ¨ÿ©.\"),\n    HumanMessage(content=\"I love building applications.\")\n]\n\nmessage = model_nemotron3_nano.invoke(conversation)\nprint(message.content)\n\nÿ£ÿ≠ÿ® ÿ™ÿ∑ŸàŸäÿ± ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™.\n\n\n\n\n2. Stream\nMost models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.\nCalling stream() returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:\n\nfor chunk in model_nemotron3_nano.stream(\"Why do parrots have colorful feathers?\"):\n    print(chunk.text, end=\"|\", flush=True)\n\n||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Par|ro|ts| are| famous| for| their| vivid| plum|age|,| and| that| color|ation| isn|‚Äôt| just| for| show| ‚Äî| it| serves| several| important| functions| that| have| been| shaped| by| evolution|.| Here|‚Äôs| a| quick| rund|own| of| the| main| reasons|:\n\n||| Reason| || How| it| works| || Why| it| matters| for| parro|ts| |\n|||--------|||------------|--|||----------------|------------||\n||| **|Sex|ual| selection|**| || Bright|,| contrasting| colors| signal| health|,| good| genetics|,| and| strong| immune| systems|.| Males| and| females| often| use| plum|age| to| attract| mates| or| to| assess| rivals|.| || In| many| par|rot| species|,| brighter| males| are| preferred| by| females|,| leading| to| stronger| reproductive| success| for| those| with| more| vivid| feathers|.| |\n||| **|Species| and| individual| recognition|**| || Dist|inct| color| patterns| help| individuals| identify| members| of| their| own| species| (|and| sometimes| specific| mates| or| offspring|).| || In| dense|,| multi|‚Äë|species| fl|ocks|,| a| unique| ‚Äú|signature|‚Äù| color|ation| reduces| the| chance| of mistaken| identity and helps| keep| the| group| cohesive|.| |\n||| **|Social| signaling|**| || Color| can| convey| status|,| dominance|,| or| readiness| to| breed|.| Some| parro|ts| change| the| intensity| or| hue| of| their| feathers| during| courts|hip| or| when| threatened|.| || A| sudden| flash| of| bright| color| can| start|le| a| predator| or| signal| aggression| to| a| rival|,| while| a| muted| palette| may| indicate| submission| or| non|‚Äë|th|reat|.| |\n||| **|Cam|oufl|age| in| specific| habitats|**| || While| many| parro|ts| are| conspicuous|,| some| (|e|.g|.,| forest|‚Äë|dw|elling| species|)| have| gre|ens| and| brown|s| that| blend| with| foli|age|.| || In| these| cases|,| color|ation| is| an| adaptation| to| the| visual| background| of| their| environment|,| helping| them| avoid| predators| while| still| being| visible| to| cons|pecific|s|.| |\n||| **|D|iet|‚Äë|derived| pigments|**| || Parro|ts| cannot| synthesize| many| bright| pigments| themselves|;| they| obtain| red|s|,| or|anges|,| and| yellow|s| from| carot|eno|ids| in| fruits|,| flowers|,| and| seeds|.| || A| diet| rich| in| these| pigments| not| only| fuels| health| but| also| directly| influences| how| vivid| a| bird|‚Äôs| feathers| appear|.| |\n||| **|Struct|ural| color|ation|**| || Mic|roscopic| structures| in| feather| barb|ules| scatter| light|,| producing| ir|ides|cent| blues| and| gre|ens| that| are| not| pigment|‚Äë|based|.| || This| creates| sh|immer|ing| effects| that| can| be| more| eye|‚Äë|catch|ing| than| pigment| alone|,| especially| under| different| lighting| conditions|.| |\n\n|###| Put|ting| it| together|\n|-| **|E|volution|ary| pressure|**:| Bright| colors| increase| an| individual|‚Äôs| chances| of| finding| a| mate| and| defending| territory|,| which| boost|s| reproductive| success|.\n|-| **|Ec|ological| context|**:| The| same| colors| that| attract| mates| can| also| serve| as| warning| signals| or| camoufl|age|,| depending| on| the| species|‚Äô| habitat| and| behavior|.\n|-| **|Phys|iological| limits|**:| Because| parro|ts| rely| on| dietary| pigments| and| structural| feather| features|,| their| color| palette| is| tightly| linked| to| what| they| eat| and| how| their| feathers| are| built.\n\n|So|,| the| spectacular| rainbow| of| par|rot| feathers| is| a| multif|unctional| adaptation| ‚Äî| primarily| a| tool| for| communication| and| reproduction|,| fine|‚Äë|t|uned| by| diet|,| habitat|,| and| the| physics| of| light|.||| üåà||||ü¶ú||||\n\n\n\n\n3. Batch\nBatching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:\n\nresponses = model_nemotron3_nano.batch([\n    \"What is the capital of Saudi Arabia?\",\n    \"What is 2 + 8\",\n    \"Is the sky blue or is it our perception? give a short and concise answer\"\n])\n\nfor response in responses:\n    print(response)\n\ncontent='The capital of Saudi Arabia is **Riyadh**.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 24, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 26, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587230-rws1sKd4yd2Ir45jBE92', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-dab8-7670-aebf-be1a19d73902-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 24, 'output_tokens': 38, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 26}}\ncontent='2\\u202f+\\u202f8\\u202f=\\u202f10.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 23, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 22, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587230-FEcwZ8VMp5p1Qgvgnrqr', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-daba-76d3-856b-d4204b17db9a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 23, 'output_tokens': 40, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 22}}\ncontent='The sky appears blue because molecules in the atmosphere scatter short‚Äëwavelength (blue) sunlight‚Äîan objective physical effect that our visual system interprets as the color blue.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 32, 'total_tokens': 206, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 159, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587231-VBn1J8V9CcCv10J2gqBT', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-dabc-7871-bdd8-f89b2bed8661-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 32, 'output_tokens': 174, 'total_tokens': 206, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 159}}\n\n\n\nfor i, response in enumerate(responses):\n    print(response.content)\n    print(\"=\"*100)\n\nThe capital of Saudi Arabia is **Riyadh**.\n====================================================================================================\n2‚ÄØ+‚ÄØ8‚ÄØ=‚ÄØ10.\n====================================================================================================\nThe sky appears blue because molecules in the atmosphere scatter short‚Äëwavelength (blue) sunlight‚Äîan objective physical effect that our visual system interprets as the color blue.\n===================================================================================================="
  },
  {
    "objectID": "L01/01_agents.html#structured-output",
    "href": "L01/01_agents.html#structured-output",
    "title": "Agents in LangChain",
    "section": "Structured output",
    "text": "Structured output\nModels can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.\nPydantic models provide the richest feature set with field validation, descriptions, and nested structures.\n\nfrom pydantic import BaseModel, Field\n\nclass Movie(BaseModel):\n    \"\"\"A movie with details.\"\"\"\n    title: str = Field(..., description=\"The title of the movie\")\n    year: int = Field(..., description=\"The year the movie was released\")\n    director: str = Field(..., description=\"The director of the movie\")\n    rating: float = Field(..., description=\"The movie's rating out of 10\")\n\ntitle='Inception' year=2010 director='Christopher Nolan' rating=8.8\n\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=Movie(title='Inception', ...pher Nolan', rating=8.8), input_type=Movie])\n  return self.__pydantic_serializer__.to_python(\n\n\n\nmodel_with_structure = model_nemotron3_nano.with_structured_output(Movie)\nresponse = model_with_structure.invoke(\"Provide details about the movie Inception\")\n\n\nprint(\"Title:\", response.title)\nprint(\"Year:\", response.year)\nprint(\"Director:\", response.director)\nprint(\"Rating:\", response.rating)\n\nTitle: Inception\nYear: 2010\nDirector: Christopher Nolan\nRating: 8.8"
  },
  {
    "objectID": "L01/01_agents.html#tool-calling",
    "href": "L01/01_agents.html#tool-calling",
    "title": "Agents in LangChain",
    "section": "Tool calling",
    "text": "Tool calling\nModels can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\n\nA schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\nA function or coroutine to execute.\n\nNote: A coroutine is a method that can suspend execution and resume at a later time\n\nfrom langchain.tools import tool\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the weather at a location.\"\"\"\n    return f\"It's always sunny in {location}.\"\n\n\nmodel_with_tools = model_nemotron3_nano.bind_tools([get_weather])\n\nresponse = model_with_tools.invoke(\"What's the weather like in the Moon?\")\nfor tool_call in response.tool_calls:\n    # View tool calls made by the model\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n\nTool: get_weather\nArgs: {'location': 'Boston'}\n\n\n\nTool Input Schemas\nDefine complex inputs with Pydantic models or JSON schemas:\n\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\nclass WeatherInput(BaseModel):\n    \"\"\"Input for weather queries.\"\"\"\n    location: str = Field(description=\"City name or coordinates\")\n    units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n        default=\"celsius\",\n        description=\"Temperature unit preference\"\n    )\n    include_forecast: bool = Field(\n        default=False,\n        description=\"Include 5-day forecast\"\n    )\n\n@tool(args_schema=WeatherInput)\ndef get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -&gt; str:\n    \"\"\"Get current weather and optional forecast.\"\"\"\n    temp = 22 if units == \"celsius\" else 72\n    result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n    if include_forecast:\n        result += \"\\nNext 5 days: Sunny\"\n    return result\n\n\nmodel_with_tools = model_nemotron3_nano.bind_tools([get_weather])\n\n\nresponse = model_with_tools.invoke(\n    \"What's the weather like in the Moon? \"\n    \"in fahrenheit and include the forecast please.\"\n)\nfor tool_call in response.tool_calls:\n    # View tool calls made by the model\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n\nTool: get_weather\nArgs: {'location': 'the Moon', 'units': 'fahrenheit', 'include_forecast': True}"
  },
  {
    "objectID": "L01/01_agents.html#search-tools",
    "href": "L01/01_agents.html#search-tools",
    "title": "Agents in LangChain",
    "section": "Search Tools",
    "text": "Search Tools\nTavily is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it‚Äôs easy to sign up and offers a generous free tier.\n\nfrom tavily import TavilyClient\n\ntavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n\n\ndef internet_search(\n    query: str,\n    max_results: int = 5,\n    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n    include_raw_content: bool = False,\n):\n    \"\"\"Run a web search\"\"\"\n    return tavily_client.search(\n        query,\n        max_results=max_results,\n        include_raw_content=include_raw_content,\n        topic=topic,\n    )\n\n\nresult = internet_search(\"What is LangGraph?\", max_results=3)\nresult\n\n{'query': 'What is LangGraph?',\n 'response_time': 0.61,\n 'follow_up_questions': None,\n 'answer': None,\n 'images': [],\n 'results': [{'url': 'https://www.ibm.com/think/topics/langgraph',\n   'title': 'What is LangGraph? - IBM',\n   'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent‚Äôs state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.',\n   'score': 0.95539,\n   'raw_content': None},\n  {'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/',\n   'title': 'What is LangGraph? - GeeksforGeeks',\n   'content': 'LangGraph is an open-source framework built by LangChain that streamlines the creation and management of AI agent workflows. At its core, LangGraph combines large language models (LLMs) with graph-based architectures allowing developers to map, organize and optimize how AI agents interact and make decisions. By treating workflows as interconnected nodes and edges, LangGraph offers a scalable, transparent and developer-friendly way to design advanced AI systems ranging from simple chatbots to multi-agent system. The diagram below shows how LangGraph structures its agent-based workflow using distinct tools and stages. By designing workflows, users combine multiple nodes into powerful, dynamic AI processes. * ****langgraph:**** Framework for building graph-based AI workflows. ### Step 6: Build LangGraph Workflow. * Build the workflow graph using LangGraph, adding nodes for classification and response, connecting them with edges and compiling the app. * Send each input through the workflow graph and returns the bot‚Äôs response, either a greeting or an AI-powered answer. + Machine Learning Interview Questions and Answers15+ min read.',\n   'score': 0.9393874,\n   'raw_content': None},\n  {'url': 'https://docs.langchain.com/oss/python/langgraph/overview',\n   'title': 'LangGraph overview - Docs by LangChain',\n   'content': 'Trusted by companies shaping the future of agents‚Äî including Klarna, Replit, Elastic, and more‚Äî LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain‚Äôs agents that provide pre-built architectures for common LLM and tool-calling loops. LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more. LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.## LangSmith Agent Server. Contains agent abstractions built on top of LangGraph.',\n   'score': 0.92507464,\n   'raw_content': None}],\n 'request_id': '4d1b1ba4-3b2c-4355-a9d6-446cbddf26e0'}"
  },
  {
    "objectID": "L01/01_agents.html#create-an-agent",
    "href": "L01/01_agents.html#create-an-agent",
    "title": "Agents in LangChain",
    "section": "Create an Agent",
    "text": "Create an Agent\nAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\nAn LLM Agent runs tools in a loop to achieve a goal. An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n\n\nAgent Loop\n\n\ncreate_agent provides a production-ready agent implementation.\n\nfrom langchain.agents import create_agent\n\n# System prompt to steer the agent to be an expert researcher\nAGENT_PROMPT = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n\nYou have access to an internet search tool as your primary means of gathering information.\n\nKeep it short and concise.\n\n## `internet_search`\n\nUse this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n\"\"\"\n\nagent = create_agent(\n    model=model_nemotron3_nano,\n    tools=[internet_search],\n    system_prompt=AGENT_PROMPT\n)\n\n\nInvoke\n\nresult = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})\n\n\nresult\n\n{'messages': [HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={}, id='29083314-fa84-4871-ba0d-f594b69ce1fe'),\n  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 327, 'prompt_tokens': 447, 'total_tokens': 774, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 292, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771589000-vrLW5z3J6cADEb7ZfFeY', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c7aee-db92-70c2-9d0e-ef86169a1ec1-0', tool_calls=[{'name': 'internet_search', 'args': {'max_results': 5, 'topic': 'general', 'query': 'LangGraph', 'include_raw_content': True}, 'id': 'call_89580e7dd35048f5a3921112', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 447, 'output_tokens': 327, 'total_tokens': 774, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 292}}),\n  ToolMessage(content='{\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph? - IBM\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent‚Äôs state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\", \"score\": 0.94251215, \"raw_content\": \"# What is LangGraph?\\\\n\\\\n## Authors\\\\n\\\\n[Bryan Clark](https://www.ibm.com/think/author/bryan-clark) \\\\n\\\\nSenior Technology Advocate\\\\n\\\\n## LangGraph overview\\\\n\\\\nLangGraph, created by [LangChain](https://www.ibm.com/think/topics/langchain), is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize [large language models](https://www.ibm.com/think/topics/large-language-models) (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an [AI agent workflow](https://www.ibm.com/think/topics/ai-agents).\\\\n\\\\nWhat does all this information mean? The following example can offer a clearer understanding of LangGraph: Think about these graph-based architectures as a powerful configurable map, a ‚ÄúSuper-Map.‚Äù Users can envision the [AI workflow](https://www.ibm.com/think/topics/ai-workflow) as being ‚ÄúThe Navigator‚Äù of this ‚ÄúSuper-Map.‚Äù Finally, in this example, the user is ‚ÄúThe Cartographer.‚Äù In this sense, the navigator charts out the optimal routes between points on the ‚ÄúSuper-Map,‚Äù all of which are created by ‚ÄúThe Cartographer.‚Äù\\\\n\\\\nTo recap, optimal routes within the graph-based architectures (‚ÄúSuper-Map‚Äù) are charted and explored by using the AI workflow\\xa0(‚ÄúThe Navigator‚Äù). This analogy is a great place to start understanding LangGraph‚Äîand if you like maps then you are welcome for the bonus opportunity to see someone use the word cartographer.\\\\n\\\\nLangGraph workflow\\\\n\\\\nLangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent‚Äôs state. Within LangGraph, the ‚Äústate‚Äù feature serves as a memory bank that records and tracks all the valuable information processed by the AI system. It‚Äôs similar to a digital notebook where the system captures and updates data as it moves through various stages of a workflow or graph analysis.\\\\n\\\\nFor example, if you were running agents to monitor the weather, this feature could track the number of times it snowed and make suggestions based on changing snowfall trends. This observability of how the system works to complete complex tasks is useful for beginners to understand more about state management. State management is helpful when it comes to debugging as it allows the application‚Äôs state to be centralized, thus often shortening the overall process.\\\\n\\\\nThis approach allows for more effective decision-making, improved scalability and enhanced overall performance. It also allows for more engagement with individuals who might be new to these processes or prefer a clearer picture of what is going on behind the scenes.\\\\n\\\\nLangGraph is also built on several key technologies, including [LangChain,](https://www.ibm.com/think/topics/langchain) a Python framework for building AI applications. LangChain includes a library for building and managing [LLMs](https://www.ibm.com/think/topics/large-language-models). LangGraph also uses the human-in-the-loop approach. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including [chatbots](https://www.ibm.com/think/topics/chatbots), state graphs and [other agent-based systems](https://www.ibm.com/think/topics/multiagent-system).\\\\n\\\\nDelve deeper into the world of LangGraph by exploring its key features, benefits and use cases. By the end of this article, you will have the knowledge and resources to take the next steps with LangGraph.\\\\n\\\\n## Key components of LangGraph\\\\n\\\\nLet‚Äôs begin by first understanding the key components that make up LangGraph. The framework is built around several key components that work together to enable users to create and manage complex AI workflows. These components include:\\\\n\\\\n#### Monitoring mechanism\\\\n\\\\n**Human-in-the-loop**: [Human-in-the-loop (HITL)](https://hdsr.mitpress.mit.edu/pub/812vijgg/release/3)\\xa0refers to the requirement of human interaction at some point in the process. In the realm of [machine learning](https://www.ibm.com/think/topics/machine-learning) (ML), HITL refers to a collaborative process where humans augment the computational capabilities of machines to make informed decisions while building a model. By using the most critical data points, HITL enhances the accuracy of machine learning algorithms, surpassing random sampling methods.\\\\n\\\\n#### Graph architecture\\\\n\\\\n**Stateful graphs**: A concept where each node in the graph represents a step in the computation, essentially devising a state graph. This stateful approach allows the graph to retain information about the previous steps, enabling continuous and contextual processing of information as the computation unfolds. Users can manage all LangGraph‚Äôs stateful graphs with its APIs.\\\\n\\\\n**Cyclical graph**: A cyclical graph is any graph that contains at least one cycle and is essential for agent runtimes. This means that there exists a path that starts and ends at the same node, forming a loop within the graph. Complex workflows often involve cyclic dependencies, where the outcome of one step depends on previous steps in the loop.\\\\n\\\\n**Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. Nodes can be thought of as ‚Äúactors‚Äù that interact with each other in a specific way. For example,\\xa0to add nodes for tool calling, one can use the ToolNode. Another example, the next node, refers to the node that will be executed following the current one.\\\\n\\\\n**Edges**: Edges are a function within Python that determines which node to execute next based on the current state. Edges can be conditional branches or fixed transitions.\\\\n\\\\n#### Tools\\\\n\\\\n**RAG**: [Retrieval-augmented generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) combines the power of LLMs with contextual information from external sources by retrieving relevant documents, which are then used as input for answer generation.\\\\n\\\\n**Workflows**: Workflows are the sequences of node interactions that define an AI workflow. By arranging nodes into a workflow, users can create more complex and dynamic workflows that use the strengths of individual components.\\\\n\\\\n**APIs**: LangGraph provides a set of [APIs](https://www.ibm.com/think/topics/api) that enable users to interact with its components in a programmatic way. Users can use an API key, add new nodes, modify existing workflows and retrieve data from an AI workflow.\\\\n\\\\n**LangSmith**: LangSmith is a specialized API for building and managing LLMs within LangGraph. It provides tools for initializing LLMs, adding conditional edges and optimizing performance. By combining these components in innovative ways, users can build more sophisticated AI workflows that use the strengths of individual components.\\\\n\\\\nIndustry newsletter\\\\n\\\\n### The latest AI trends, brought to you by experts\\\\n\\\\nGet curated insights on the most important‚Äîand intriguing‚ÄîAI news. Subscribe to our weekly Think newsletter. See the [IBM Privacy Statement](https://www.ibm.com/privacy).\\\\n\\\\n### Thank you! You are subscribed.\\\\n\\\\nYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe [here](https://www.ibm.com/account/reg/signup?formid=news-urx-51525). Refer to our [IBM Privacy Statement](https://www.ibm.com/us-en/privacy) for more information.\\\\n\\\\n## How LangGraph scales\\\\n\\\\nBy using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.\\\\n\\\\n**Enhanced decision-making**: By modeling complex relationships between nodes, LangGraph provides a framework for building more effective decision-making systems.\\\\n\\\\n**Increased flexibility**: An open source nature and modular design for developers to integrate new components and adapt existing workflows.\\\\n\\\\n**Multiagent workflows:** Complex tasks can be tackled through multiagent workflows. This approach involves creating dedicated LangChain agents for specific tasks or domains. Routing tasks to the appropriate LangChain agents allows for parallel execution and efficient handling of diverse workloads. Such a multiagent network architecture exemplifies the decentralized\\xa0coordination of agent automation.\\\\n\\\\nA great example, created by Joao Moura, is using CrewAI with LangChain and LangGraph. Checking emails and creating drafts is automated with CrewAI orchestrating autonomous AI agents, enabling them to collaborate and run complex tasks efficiently.\\\\n\\\\nIBM watsonx.ai\\\\n\\\\n### Data Insights with LangGraph and watsonx.ai\\\\n\\\\nCan an AI agent take our natural language query and do the processing for us to give us that meaningful output? We use several pieces of open source technology and the power of watsonx.ai to\\xa0put this to the test.\\\\n\\\\n[Explore watsonx.ai](https://www.ibm.com/products/watsonx-ai)\\\\n\\\\n## LangGraph use cases\\\\n\\\\n**Chatbots**: Users can build an agentic application for vacation planning, with node-based workflows and directed acyclic graphs (DAGs). The chatbot learns to respond to minimal user input and tailor recommendations. Currently, services such as Google‚Äôs Duplex are using LangGraph in a similar fashion to mimic human-like conversations.\\\\n\\\\n**Agent systems**: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\\\\n\\\\n**LLM applications**: By using LangGraph‚Äôs capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences.\\\\n\\\\n## LLM integration in LangGraph\\\\n\\\\nLangGraph‚Äôs agents are based on OpenAI‚Äôs series of GPT (generative pretrained transformer) models GPT-3.5 and GPT-4. However, LangGraph and its open source community have contributed to the addition of several other models that initialize through LLM API configuration, including Anthropic and AzureChatOpenAI models. The relatively small loop is similar to projects such as Auto-GPT.\\\\n\\\\nLangGraph offers a YouTube tutorial that facilitates the exploration of how to integrate with open source LLMs on its GitHub docs site. The first step to integrating an LLM is to set up an inference repository (repo) such as LLaMA-Factory, FastChat and Ollama. This repository enables deployment of the corresponding LLM model that is configured through its API credentials.\\\\n\\\\n## Other AI agent frameworks\\\\n\\\\nCrewAI, MetaGPT and AutoGen are just a few multiagent frameworks that can handle complex workflows. This operation allows for a more flexible and nuanced approach to tackling diverse computational challenges. By providing comprehensive debugging capabilities, these\\xa0frameworks enable developers to quickly identify and resolve issues, leading to more efficient development and optimization processes.\\\\n\\\\n## LangGraph Studio: A visual interface for workflow development\\\\n\\\\nLangGraph has also introduced LangGraph Studio, a visual interface for workflow development. With LangGraph Studio, users can design and build workflows by using a graphical interface, without having to write code. The downloadable desktop application makes LangGraph Studio more usable for beginners. LangGraph Studio has also made these additional features available:\\\\n\\\\n**Shallow learning curve**: LangGraph Studio is not needed to access LangGraph. However, by using LangGraph Studio‚Äôs visual interface, users can focus on designing their workflows without getting bogged down in code.\\\\n\\\\n**Improved collaboration**: LangGraph Studio enables the sharing of workflows with others, whether that‚Äôs a team of developers or a client.\\\\n\\\\n**Debugging**: The capabilities do not end with building a graph, debugging features are included to ensure the graph is accurate and reliable. LangGraph Studio, with its cutting-edge integrated development environment (IDE), helps visualize and debug LangGraph applications.\\\\n\\\\n## Future developments\\\\n\\\\n**Enhanced natural language processing (NLP)**: LangGraph will have more advanced [NLP](https://www.ibm.com/think/topics/natural-language-processing) capabilities, allowing it to better understand natural language and provide more accurate responses.\\\\n\\\\n**Improved machine learning**: LangGraph will have improved machine learning capabilities, allowing it to learn and improve over time.\\\\n\\\\n**Support for new platforms**: LangGraph will support new platforms, such as mobile devices and edge computing to make its technology more accessible.\\\\n\\\\nLink copied\\\\n\\\\n[Ebook   Start realizing ROI: A practical guide to agentic AI \\\\n\\\\nDiscover ways to get ahead, successfully scaling AI across your business with real results.\\\\n\\\\n Read the ebook](https://www.ibm.com/account/reg/signup?formid=urx-54067)\\\\n\\\\n[Build, run and manage AI agents with watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)\\\\n\\\\n## Resources\\\\n\\\\n[IBV Report   The enterprise in 2030: Engineered for perpetual innovation \\\\n\\\\nDiscover our five predictions about what will define the most successful enterprises in 2030, and the steps leaders can take to gain an AI-first advantage.\\\\n\\\\n Read the report](https://www.ibm.com/thought-leadership/institute-business-value/report/enterprise-2030)\\\\n\\\\n[Report   AI governance imperative: evolving regulations and emergence of agentic AI \\\\n\\\\nLearn how evolving regulations and the emergence of AI agents are reshaping the need for robust AI governance frameworks.\\\\n\\\\n Read the report](https://www.ibm.com/forms/mkt-54070)\\\\n\\\\n[Techsplainers podcast   Agentic AI explained \\\\n\\\\nTechsplainers by IBM breaks down the essentials of agentic AI, from key concepts to real‚Äëworld use cases. Clear, quick episodes help you learn the fundamentals fast.\\\\n\\\\n Listen now](https://www.ibm.com/think/podcasts/techsplainers#tabs-fw-44e285b2cc-item-a3ea4f0927-tab)\\\\n\\\\n[Guidebook   Unlock AI ROI: A tactical guide to enterprise productivity \\\\n\\\\nLearn proven strategies to boost productivity and power enterprise transformation with AI and innovation at the core.\\\\n\\\\n Read the guidebook](https://www.ibm.com/account/reg/signup?formid=urx-54227)\\\\n\\\\n[Report   IDC MarketScape names IBM a leader in 2025 gen AI evaluation technology \\\\n\\\\nDownload the report to learn why IDC MarketScape named IBM a leader in 2025 gen AI evaluation technology, and how watsonx.governance¬Æ advances risk management, reporting and integration.\\\\n\\\\n Read the report](https://www.ibm.com/forms/mkt-54030)\\\\n\\\\n[Buyer guide   How AI agents and assistants can benefit your organization \\\\n\\\\nDive into this comprehensive guide that breaks down key use cases and core capabilities, providing step-by-step recommendations to help you choose the right solutions for your business.\\\\n\\\\n Read the guide](https://www.ibm.com/forms/mkt-53811)\\\\n\\\\n[Video   Reimagine business productivity with AI agents and assistants \\\\n\\\\nLearn how AI agents and AI assistants can work together to achieve new levels of productivity.\\\\n\\\\n Watch now](https://www.ibm.com/think/videos/ai-academy/reimagine-business-productivity-with-ai)\\\\n\\\\n[Demo   Try watsonx Orchestrate¬Æ \\\\n\\\\nExplore how generative AI assistants can lighten your workload and improve productivity.\\\\n\\\\n Start the demo](https://www.ibm.com/products/watsonx-orchestrate/demos)\\\\n\\\\n[Report   From AI projects to profits: How agentic AI can sustain financial returns \\\\n\\\\nLearn how organizations are shifting from launching AI in disparate pilots to using it to drive transformation at the core.\\\\n\\\\n Read the report](https://www.ibm.com/thought-leadership/institute-business-value/report/agentic-ai-profits)\\\\n\\\\n[Report   Omdia Report on empowered intelligence: The impact of AI agents \\\\n\\\\nDiscover how you can unlock the full potential of gen AI with AI agents.\\\\n\\\\n Read the report](https://www.ibm.com/forms/mkt-53501)\\\\n\\\\n[Podcast   How AI agents will reinvent productivity \\\\n\\\\nLearn ways to use AI to be more creative, efficient and start adapting to a future that involves working closely with AI agents.\\\\n\\\\n Listen now](https://www.ibm.com/think/podcasts/ai-in-action/how-ai-agents-will-reinvent-productivity)\\\\n\\\\n[News   Ushering in the agentic enterprise: Putting AI to work across your entire technology estate \\\\n\\\\nStay updated about the new emerging AI agents, a fundamental tipping point in the AI revolution.\\\\n\\\\n Read the news](https://www.ibm.com/new/announcements/productivity-revolution-with-ai-agents-that-work-across-stack)\\\\n\\\\n[Podcast   The future of agents, AI energy consumption, Anthropic computer use and Google watermarking AI-generated text \\\\n\\\\nStay ahead of the curve with our AI experts on this episode of Mixture of Experts as they dive deep into the future of AI agents and more.\\\\n\\\\n Listen now](https://www.ibm.com/think/podcasts/mixture-of-experts/future-of-ai-agents-ai-energy-consumption-anthropic-computer-use-google-watermarking-ai)\\\\n\\\\n[Case study   How Comparus is using a \\\\\"banking assistant\\\\\" \\\\n\\\\nComparus used solutions from IBM watsonx.ai¬Æ and impressively demonstrated the potential of conversational banking as a new interaction model.\\\\n\\\\n Read the case study](https://www.ibm.com/case-studies/comparus-gmbh)\\\\n\\\\nRelated solutions   \\\\n IBM¬Æ\\xa0watsonx\\xa0Orchestrate‚Ñ¢\\xa0   \\\\n\\\\nEasily design scalable AI assistants and agents, automate repetitive tasks and simplify complex processes with\\xa0IBM¬Æ\\xa0watsonx\\xa0Orchestrate‚Ñ¢.\\\\n\\\\n   [Explore watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)   \\\\n Artificial intelligence solutions   \\\\n\\\\nPut AI to work in your business with IBM\\'s industry-leading AI expertise and portfolio of solutions at your side.\\\\n\\\\n   [Explore AI solutions](https://www.ibm.com/artificial-intelligence)   \\\\n AI consulting and services   \\\\n\\\\nReinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.\\\\n\\\\n   [Explore AI services](https://www.ibm.com/consulting/artificial-intelligence)\\\\n\\\\nTake the next step\\\\n\\\\n \\\\n\\\\nWhether you choose to customize pre-built apps and skills or build and deploy custom agentic services using an AI studio, the IBM watsonx platform has you covered.\\\\n\\\\n    [Explore watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)   [Explore watsonx.ai](https://www.ibm.com/products/watsonx-ai)\"}, {\"url\": \"https://docs.langchain.com/oss/python/langgraph/overview\", \"title\": \"LangGraph overview - Docs by LangChain\", \"content\": \"Trusted by companies shaping the future of agents‚Äî including Klarna, Replit, Elastic, and more‚Äî LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain‚Äôs agents that provide pre-built architectures for common LLM and tool-calling loops. LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more. LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.## LangSmith Agent Server. Contains agent abstractions built on top of LangGraph.\", \"score\": 0.90039873, \"raw_content\": \"[Docs by LangChain home page](/)\\\\n\\\\n[Deep Agents](/oss/python/deepagents/overview)[LangChain](/oss/python/langchain/overview)[LangGraph](/oss/python/langgraph/overview)[Integrations](/oss/python/integrations/providers/overview)[Learn](/oss/python/learn)[Reference](/oss/python/reference/overview)[Contribute](/oss/python/contributing/overview)\\\\n\\\\n* [Overview](/oss/python/langgraph/overview)\\\\n\\\\n##### Get started\\\\n\\\\n* [Install](/oss/python/langgraph/install)\\\\n* [Quickstart](/oss/python/langgraph/quickstart)\\\\n* [Local server](/oss/python/langgraph/local-server)\\\\n* [Changelog](https://docs.langchain.com/oss/python/releases/changelog)\\\\n* [Thinking in LangGraph](/oss/python/langgraph/thinking-in-langgraph)\\\\n* [Workflows + agents](/oss/python/langgraph/workflows-agents)\\\\n\\\\n##### Capabilities\\\\n\\\\n* [Persistence](/oss/python/langgraph/persistence)\\\\n* [Durable execution](/oss/python/langgraph/durable-execution)\\\\n* [Streaming](/oss/python/langgraph/streaming)\\\\n* [Interrupts](/oss/python/langgraph/interrupts)\\\\n* [Time travel](/oss/python/langgraph/use-time-travel)\\\\n* [Memory](/oss/python/langgraph/add-memory)\\\\n* [Subgraphs](/oss/python/langgraph/use-subgraphs)\\\\n\\\\n##### Production\\\\n\\\\n* [Application structure](/oss/python/langgraph/application-structure)\\\\n* [Test](/oss/python/langgraph/test)\\\\n* [LangSmith Studio](/oss/python/langgraph/studio)\\\\n* [Agent Chat UI](/oss/python/langgraph/ui)\\\\n* [LangSmith Deployment](/oss/python/langgraph/deploy)\\\\n* [LangSmith Observability](/oss/python/langgraph/observability)\\\\n\\\\n##### LangGraph APIs\\\\n\\\\n* [Runtime](/oss/python/langgraph/pregel)\\\\n\\\\n* [Install](#install)\\\\n* [Core benefits](#core-benefits)\\\\n* [LangGraph ecosystem](#langgraph-ecosystem)\\\\n* [Acknowledgements](#acknowledgements)\\\\n\\\\n# LangGraph overview\\\\n\\\\nGain control with LangGraph to design agents that reliably handle complex tasks\\\\n\\\\nTrusted by companies shaping the future of agents‚Äî including Klarna, Replit, Elastic, and more‚Äî LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/python/langchain/models) and [tools](/oss/python/langchain/tools). We will commonly use [LangChain](/oss/python/langchain/overview) components throughout the documentation to integrate models and tools, but you don‚Äôt need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain‚Äôs [agents](/oss/python/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops. LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\\\n\\\\n## [\\u200b](#install) Install\\\\n\\\\nCopy\\\\n\\\\n```\\\\npip install -U langgraph pip  install -U  langgraph\\\\n```\\\\n\\\\nThen, create a simple hello world example:\\\\n\\\\nCopy\\\\n\\\\n```\\\\nfrom langgraph.graph import StateGraph, MessagesState, START, END from langgraph.graph import StateGraph, MessagesState, START, END def mock_llm(state: MessagesState): def  mock_llm(state: MessagesState): return {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"ai\\\\\", \\\\\"content\\\\\": \\\\\"hello world\\\\\"}]}  return {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"ai\\\\\", \\\\\"content\\\\\": \\\\\"hello world\\\\\"}]} graph = StateGraph(MessagesState) graph = StateGraph(MessagesState)graph.add_node(mock_llm)graph.add_node(mock_llm)graph.add_edge(START, \\\\\"mock_llm\\\\\")graph.add_edge(START, \\\\\"mock_llm\\\\\")graph.add_edge(\\\\\"mock_llm\\\\\", END)graph.add_edge(\\\\\"mock_llm\\\\\", END)graph = graph.compile() graph = graph.compile() graph.invoke({\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"hi!\\\\\"}]})graph.invoke({\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"hi!\\\\\"}]})\\\\n```\\\\n\\\\n## [\\u200b](#core-benefits) Core benefits\\\\n\\\\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\\\n\\\\n* [Durable execution](/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\\\n* [Human-in-the-loop](/oss/python/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\\\\n* [Comprehensive memory](/oss/python/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\\\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\\\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\\\n\\\\n## [\\u200b](#langgraph-ecosystem) LangGraph ecosystem\\\\n\\\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\\\n\\\\n[## LangSmith\\\\n\\\\nTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.](http://www.langchain.com/langsmith)[## LangSmith Agent Server\\\\n\\\\nDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äî and iterate quickly with visual prototyping in Studio.](/langsmith/agent-server)[## LangChain\\\\n\\\\nProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.](/oss/python/langchain/overview)\\\\n\\\\n## [\\u200b](#acknowledgements) Acknowledgements\\\\n\\\\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain. \\\\n\\\\n---\\\\n\\\\n[Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\\\\n\\\\n[Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\\\\n\\\\nWas this page helpful?\\\\n\\\\n[Install LangGraph](/oss/python/langgraph/install)\"}, {\"url\": \"https://www.reddit.com/r/AI_Agents/comments/1l4uq7v/why_use_langgraph/\", \"title\": \"Why use LangGraph? : r/AI_Agents - Reddit\", \"content\": \"LangGraph emphasizes graph-based workflows and state management, making it ideal for complex applications with sophisticated logic and memory persistence.\", \"score\": 0.78314424, \"raw_content\": null}, {\"url\": \"https://www.langchain.com/langgraph\", \"title\": \"LangGraph: Agent Orchestration Framework for Reliable AI Agents\", \"content\": \"[![Image 1](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270df09334914882b88_Frame%209.svg) Observability Debug and monitor in-depth traces](https://www.langchain.com/langsmith/observability)[![Image 2](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270f9e8de1d368764a8_Frame%20206.svg) Evaluation Iterate on prompts and models](https://www.langchain.com/langsmith/evaluation)[![Image 3](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e2709eef27fc61465416_Frame%20100039.svg) Deployment Ship and scale agents in production](https://www.langchain.com/langsmith/deployment)[![Image 4](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/692e7522b3ef627cb0ec1155_Centralized%20management.svg) Agent Builder New Build no-code agents](https://www.langchain.com/langsmith/agent-builder). [![Image 5](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e27023585643370a6471_icons.svg) LangChain Quick start agents with any model provider](https://www.langchain.com/langchain)[![Image 6](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270417338c7f027082d_d035ce400e48f9bc4dd0578d0e3e3211_icons-1.svg) LangGraph Build custom agents with low-level control](https://www.langchain.com/langgraph)[![Image 7](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68f20863b71dbae1af829979_DeepAgents.svg) Deep Agents New Use planning, memory, and sub-agents for complex, long-running tasks](https://docs.langchain.com/oss/python/deepagents/overview#deep-agents-overview). [Start building](https://docs.langchain.com/oss/python/langgraph/overview). ![Image 9](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 10](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 11](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 13](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 14](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 16](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 18](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 19](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 20](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 21](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 23](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 24](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 26](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 28](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 29](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 30](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 31](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 33](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 34](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 36](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 38](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 39](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 40](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 41](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 43](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 44](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 46](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 48](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 49](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b080e4b3ca12dc5d5d439_Langgraph%20UI-2.webp). [Read a conceptual guide](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop). ![Image 50](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c93d559216bb904fe85a8_gif7%20(1).gif). [Learn how to add human-in-the-loop](https://docs.langchain.com/oss/python/langgraph/interrupts). ![Image 51](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68663ca715b9bd5d707bee71_Modified-Human-in-the-loop_white.gif). ![Image 52](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b895f061b6f892568ff6_Modified-Customizable-Agent-Architectures_white.gif). [See different agent architectures](https://docs.langchain.com/oss/python/langgraph/workflows-agents). [Learn about agent memory](https://docs.langchain.com/oss/python/langgraph/add-memory). ![Image 54](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b8b7df5aea00af3a4554_Modified-Streaming-intermediate-steps_white.gif). [See how to use streaming](https://docs.langchain.com/oss/python/langgraph/streaming). [![Image 57](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/678e35d6c553c4fb20f9b753_Frame%2099644.webp)![Image 58](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6787ae0bce5c99dd808545ce_card%202.webp)](https://academy.langchain.com/courses/intro-to-langgraph). ![Image 64](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png). ![Image 70](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png). ![Image 71](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c6a38f9c53ec71f5fc73de_langchain-word.svg).\", \"score\": 0.7730283, \"raw_content\": \"LangGraph: Agent Orchestration Framework for Reliable AI Agents\\\\n===============\\\\n\\\\n[](https://www.langchain.com/)\\\\n\\\\nProducts\\\\n\\\\nLangSmith\\\\n\\\\n[![Image 1](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270df09334914882b88_Frame%209.svg) Observability Debug and monitor in-depth traces](https://www.langchain.com/langsmith/observability)[![Image 2](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270f9e8de1d368764a8_Frame%20206.svg) Evaluation Iterate on prompts and models](https://www.langchain.com/langsmith/evaluation)[![Image 3](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e2709eef27fc61465416_Frame%20100039.svg) Deployment Ship and scale agents in production](https://www.langchain.com/langsmith/deployment)[![Image 4](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/692e7522b3ef627cb0ec1155_Centralized%20management.svg) Agent Builder New Build no-code agents](https://www.langchain.com/langsmith/agent-builder)\\\\n\\\\nOpen Source Frameworks\\\\n\\\\n[![Image 5](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e27023585643370a6471_icons.svg) LangChain Quick start agents with any model provider](https://www.langchain.com/langchain)[![Image 6](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270417338c7f027082d_d035ce400e48f9bc4dd0578d0e3e3211_icons-1.svg) LangGraph Build custom agents with low-level control](https://www.langchain.com/langgraph)[![Image 7](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68f20863b71dbae1af829979_DeepAgents.svg) Deep Agents New Use planning, memory, and sub-agents for complex, long-running tasks](https://docs.langchain.com/oss/python/deepagents/overview#deep-agents-overview)\\\\n\\\\nLearn\\\\n\\\\nResources\\\\n\\\\n[Blog](https://blog.langchain.com/)[2026 State of Agent Engineering](https://www.langchain.com/state-of-agent-engineering)[Customer Stories](https://www.langchain.com/customers)[Guides](https://www.langchain.com/resources)[Changelog](https://changelog.langchain.com/)[Trust Center](https://trust.langchain.com/)[Support](http://support.langchain.com/)\\\\n\\\\nHow-To\\\\n\\\\n[LangChain Academy](https://academy.langchain.com/)[YouTube](https://www.youtube.com/@LangChain)[Documentation](https://docs.langchain.com/)\\\\n\\\\nCommunity\\\\n\\\\n[LangSmith for Startups](https://www.langchain.com/startups)[Events](https://luma.com/langchain?k=c)[Community](https://www.langchain.com/community)[Community Forum](https://forum.langchain.com/)[Slack](https://www.langchain.com/join-community)\\\\n\\\\n[Docs](https://docs.langchain.com/)\\\\n\\\\nCompany\\\\n\\\\n[About](https://www.langchain.com/about)[Careers](https://www.langchain.com/careers)[Partners](https://www.langchain.com/langchain-partner-network)\\\\n\\\\n[Pricing](https://www.langchain.com/pricing)\\\\n\\\\n[Get a demo](https://www.langchain.com/contact-sales)\\\\n\\\\n[Try LangSmith](https://smith.langchain.com/)\\\\n\\\\nBalance agent control with agency\\\\n=================================\\\\n\\\\nGain control with LangGraph to design agents \\\\n\\\\nthat reliably handle complex tasks.\\\\n\\\\n[Start building](https://docs.langchain.com/oss/python/langgraph/overview)\\\\n\\\\n![Image 8](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66db8c2317fe5b9ad2b84ea0_lcacademylogo.png)\\\\nIntroduction to LangGraph\\\\n-------------------------\\\\n\\\\nLearn the basics of LangGraph in this LangChain Academy Course. You\\'ll learn how to build agents that automate real-world tasks with LangGraph orchestration.\\\\n\\\\n[Enroll for free](https://academy.langchain.com/courses/intro-to-langgraph)[Book enterprise training](https://airtable.com/appGjCAN6126Jm7K8/pagNAp7niHQzRH8zk/form)\\\\n\\\\nTrusted by companies shaping the future of agents\\\\n-------------------------------------------------\\\\n\\\\n[See LangGraph use cases in production](https://www.langchain.com/built-with-langgraph)\\\\n\\\\n![Image 9](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\\\n\\\\n![Image 10](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\\\n\\\\n![Image 11](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\\\n\\\\n![Image 12](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\\\n\\\\n![Image 13](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\\\n\\\\n![Image 14](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\\\n\\\\n![Image 15](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\\\n\\\\n![Image 16](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\\\n\\\\n![Image 17](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\\\n\\\\n![Image 18](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\\\n\\\\n![Image 19](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\\\n\\\\n![Image 20](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\\\n\\\\n![Image 21](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\\\n\\\\n![Image 22](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\\\n\\\\n![Image 23](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\\\n\\\\n![Image 24](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\\\n\\\\n![Image 25](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\\\n\\\\n![Image 26](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\\\n\\\\n![Image 27](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\\\n\\\\n![Image 28](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\\\n\\\\n![Image 29](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\\\n\\\\n![Image 30](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\\\n\\\\n![Image 31](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\\\n\\\\n![Image 32](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\\\n\\\\n![Image 33](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\\\n\\\\n![Image 34](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\\\n\\\\n![Image 35](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\\\n\\\\n![Image 36](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\\\n\\\\n![Image 37](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\\\n\\\\n![Image 38](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\\\n\\\\n![Image 39](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\\\n\\\\n![Image 40](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\\\n\\\\n![Image 41](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\\\n\\\\n![Image 42](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\\\n\\\\n![Image 43](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\\\n\\\\n![Image 44](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\\\n\\\\n![Image 45](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\\\n\\\\n![Image 46](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\\\n\\\\n![Image 47](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\\\n\\\\n![Image 48](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\\\n\\\\n![Image 49](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b080e4b3ca12dc5d5d439_Langgraph%20UI-2.webp)\\\\n\\\\nControllable cognitive architecture for any task\\\\n------------------------------------------------\\\\n\\\\nLangGraph\\'s flexible framework supports diverse control flows ‚Äì single agent, multi-agent, hierarchical, sequential ‚Äì and robustly handles realistic, complex scenarios. \\\\n\\\\nEnsure reliability with easy-to-add moderation and quality loops that prevent agents from veering off course.\\\\n\\\\nUse LangGraph Platform to templatize your cognitive architecture so that tools, prompts, and models are easily configurable with LangGraph Platform Assistants.\\\\n\\\\n[See the docs](https://langchain-ai.github.io/langgraph/)\\\\n\\\\nThousands of companies build AI apps better with LangChain products.\\\\n--------------------------------------------------------------------\\\\n\\\\nRead our select customer stories.\\\\n\\\\nDesigned for human-agent collaboration\\\\n--------------------------------------\\\\n\\\\nWith built-in statefulness, LangGraph agents seamlessly collaborate with humans by writing drafts for review and awaiting approval before acting. Easily inspect the agent‚Äôs actions and \\\\\"time-travel\\\\\" to roll back and take a different action to correct course.\\\\n\\\\n[Read a conceptual guide](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop)\\\\n\\\\n![Image 50](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c93d559216bb904fe85a8_gif7%20(1).gif)\\\\n\\\\nHow does LangGraph help?\\\\n------------------------\\\\n\\\\nGuide, moderate, and control your agent with human-in-the-loop\\\\n--------------------------------------------------------------\\\\n\\\\nPrevent agents from veering off course with easy-to-add moderation and quality controls. Add human-in-the-loop checks to steer and approve agent actions.\\\\n\\\\n[Learn how to add human-in-the-loop](https://docs.langchain.com/oss/python/langgraph/interrupts)\\\\n\\\\n![Image 51](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68663ca715b9bd5d707bee71_Modified-Human-in-the-loop_white.gif)\\\\n\\\\n![Image 52](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b895f061b6f892568ff6_Modified-Customizable-Agent-Architectures_white.gif)\\\\n\\\\nBuild expressive, customizable agent workflows\\\\n----------------------------------------------\\\\n\\\\nLangGraph‚Äôs low-level primitives provide the flexibility needed to create fully customizable agents. Design diverse control flows ‚Äî single, multi-agent, hierarchical ‚Äî all using one framework.\\\\n\\\\n[See different agent architectures](https://docs.langchain.com/oss/python/langgraph/workflows-agents)\\\\n\\\\nPersist context for long-term interactions\\\\n------------------------------------------\\\\n\\\\nLangGraph‚Äôs built-in memory stores conversation histories and maintains context over time, enabling rich, personalized interactions across sessions.\\\\n\\\\n[Learn about agent memory](https://docs.langchain.com/oss/python/langgraph/add-memory)\\\\n\\\\n![Image 53](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/684b39acf796422a803c3a03_Memory-GIF-edited.gif)\\\\n\\\\n![Image 54](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b8b7df5aea00af3a4554_Modified-Streaming-intermediate-steps_white.gif)\\\\n\\\\nFirst-class streaming for better UX design\\\\n------------------------------------------\\\\n\\\\nBridge user expectations and agent capabilities with native token-by-token streaming, showing agent reasoning and actions in real time.\\\\n\\\\n[See how to use streaming](https://docs.langchain.com/oss/python/langgraph/streaming)\\\\n\\\\n![Image 55](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c57f274b66a77e2a26b82_CleanShot2024-06-26at17.08.03-ezgif.com-video-to-gif-converter.gif)\\\\n\\\\nFirst class streaming support for better UX design\\\\n--------------------------------------------------\\\\n\\\\nBridge user expectations and agent capabilities with native token-by-token streaming and streaming of intermediate steps, helpful for showing agent reasoning and actions back to the user as they happen. Use LangGraph Platform\\'s API to deliver dynamic and interactive user experiences.\\\\n\\\\n[Learn more](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/)\\\\n\\\\n![Image 56](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66db8c2317fe5b9ad2b84ea0_lcacademylogo.png)\\\\n\\\\nIntroduction to LangGraph\\\\n-------------------------\\\\n\\\\nLearn the basics of LangGraph in this LangChain Academy Course. You\\'ll learn how to build agents that automate real-world tasks with LangGraph orchestration.\\\\n\\\\n[Enroll for free](https://academy.langchain.com/courses/intro-to-langgraph)\\\\n\\\\n[![Image 57](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/678e35d6c553c4fb20f9b753_Frame%2099644.webp)![Image 58](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6787ae0bce5c99dd808545ce_card%202.webp)](https://academy.langchain.com/courses/intro-to-langgraph)\\\\n\\\\nDeploy agents at scale, monitor carefully, iterate boldly\\\\n---------------------------------------------------------\\\\n\\\\nDesign agent-driven user experiences with LangGraph Platform\\'s APIs. Quickly deploy and scale your application with infrastructure built for agents. Choose from multiple deployment options.\\\\n\\\\n### Fault-tolerant scalability\\\\n\\\\nHandle large workloads gracefully with horizontally-scaling servers, task queues, and built-in persistence. Enhance resilience with intelligent caching and automated retries.\\\\n\\\\n### Dynamic APIs for designing agent experience\\\\n\\\\nCraft personalized user experiences with APIs featuring long-term memory to recall information across conversation sessions. Track, update, and rewind your app\\'s state for easy human steering and interaction. Kick off long-running background jobs for research-style or multi-step work.\\\\n\\\\n### Integrated developer experience\\\\n\\\\nSimplify prototyping, debugging, and sharing of agents in our visual LangGraph Studio. Deploy your application with 1-click deploy with our SaaS offering or within your own VPC. Then, monitor app performance with LangSmith.\\\\n\\\\n### Developers trust LangGraph to build reliable agents\\\\n\\\\nLangGraph helps teams of all sizes, across all industries, build reliable agents ready for production.\\\\n\\\\n[Hear how industry leaders use LangGraph](https://www.langchain.com/built-with-langgraph)\\\\n\\\\n![Image 59](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308aea1371b447cc4af9_elastic-ar21.png)\\\\n\\\\n‚ÄúLangChain is streets ahead with what they\\'ve put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads ‚Äî from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.‚Äù\\\\n\\\\n![Image 60](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b26a1b4576291d6a9335b_garrett%20spong%201.webp)\\\\n\\\\nGarrett Spong\\\\n\\\\nPrincipal SWE \\\\n\\\\n![Image 61](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679de9dc4e7bee218d4b058_Norwegian-Cruise-Line-Logo%202-2.webp)\\\\n\\\\n‚ÄúLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent\\'s thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.‚Äù\\\\n\\\\n![Image 62](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b265bed5f5a9d26d6b7d6_andres%20torres%201.webp)\\\\n\\\\nAndres Torres\\\\n\\\\nSr. Solutions Architect\\\\n\\\\n![Image 63](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e1baf7ea357d0763cde1_ally-bank%201-2.png)\\\\n\\\\n‚ÄúAs Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments with multi-actor agentic workflows. We are committed to deepening our partnership with LangChain.‚Äù\\\\n\\\\n‚ÄúAs Ally advances its exploration of Generative AI,\\\\n\\\\n![Image 64](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png)\\\\n\\\\nSathish Muthukrishnan\\\\n\\\\nChief Information, Data and Digital Officer\\\\n\\\\n![Image 65](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308aea1371b447cc4af9_elastic-ar21.png)\\\\n\\\\n‚ÄúLangChain is streets ahead with what they\\'ve put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads ‚Äî from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.‚Äù\\\\n\\\\n![Image 66](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b26a1b4576291d6a9335b_garrett%20spong%201.webp)\\\\n\\\\nGarrett Spong\\\\n\\\\nPrincipal SWE \\\\n\\\\n![Image 67](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679de9dc4e7bee218d4b058_Norwegian-Cruise-Line-Logo%202-2.webp)\\\\n\\\\n‚ÄúLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent\\'s thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.‚Äù\\\\n\\\\n![Image 68](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b265bed5f5a9d26d6b7d6_andres%20torres%201.webp)\\\\n\\\\nAndres Torres\\\\n\\\\nSr. Solutions Architect\\\\n\\\\n![Image 69](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e1baf7ea357d0763cde1_ally-bank%201-2.png)\\\\n\\\\n‚ÄúAs Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments with multi-actor agentic workflows. We are committed to deepening our partnership with LangChain.‚Äù\\\\n\\\\n‚ÄúAs Ally advances its exploration of Generative AI,\\\\n\\\\n![Image 70](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png)\\\\n\\\\nSathish Muthukrishnan\\\\n\\\\nChief Information, Data and Digital Officer\\\\n\\\\nLangGraph FAQs\\\\n--------------\\\\n\\\\nHow is LangGraph different from other agent frameworks?\\\\n\\\\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company‚Äôs needs. LangGraph provides a more expressive framework to handle companies‚Äô unique tasks without restricting users to a single black-box cognitive architecture.\\\\n\\\\nDoes LangGraph impact the performance of my app?\\\\n\\\\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\\\\n\\\\nIs LangGraph open source? Is it free?\\\\n\\\\nYes. LangGraph is an MIT-licensed open-source library and is free to use.\\\\n\\\\nReady to start shipping reliable agents faster?\\\\n-----------------------------------------------\\\\n\\\\nGet started with tools from the LangChain product suite for every step of the agent development lifecycle.\\\\n\\\\n[Talk to sales](https://www.langchain.com/contact-sales)[Sign Up](https://smith.langchain.com/)\\\\n\\\\nProducts\\\\n\\\\n[LangChain](https://www.langchain.com/langchain)[LangGraph](https://www.langchain.com/langgraph)[LangSmith Observability](https://www.langchain.com/langsmith/observability)[LangSmith Evaluation](https://www.langchain.com/langsmith/evaluation)[LangSmith Deployment](https://www.langchain.com/langsmith/deployment)\\\\n\\\\nResources\\\\n\\\\n[Guides](https://www.langchain.com/resources)[Blog](https://blog.langchain.com/)[Customer Stories](https://www.langchain.com/customers)[LangChain Academy](https://academy.langchain.com/)[Community](https://www.langchain.com/join-community)[Events](https://lu.ma/langchain)[Changelog](https://changelog.langchain.com/)[Docs](http://docs.langchain.com/)[Support](https://support.langchain.com/)\\\\n\\\\nCompany\\\\n\\\\n[About](https://www.langchain.com/about)[Careers](https://www.langchain.com/careers)[X](https://twitter.com/LangChain)[LinkedIn](https://www.linkedin.com/company/langchain/)[YouTube](https://www.youtube.com/@LangChain)[Marketing Assets](https://drive.google.com/drive/folders/17xybjzmVBdsQA-VxouuGLxF6bDsHDe80?usp=sharing)[Security](https://trust.langchain.com/)\\\\n\\\\nSign up for our newsletter to stay up to date\\\\n\\\\nThank you! Your submission has been received!\\\\n\\\\nOops! Something went wrong while submitting the form.\\\\n\\\\n![Image 71](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c6a38f9c53ec71f5fc73de_langchain-word.svg)\\\\n\\\\n[All systems operational](https://status.smith.langchain.com/)\\\\n\\\\n[Responsible Disclosure Policy](https://www.langchain.com/responsible-disclosure-policy)[Privacy Policy](https://www.langchain.com/privacy-policy)[Terms of Service](https://www.langchain.com/terms-of-service)\"}, {\"url\": \"https://github.com/langchain-ai/langgraph\", \"title\": \"langchain-ai/langgraph: Build resilient language agents as graphs.\", \"content\": \"[Skip to content](https://github.com/langchain-ai/langgraph#start-of-content). [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.Dismiss alert. [MIT license](https://github.com/langchain-ai/langgraph/blob/main/LICENSE). *   [Code](https://github.com/langchain-ai/langgraph). *   [Issues 179](https://github.com/langchain-ai/langgraph/issues). *   [Pull requests 126](https://github.com/langchain-ai/langgraph/pulls). *   [Actions](https://github.com/langchain-ai/langgraph/actions). *   [Projects 0](https://github.com/langchain-ai/langgraph/projects). *   [Security 3](https://github.com/langchain-ai/langgraph/security). *   [Insights](https://github.com/langchain-ai/langgraph/pulse). *   [Code](https://github.com/langchain-ai/langgraph). *   [Issues](https://github.com/langchain-ai/langgraph/issues). *   [Pull requests](https://github.com/langchain-ai/langgraph/pulls). *   [Actions](https://github.com/langchain-ai/langgraph/actions). *   [Projects](https://github.com/langchain-ai/langgraph/projects). *   [Security](https://github.com/langchain-ai/langgraph/security). *   [Insights](https://github.com/langchain-ai/langgraph/pulse). [](https://github.com/langchain-ai/langgraph/branches)[](https://github.com/langchain-ai/langgraph/tags). Please comment here if you encounter any issues\\\\\")[#6720](https://github.com/langchain-ai/langgraph/pull/6720) | Jan 26, 2026 |. *   [README](https://github.com/langchain-ai/langgraph#). *   [Code of conduct](https://github.com/langchain-ai/langgraph#). *   [Contributing](https://github.com/langchain-ai/langgraph#). *   [MIT license](https://github.com/langchain-ai/langgraph#). *   [Security](https://github.com/langchain-ai/langgraph#). [](https://github.com/langchain-ai/langgraph#get-started). [](https://github.com/langchain-ai/langgraph#core-benefits). [](https://github.com/langchain-ai/langgraph#langgraphs-ecosystem). See the [JS repo](https://github.com/langchain-ai/langgraphjs) and the [JS docs](https://docs.langchain.com/oss/javascript/langgraph/overview). [](https://github.com/langchain-ai/langgraph#additional-resources). [](https://github.com/langchain-ai/langgraph#acknowledgements). [Readme](https://github.com/langchain-ai/langgraph#readme-ov-file). [MIT license](https://github.com/langchain-ai/langgraph#MIT-1-ov-file). [Code of conduct](https://github.com/langchain-ai/langgraph#coc-ov-file). [Contributing](https://github.com/langchain-ai/langgraph#contributing-ov-file). [Security policy](https://github.com/langchain-ai/langgraph#security-ov-file). [Please reload this page](https://github.com/langchain-ai/langgraph). [Activity](https://github.com/langchain-ai/langgraph/activity). [Custom properties](https://github.com/langchain-ai/langgraph/custom-properties). [**24.8k** stars](https://github.com/langchain-ai/langgraph/stargazers). [**142** watching](https://github.com/langchain-ai/langgraph/watchers). [**4.3k** forks](https://github.com/langchain-ai/langgraph/forks). [Releases 466](https://github.com/langchain-ai/langgraph/releases). [langgraph==1.0.9 Latest Feb 19, 2026](https://github.com/langchain-ai/langgraph/releases/tag/1.0.9). [+ 465 releases](https://github.com/langchain-ai/langgraph/releases). [Used by 36k](https://github.com/langchain-ai/langgraph/network/dependents). [Contributors 286](https://github.com/langchain-ai/langgraph/graphs/contributors). [+ 272 contributors](https://github.com/langchain-ai/langgraph/graphs/contributors). *   [Python 99.3%](https://github.com/langchain-ai/langgraph/search?l=python).\", \"score\": 0.65146625, \"raw_content\": \"GitHub - langchain-ai/langgraph: Build resilient language agents as graphs.\\\\n===============\\\\n\\\\n[Skip to content](https://github.com/langchain-ai/langgraph#start-of-content)\\\\nNavigation Menu\\\\n---------------\\\\n\\\\nToggle navigation\\\\n\\\\n[](https://github.com/)\\\\n\\\\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flanggraph)\\\\n\\\\nAppearance settings\\\\n\\\\n*   \\\\nPlatform\\\\n\\\\n    *   \\\\nAI CODE CREATION\\\\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\\\\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\\\\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\\\\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\\\\n\\\\n    *   \\\\nDEVELOPER WORKFLOWS\\\\n        *   [Actions Automate any workflow](https://github.com/features/actions)\\\\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\\\\n        *   [Issues Plan and track work](https://github.com/features/issues)\\\\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\\\\n\\\\n    *   \\\\nAPPLICATION SECURITY\\\\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\\\\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\\\\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\\\\n\\\\n    *   \\\\nEXPLORE\\\\n        *   [Why GitHub](https://github.com/why-github)\\\\n        *   [Documentation](https://docs.github.com/)\\\\n        *   [Blog](https://github.blog/)\\\\n        *   [Changelog](https://github.blog/changelog)\\\\n        *   [Marketplace](https://github.com/marketplace)\\\\n\\\\n[View all features](https://github.com/features)\\\\n\\\\n*   \\\\nSolutions\\\\n\\\\n    *   \\\\nBY COMPANY SIZE\\\\n        *   [Enterprises](https://github.com/enterprise)\\\\n        *   [Small and medium teams](https://github.com/team)\\\\n        *   [Startups](https://github.com/enterprise/startups)\\\\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\\\\n\\\\n    *   \\\\nBY USE CASE\\\\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\\\\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\\\\n        *   [DevOps](https://github.com/solutions/use-case/devops)\\\\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\\\\n        *   [View all use cases](https://github.com/solutions/use-case)\\\\n\\\\n    *   \\\\nBY INDUSTRY\\\\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\\\\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\\\\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\\\\n        *   [Government](https://github.com/solutions/industry/government)\\\\n        *   [View all industries](https://github.com/solutions/industry)\\\\n\\\\n[View all solutions](https://github.com/solutions)\\\\n\\\\n*   \\\\nResources\\\\n\\\\n    *   \\\\nEXPLORE BY TOPIC\\\\n        *   [AI](https://github.com/resources/articles?topic=ai)\\\\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\\\\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\\\\n        *   [Security](https://github.com/resources/articles?topic=security)\\\\n        *   [View all topics](https://github.com/resources/articles)\\\\n\\\\n    *   \\\\nEXPLORE BY TYPE\\\\n        *   [Customer stories](https://github.com/customer-stories)\\\\n        *   [Events & webinars](https://github.com/resources/events)\\\\n        *   [Ebooks & reports](https://github.com/resources/whitepapers)\\\\n        *   [Business insights](https://github.com/solutions/executive-insights)\\\\n        *   [GitHub Skills](https://skills.github.com/)\\\\n\\\\n    *   \\\\nSUPPORT & SERVICES\\\\n        *   [Documentation](https://docs.github.com/)\\\\n        *   [Customer support](https://support.github.com/)\\\\n        *   [Community forum](https://github.com/orgs/community/discussions)\\\\n        *   [Trust center](https://github.com/trust-center)\\\\n        *   [Partners](https://github.com/partners)\\\\n\\\\n*   \\\\nOpen Source\\\\n\\\\n    *   \\\\nCOMMUNITY\\\\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\\\\n\\\\n    *   \\\\nPROGRAMS\\\\n        *   [Security Lab](https://securitylab.github.com/)\\\\n        *   [Maintainer Community](https://maintainers.github.com/)\\\\n        *   [Accelerator](https://github.com/accelerator)\\\\n        *   [Archive Program](https://archiveprogram.github.com/)\\\\n\\\\n    *   \\\\nREPOSITORIES\\\\n        *   [Topics](https://github.com/topics)\\\\n        *   [Trending](https://github.com/trending)\\\\n        *   [Collections](https://github.com/collections)\\\\n\\\\n*   \\\\nEnterprise\\\\n\\\\n    *   \\\\nENTERPRISE SOLUTIONS\\\\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\\\\n\\\\n    *   \\\\nAVAILABLE ADD-ONS\\\\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\\\\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\\\\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\\\\n\\\\n*   [Pricing](https://github.com/pricing)\\\\n\\\\nSearch or jump to...\\\\n\\\\nSearch code, repositories, users, issues, pull requests...\\\\n==========================================================\\\\n\\\\n Search  \\\\n\\\\nClear\\\\n\\\\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\\\\n\\\\nProvide feedback\\\\n================\\\\n\\\\nWe read every piece of feedback, and take your input very seriously.\\\\n\\\\n- [x] Include my email address so I can be contacted \\\\n\\\\n Cancel  Submit feedback \\\\n\\\\nSaved searches\\\\n==============\\\\n\\\\nUse saved searches to filter your results more quickly\\\\n------------------------------------------------------\\\\n\\\\nName \\\\n\\\\nQuery \\\\n\\\\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\\\\n\\\\n Cancel  Create saved search \\\\n\\\\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flanggraph)\\\\n\\\\n[Sign up](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=langchain-ai%2Flanggraph)\\\\n\\\\nAppearance settings\\\\n\\\\nResetting focus\\\\n\\\\nYou signed in with another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.Dismiss alert\\\\n\\\\n{{ message }}\\\\n\\\\n[langchain-ai](https://github.com/langchain-ai)/**[langgraph](https://github.com/langchain-ai/langgraph)**Public\\\\n\\\\n*   [Notifications](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)You must be signed in to change notification settings\\\\n*   [Fork 4.3k](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)\\\\n*   [Star 24.8k](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph) \\\\n\\\\nBuild resilient language agents as graphs.\\\\n\\\\n[docs.langchain.com/oss/python/langgraph/](https://docs.langchain.com/oss/python/langgraph/ \\\\\"https://docs.langchain.com/oss/python/langgraph/\\\\\")\\\\n\\\\n### License\\\\n\\\\n[MIT license](https://github.com/langchain-ai/langgraph/blob/main/LICENSE)\\\\n\\\\n[24.8k stars](https://github.com/langchain-ai/langgraph/stargazers)[4.3k forks](https://github.com/langchain-ai/langgraph/forks)[Branches](https://github.com/langchain-ai/langgraph/branches)[Tags](https://github.com/langchain-ai/langgraph/tags)[Activity](https://github.com/langchain-ai/langgraph/activity)\\\\n\\\\n[Star](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)\\\\n\\\\n[Notifications](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)You must be signed in to change notification settings\\\\n\\\\n*   [Code](https://github.com/langchain-ai/langgraph)\\\\n*   [Issues 179](https://github.com/langchain-ai/langgraph/issues)\\\\n*   [Pull requests 126](https://github.com/langchain-ai/langgraph/pulls)\\\\n*   [Actions](https://github.com/langchain-ai/langgraph/actions)\\\\n*   [Projects 0](https://github.com/langchain-ai/langgraph/projects)\\\\n*   [Security 3](https://github.com/langchain-ai/langgraph/security)\\\\n*   [Insights](https://github.com/langchain-ai/langgraph/pulse)\\\\n\\\\nAdditional navigation options\\\\n\\\\n*   [Code](https://github.com/langchain-ai/langgraph)\\\\n*   [Issues](https://github.com/langchain-ai/langgraph/issues)\\\\n*   [Pull requests](https://github.com/langchain-ai/langgraph/pulls)\\\\n*   [Actions](https://github.com/langchain-ai/langgraph/actions)\\\\n*   [Projects](https://github.com/langchain-ai/langgraph/projects)\\\\n*   [Security](https://github.com/langchain-ai/langgraph/security)\\\\n*   [Insights](https://github.com/langchain-ai/langgraph/pulse)\\\\n\\\\nlangchain-ai/langgraph\\\\n======================\\\\n\\\\nmain\\\\n\\\\n[**206**Branches](https://github.com/langchain-ai/langgraph/branches)[**477**Tags](https://github.com/langchain-ai/langgraph/tags)\\\\n\\\\n[](https://github.com/langchain-ai/langgraph/branches)[](https://github.com/langchain-ai/langgraph/tags)\\\\n\\\\nGo to file\\\\n\\\\nCode\\\\n\\\\nOpen more actions menu\\\\n\\\\nFolders and files\\\\n-----------------\\\\n\\\\n| Name | Name | Last commit message | Last commit date |\\\\n| --- | --- | --- | --- |\\\\n| Latest commit ------------- ![Image 1: jkennedyvz](https://avatars.githubusercontent.com/u/65985482?v=4&size=40)![Image 2: claude](https://avatars.githubusercontent.com/u/81847?v=4&size=40) [John Kennedy (jkennedyvz)](https://github.com/langchain-ai/langgraph/commits?author=jkennedyvz) and [Claude (claude)](https://github.com/langchain-ai/langgraph/commits?author=claude) [fix: bump js-yaml to 3.14.2 to resolve](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9)[CVE-2025-64718](https://github.com/advisories/GHSA-mh29-5h37-fv8m \\\\\"CVE-2025-64718\\\\\")[(](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9)[#6879](https://github.com/langchain-ai/langgraph/pull/6879)[)](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9) Open commit details success Feb 19, 2026 [ea20432](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9)¬∑Feb 19, 2026 History ------- [6,521 Commits](https://github.com/langchain-ai/langgraph/commits/main/) Open commit details [](https://github.com/langchain-ai/langgraph/commits/main/)6,521 Commits |\\\\n| [.github](https://github.com/langchain-ai/langgraph/tree/main/.github \\\\\".github\\\\\") | [.github](https://github.com/langchain-ai/langgraph/tree/main/.github \\\\\".github\\\\\") | [chore(docs): Add new redirects file(s) so we can update/add (](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\\\\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\\\\")[#6778](https://github.com/langchain-ai/langgraph/pull/6778)[)](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\\\\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\\\\") | Feb 18, 2026 |\\\\n| [docs](https://github.com/langchain-ai/langgraph/tree/main/docs \\\\\"docs\\\\\") | [docs](https://github.com/langchain-ai/langgraph/tree/main/docs \\\\\"docs\\\\\") | [chore(docs): Add new redirects file(s) so we can update/add (](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\\\\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\\\\")[#6778](https://github.com/langchain-ai/langgraph/pull/6778)[)](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\\\\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\\\\") | Feb 18, 2026 |\\\\n| [examples](https://github.com/langchain-ai/langgraph/tree/main/examples \\\\\"examples\\\\\") | [examples](https://github.com/langchain-ai/langgraph/tree/main/examples \\\\\"examples\\\\\") | [docs: update notebook links and add archival notices for examples (](https://github.com/langchain-ai/langgraph/commit/fbcb8a911b626b4373fd4aff9624124e9532987f \\\\\"docs: update notebook links and add archival notices for examples (#6720) Addresses some comments in #6682 - Update links in notebooks to point to the new documentation location. - Add archival notices indicating that the examples are no longer updated. - Remove some obsolete notebooks that have been moved to the new documentation. Please comment here if you encounter any issues\\\\\")[#6720](https://github.com/langchain-ai/langgraph/pull/6720) | Jan 26, 2026 |\\\\n| [libs](https://github.com/langchain-ai/langgraph/tree/main/libs \\\\\"libs\\\\\") | [libs](https://github.com/langchain-ai/langgraph/tree/main/libs \\\\\"libs\\\\\") | [fix: bump js-yaml to 3.14.2 to resolve](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9 \\\\\"fix: bump js-yaml to 3.14.2 to resolve CVE-2025-64718 (#6879) ## Security Alert Patch Resolves 1 Dependabot security alert (medium severity). ### Package Updated | Package | Old Version | New Version | Strategy | CVE Resolved | |---------|-------------|-------------|----------|--------------| | `js-yaml` | 3.14.1 | 3.14.2 | Lockfile patch (within-range bump) | CVE-2025-64718 | ### CVE Details **CVE-2025-64718** / [GHSA-mh29-5h37-fv8m](https://github.com/advisories/GHSA-mh29-5h37-fv8m) ‚Äî `js-yaml` prototype pollution via YAML merge keys (`&lt;&lt;`). Affects versions &lt; 3.14.2. The vulnerable package is a transitive dev dependency pulled in by `@istanbuljs/load-nyc-config@1.1.0` (a Jest internal). No runtime impact. ### Fix Strategy Lockfile-only patch in `libs/cli/js-examples/yarn.lock`. The `^3.13.1` version range already allows 3.14.2, so no manifest changes were needed. The `js-yaml@^4.1.1` entry (used by `@eslint/eslintrc`) is untouched. ### Verification - [x] Lockfile updated ‚Äî `js-yaml@^3.13.1` now resolves to `3.14.2` - [x] `js-yaml@^4.1.1` entry unchanged (`4.1.1`) - [x] `yarn install --frozen-lockfile` passes ü§ñ Submitted by langster-patch Co-authored-by: Claude Sonnet 4.6 &lt;noreply@anthropic.com&gt;\\\\\")[CVE-2025-64718](https://github.com/advisories/GHSA-mh29-5h37-fv8m \\\\\"CVE-2025-64718\\\\\")[(](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9 \\\\\"fix: bump js-yaml to 3.14.2 to resolve CVE-2025-64718 (#6879) ## Security Alert Patch Resolves 1 Dependabot security alert (medium severity). ### Package Updated | Package | Old Version | New Version | Strategy | CVE Resolved | |---------|-------------|-------------|----------|--------------| | `js-yaml` | 3.14.1 | 3.14.2 | Lockfile patch (within-range bump) | CVE-2025-64718 | ### CVE Details **CVE-2025-64718** / [GHSA-mh29-5h37-fv8m](https://github.com/advisories/GHSA-mh29-5h37-fv8m) ‚Äî `js-yaml` prototype pollution via YAML merge keys (`&lt;&lt;`). Affects versions &lt; 3.14.2. The vulnerable package is a transitive dev dependency pulled in by `@istanbuljs/load-nyc-config@1.1.0` (a Jest internal). No runtime impact. ### Fix Strategy Lockfile-only patch in `libs/cli/js-examples/yarn.lock`. The `^3.13.1` version range already allows 3.14.2, so no manifest changes were needed. The `js-yaml@^4.1.1` entry (used by `@eslint/eslintrc`) is untouched. ### Verification - [x] Lockfile updated ‚Äî `js-yaml@^3.13.1` now resolves to `3.14.2` - [x] `js-yaml@^4.1.1` entry unchanged (`4.1.1`) - [x] `yarn install --frozen-lockfile` passes ü§ñ Submitted by langster-patch Co-authored-by: Claude Sonnet 4.6 &lt;noreply@anthropic.com&gt;\\\\\")[#6879](https://github.com/langchain-ai/langgraph/pull/6879)[)](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9 \\\\\"fix: bump js-yaml to 3.14.2 to resolve CVE-2025-64718 (#6879) ## Security Alert Patch Resolves 1 Dependabot security alert (medium severity). ### Package Updated | Package | Old Version | New Version | Strategy | CVE Resolved | |---------|-------------|-------------|----------|--------------| | `js-yaml` | 3.14.1 | 3.14.2 | Lockfile patch (within-range bump) | CVE-2025-64718 | ### CVE Details **CVE-2025-64718** / [GHSA-mh29-5h37-fv8m](https://github.com/advisories/GHSA-mh29-5h37-fv8m) ‚Äî `js-yaml` prototype pollution via YAML merge keys (`&lt;&lt;`). Affects versions &lt; 3.14.2. The vulnerable package is a transitive dev dependency pulled in by `@istanbuljs/load-nyc-config@1.1.0` (a Jest internal). No runtime impact. ### Fix Strategy Lockfile-only patch in `libs/cli/js-examples/yarn.lock`. The `^3.13.1` version range already allows 3.14.2, so no manifest changes were needed. The `js-yaml@^4.1.1` entry (used by `@eslint/eslintrc`) is untouched. ### Verification - [x] Lockfile updated ‚Äî `js-yaml@^3.13.1` now resolves to `3.14.2` - [x] `js-yaml@^4.1.1` entry unchanged (`4.1.1`) - [x] `yarn install --frozen-lockfile` passes ü§ñ Submitted by langster-patch Co-authored-by: Claude Sonnet 4.6 &lt;noreply@anthropic.com&gt;\\\\\") | Feb 19, 2026 |\\\\n| [.gitignore](https://github.com/langchain-ai/langgraph/blob/main/.gitignore \\\\\".gitignore\\\\\") | [.gitignore](https://github.com/langchain-ai/langgraph/blob/main/.gitignore \\\\\".gitignore\\\\\") | [chore: more cleanup (](https://github.com/langchain-ai/langgraph/commit/d066a321bc5a6ca08c2532873a2759cdce1c4d0a \\\\\"chore: more cleanup (#6667)\\\\\")[#6667](https://github.com/langchain-ai/langgraph/pull/6667)[)](https://github.com/langchain-ai/langgraph/commit/d066a321bc5a6ca08c2532873a2759cdce1c4d0a \\\\\"chore: more cleanup (#6667)\\\\\") | Jan 9, 2026 |\\\\n| [AGENTS.md](https://github.com/langchain-ai/langgraph/blob/main/AGENTS.md \\\\\"AGENTS.md\\\\\") | [AGENTS.md](https://github.com/langchain-ai/langgraph/blob/main/AGENTS.md \\\\\"AGENTS.md\\\\\") | [chore(infra): update `AGENTS.md` for inline code formatting guidelines (](https://github.com/langchain-ai/langgraph/commit/f688b068e7f06111fc69ee70adc1b66c75fa5a93 \\\\\"chore(infra): update `AGENTS.md` for inline code formatting guidelines (#6752)\\\\\") | Feb 5, 2026 |\\\\n| [CLAUDE.md](https://github.com/langchain-ai/langgraph/blob/main/CLAUDE.md \\\\\"CLAUDE.md\\\\\") | [CLAUDE.md](https://github.com/langchain-ai/langgraph/blob/main/CLAUDE.md \\\\\"CLAUDE.md\\\\\") | [chore(infra): update `AGENTS.md` for inline code formatting guidelines (](https://github.com/langchain-ai/langgraph/commit/f688b068e7f06111fc69ee70adc1b66c75fa5a93 \\\\\"chore(infra): update `AGENTS.md` for inline code formatting guidelines (#6752)\\\\\") | Feb 5, 2026 |\\\\n| [LICENSE](https://github.com/langchain-ai/langgraph/blob/main/LICENSE \\\\\"LICENSE\\\\\") | [LICENSE](https://github.com/langchain-ai/langgraph/blob/main/LICENSE \\\\\"LICENSE\\\\\") | [libs: add cli, sdk-py, sdk-js and move core langgraph](https://github.com/langchain-ai/langgraph/commit/1a06d500d4282cfdb2ae9d7748bb570e8162acdf \\\\\"libs: add cli, sdk-py, sdk-js and move core langgraph\\\\\") | Jun 18, 2024 |\\\\n| [Makefile](https://github.com/langchain-ai/langgraph/blob/main/Makefile \\\\\"Makefile\\\\\") | [Makefile](https://github.com/langchain-ai/langgraph/blob/main/Makefile \\\\\"Makefile\\\\\") | [ci: add automated](https://github.com/langchain-ai/langgraph/commit/b3708bd7f66675f7a04bf9950c33d495c49cd843 \\\\\"ci: add automated `uv lock --upgrade` workflow (#5307)\\\\\")`uv lock --upgrade`[workflow (](https://github.com/langchain-ai/langgraph/commit/b3708bd7f66675f7a04bf9950c33d495c49cd843 \\\\\"ci: add automated `uv lock --upgrade` workflow (#5307)\\\\\")[#5307](https://github.com/langchain-ai/langgraph/pull/5307)[)](https://github.com/langchain-ai/langgraph/commit/b3708bd7f66675f7a04bf9950c33d495c49cd843 \\\\\"ci: add automated `uv lock --upgrade` workflow (#5307)\\\\\") | Jul 2, 2025 |\\\\n| [README.md](https://github.com/langchain-ai/langgraph/blob/main/README.md \\\\\"README.md\\\\\") | [README.md](https://github.com/langchain-ai/langgraph/blob/main/README.md \\\\\"README.md\\\\\") | [chore(docs): Update link for LangGraph guides in README (](https://github.com/langchain-ai/langgraph/commit/8cb87eaf7637ca591adc9715fc412e6661e4a77d \\\\\"chore(docs): Update link for LangGraph guides in README (#6680) https://langchain.slack.com/archives/C04GWPE38LV/p1768251150861949\\\\\")[#6680](https://github.com/langchain-ai/langgraph/pull/6680)[)](https://github.com/langchain-ai/langgraph/commit/8cb87eaf7637ca591adc9715fc412e6661e4a77d \\\\\"chore(docs): Update link for LangGraph guides in README (#6680) https://langchain.slack.com/archives/C04GWPE38LV/p1768251150861949\\\\\") | Jan 13, 2026 |\\\\n| View all files |\\\\n\\\\nRepository files navigation\\\\n---------------------------\\\\n\\\\n*   [README](https://github.com/langchain-ai/langgraph#)\\\\n*   [Code of conduct](https://github.com/langchain-ai/langgraph#)\\\\n*   [Contributing](https://github.com/langchain-ai/langgraph#)\\\\n*   [MIT license](https://github.com/langchain-ai/langgraph#)\\\\n*   [Security](https://github.com/langchain-ai/langgraph#)\\\\n\\\\n![Image 3: LangGraph Logo](https://camo.githubusercontent.com/35c8690644d21b455613e70f617eee193be16e02684824dc60be8ab1216eed6c/68747470733a2f2f6c616e67636861696e2d61692e6769746875622e696f2f6c616e6767726170682f7374617469632f776f72646d61726b5f6461726b2e737667)\\\\n\\\\n[![Image 4: Version](https://camo.githubusercontent.com/28715b4724dc05f3ffb3a0cda4069876d0e8ada606267c944bc83639b3ebe5ff/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c616e6767726170682e737667)](https://pypi.org/project/langgraph/)[![Image 5: Downloads](https://camo.githubusercontent.com/81ef74dbbdd86537708ac52c5757df06b12129ea96d7aa92254c8276fee2b3fa/68747470733a2f2f7374617469632e706570792e746563682f62616467652f6c616e6767726170682f6d6f6e7468)](https://pepy.tech/project/langgraph)[![Image 6: Open Issues](https://camo.githubusercontent.com/bcb848d635a6a73f5c1c2be97875ac1369539d5a025be181e6d1f6125933b2d1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6c616e67636861696e2d61692f6c616e676772617068)](https://github.com/langchain-ai/langgraph/issues)[![Image 7: Docs](https://camo.githubusercontent.com/b98c4ce4549448d09f2217965c7d6f2cf39ee6800b2b4c63dfd62080fb5533d8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c7565)](https://docs.langchain.com/oss/python/langgraph/overview)\\\\n\\\\nTrusted by companies shaping the future of agents ‚Äì including Klarna, Replit, Elastic, and more ‚Äì LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents.\\\\n\\\\nGet started\\\\n-----------\\\\n\\\\n[](https://github.com/langchain-ai/langgraph#get-started)\\\\n\\\\nInstall LangGraph:\\\\n\\\\n```\\\\npip install -U langgraph\\\\n```\\\\n\\\\nCreate a simple workflow:\\\\n\\\\nundefinedpython\\\\nfrom langgraph.graph import START, StateGraph\\\\nfrom typing_extensions import TypedDict\\\\n\\\\nclass State(TypedDict):\\\\n    text: str\\\\n\\\\ndef node_a(state: State) -&gt; dict:\\\\n    return {\\\\\"text\\\\\": state[\\\\\"text\\\\\"] + \\\\\"a\\\\\"}\\\\n\\\\ndef node_b(state: State) -&gt; dict:\\\\n    return {\\\\\"text\\\\\": state[\\\\\"text\\\\\"] + \\\\\"b\\\\\"}\\\\n\\\\ngraph = StateGraph(State)\\\\ngraph.add_node(\\\\\"node_a\\\\\", node_a)\\\\ngraph.add_node(\\\\\"node_b\\\\\", node_b)\\\\ngraph.add_edge(START, \\\\\"node_a\\\\\")\\\\ngraph.add_edge(\\\\\"node_a\\\\\", \\\\\"node_b\\\\\")\\\\n\\\\nprint(graph.compile().invoke({\\\\\"text\\\\\": \\\\\"\\\\\"}))\\\\n# {\\'text\\': \\'ab\\'}\\\\nundefined\\\\n\\\\nGet started with the [LangGraph Quickstart](https://docs.langchain.com/oss/python/langgraph/quickstart).\\\\n\\\\nTo quickly build agents with LangChain\\'s `create_agent` (built on LangGraph), see the [LangChain Agents documentation](https://docs.langchain.com/oss/python/langchain/agents).\\\\n\\\\nCore benefits\\\\n-------------\\\\n\\\\n[](https://github.com/langchain-ai/langgraph#core-benefits)\\\\n\\\\nLangGraph provides low-level supporting infrastructure for _any_ long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\\\n\\\\n*   [Durable execution](https://docs.langchain.com/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, automatically resuming from exactly where they left off.\\\\n*   [Human-in-the-loop](https://docs.langchain.com/oss/python/langgraph/interrupts): Seamlessly incorporate human oversight by inspecting and modifying agent state at any point during execution.\\\\n*   [Comprehensive memory](https://docs.langchain.com/oss/python/langgraph/memory): Create truly stateful agents with both short-term working memory for ongoing reasoning and long-term persistent memory across sessions.\\\\n*   [Debugging with LangSmith](http://www.langchain.com/langsmith): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\\\n*   [Production-ready deployment](https://docs.langchain.com/langsmith/app-development): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\\\n\\\\nLangGraph‚Äôs ecosystem\\\\n---------------------\\\\n\\\\n[](https://github.com/langchain-ai/langgraph#langgraphs-ecosystem)\\\\n\\\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\\\n\\\\n*   [LangSmith](http://www.langchain.com/langsmith) ‚Äî Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\\\n*   [LangSmith Deployment](https://docs.langchain.com/langsmith/deployments) ‚Äî Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äî and iterate quickly with visual prototyping in [LangGraph Studio](https://docs.langchain.com/oss/python/langgraph/studio).\\\\n*   [LangChain](https://docs.langchain.com/oss/python/langchain/overview) ‚Äì Provides integrations and composable components to streamline LLM application development.\\\\n\\\\nNote\\\\n\\\\nLooking for the JS version of LangGraph? See the [JS repo](https://github.com/langchain-ai/langgraphjs) and the [JS docs](https://docs.langchain.com/oss/javascript/langgraph/overview).\\\\n\\\\nAdditional resources\\\\n--------------------\\\\n\\\\n[](https://github.com/langchain-ai/langgraph#additional-resources)\\\\n\\\\n*   [Guides](https://docs.langchain.com/oss/python/langgraph/overview): Quick, actionable code snippets for topics such as streaming, adding memory & persistence, and design patterns (e.g. branching, subgraphs, etc.).\\\\n*   [Reference](https://reference.langchain.com/python/langgraph/): Detailed reference on core classes, methods, how to use the graph and checkpointing APIs, and higher-level prebuilt components.\\\\n*   [Examples](https://docs.langchain.com/oss/python/langgraph/agentic-rag): Guided examples on getting started with LangGraph.\\\\n*   [LangChain Forum](https://forum.langchain.com/): Connect with the community and share all of your technical questions, ideas, and feedback.\\\\n*   [LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph): Learn the basics of LangGraph in our free, structured course.\\\\n*   [Case studies](https://www.langchain.com/built-with-langgraph): Hear how industry leaders use LangGraph to ship AI applications at scale.\\\\n\\\\nAcknowledgements\\\\n----------------\\\\n\\\\n[](https://github.com/langchain-ai/langgraph#acknowledgements)\\\\n\\\\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\\\n\\\\nAbout\\\\n-----\\\\n\\\\nBuild resilient language agents as graphs.\\\\n\\\\n[docs.langchain.com/oss/python/langgraph/](https://docs.langchain.com/oss/python/langgraph/ \\\\\"https://docs.langchain.com/oss/python/langgraph/\\\\\")\\\\n\\\\n### Topics\\\\n\\\\n[python](https://github.com/topics/python \\\\\"Topic: python\\\\\")[open-source](https://github.com/topics/open-source \\\\\"Topic: open-source\\\\\")[enterprise](https://github.com/topics/enterprise \\\\\"Topic: enterprise\\\\\")[framework](https://github.com/topics/framework \\\\\"Topic: framework\\\\\")[ai](https://github.com/topics/ai \\\\\"Topic: ai\\\\\")[gemini](https://github.com/topics/gemini \\\\\"Topic: gemini\\\\\")[openai](https://github.com/topics/openai \\\\\"Topic: openai\\\\\")[multiagent](https://github.com/topics/multiagent \\\\\"Topic: multiagent\\\\\")[agents](https://github.com/topics/agents \\\\\"Topic: agents\\\\\")[ai-agents](https://github.com/topics/ai-agents \\\\\"Topic: ai-agents\\\\\")[rag](https://github.com/topics/rag \\\\\"Topic: rag\\\\\")[pydantic](https://github.com/topics/pydantic \\\\\"Topic: pydantic\\\\\")[llm](https://github.com/topics/llm \\\\\"Topic: llm\\\\\")[generative-ai](https://github.com/topics/generative-ai \\\\\"Topic: generative-ai\\\\\")[chatgpt](https://github.com/topics/chatgpt \\\\\"Topic: chatgpt\\\\\")[langchain](https://github.com/topics/langchain \\\\\"Topic: langchain\\\\\")[langgraph](https://github.com/topics/langgraph \\\\\"Topic: langgraph\\\\\")[deepagents](https://github.com/topics/deepagents \\\\\"Topic: deepagents\\\\\")\\\\n\\\\n### Resources\\\\n\\\\n[Readme](https://github.com/langchain-ai/langgraph#readme-ov-file)\\\\n\\\\n### License\\\\n\\\\n[MIT license](https://github.com/langchain-ai/langgraph#MIT-1-ov-file)\\\\n\\\\n### Code of conduct\\\\n\\\\n[Code of conduct](https://github.com/langchain-ai/langgraph#coc-ov-file)\\\\n\\\\n### Contributing\\\\n\\\\n[Contributing](https://github.com/langchain-ai/langgraph#contributing-ov-file)\\\\n\\\\n### Security policy\\\\n\\\\n[Security policy](https://github.com/langchain-ai/langgraph#security-ov-file)\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. [Please reload this page](https://github.com/langchain-ai/langgraph).\\\\n\\\\n[Activity](https://github.com/langchain-ai/langgraph/activity)\\\\n\\\\n[Custom properties](https://github.com/langchain-ai/langgraph/custom-properties)\\\\n\\\\n### Stars\\\\n\\\\n[**24.8k** stars](https://github.com/langchain-ai/langgraph/stargazers)\\\\n\\\\n### Watchers\\\\n\\\\n[**142** watching](https://github.com/langchain-ai/langgraph/watchers)\\\\n\\\\n### Forks\\\\n\\\\n[**4.3k** forks](https://github.com/langchain-ai/langgraph/forks)\\\\n\\\\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flanggraph&report=langchain-ai+%28user%29)\\\\n\\\\n[Releases 466](https://github.com/langchain-ai/langgraph/releases)\\\\n------------------------------------------------------------------\\\\n\\\\n[langgraph==1.0.9 Latest Feb 19, 2026](https://github.com/langchain-ai/langgraph/releases/tag/1.0.9)\\\\n\\\\n[+ 465 releases](https://github.com/langchain-ai/langgraph/releases)\\\\n\\\\n[Used by 36k](https://github.com/langchain-ai/langgraph/network/dependents)\\\\n---------------------------------------------------------------------------\\\\n\\\\n[* ![Image 8: @ry86pkqf74-rgb](https://avatars.githubusercontent.com/u/245083510?s=64&v=4) * ![Image 9: @SURESHBEEKHANI](https://avatars.githubusercontent.com/u/107859372?s=64&v=4) * ![Image 10: @scaleapi](https://avatars.githubusercontent.com/u/21693938?s=64&v=4) * ![Image 11: @rahulsharmavishwakarma](https://avatars.githubusercontent.com/u/93391253?s=64&v=4) * ![Image 12: @Konscig](https://avatars.githubusercontent.com/u/71553656?s=64&v=4) * ![Image 13: @asadullah48](https://avatars.githubusercontent.com/u/161634653?s=64&v=4) * ![Image 14: @Ontotext-AD](https://avatars.githubusercontent.com/u/6817639?s=64&v=4) * ![Image 15: @jason-jaemin-lee](https://avatars.githubusercontent.com/u/226609390?s=64&v=4) + 36,021](https://github.com/langchain-ai/langgraph/network/dependents)\\\\n\\\\n[Contributors 286](https://github.com/langchain-ai/langgraph/graphs/contributors)\\\\n---------------------------------------------------------------------------------\\\\n\\\\n*   [![Image 16: @nfcampos](https://avatars.githubusercontent.com/u/56902?s=64&v=4)](https://github.com/nfcampos)\\\\n*   [![Image 17: @hinthornw](https://avatars.githubusercontent.com/u/13333726?s=64&v=4)](https://github.com/hinthornw)\\\\n*   [![Image 18: @dqbd](https://avatars.githubusercontent.com/u/1443449?s=64&v=4)](https://github.com/dqbd)\\\\n*   [![Image 19: @eyurtsev](https://avatars.githubusercontent.com/u/3205522?s=64&v=4)](https://github.com/eyurtsev)\\\\n*   [![Image 20: @sydney-runkle](https://avatars.githubusercontent.com/u/54324534?s=64&v=4)](https://github.com/sydney-runkle)\\\\n*   [![Image 21: @andrewnguonly](https://avatars.githubusercontent.com/u/7654246?s=64&v=4)](https://github.com/andrewnguonly)\\\\n*   [![Image 22: @hwchase17](https://avatars.githubusercontent.com/u/11986836?s=64&v=4)](https://github.com/hwchase17)\\\\n*   [![Image 23: @isahers1](https://avatars.githubusercontent.com/u/78627776?s=64&v=4)](https://github.com/isahers1)\\\\n*   [![Image 24: @lnhsingh](https://avatars.githubusercontent.com/u/15386648?s=64&v=4)](https://github.com/lnhsingh)\\\\n*   [![Image 25: @rlancemartin](https://avatars.githubusercontent.com/u/122662504?s=64&v=4)](https://github.com/rlancemartin)\\\\n*   [![Image 26: @ccurme](https://avatars.githubusercontent.com/u/26529506?s=64&v=4)](https://github.com/ccurme)\\\\n*   [![Image 27: @dependabot[bot]](https://avatars.githubusercontent.com/in/29110?s=64&v=4)](https://github.com/apps/dependabot)\\\\n*   [![Image 28: @bracesproul](https://avatars.githubusercontent.com/u/46789226?s=64&v=4)](https://github.com/bracesproul)\\\\n*   [![Image 29: @lc-arjun](https://avatars.githubusercontent.com/u/185099244?s=64&v=4)](https://github.com/lc-arjun)\\\\n\\\\n[+ 272 contributors](https://github.com/langchain-ai/langgraph/graphs/contributors)\\\\n\\\\nLanguages\\\\n---------\\\\n\\\\n*   [Python 99.3%](https://github.com/langchain-ai/langgraph/search?l=python)\\\\n*   Other 0.7%\\\\n\\\\nFooter\\\\n------\\\\n\\\\n[](https://github.com/) ¬© 2026 GitHub,Inc. \\\\n\\\\n### Footer navigation\\\\n\\\\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\\\\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\\\\n*   [Security](https://github.com/security)\\\\n*   [Status](https://www.githubstatus.com/)\\\\n*   [Community](https://github.community/)\\\\n*   [Docs](https://docs.github.com/)\\\\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\\\\n*    Manage cookies \\\\n*    Do not share my personal information \\\\n\\\\n You can‚Äôt perform that action at this time.\"}], \"response_time\": 0.92, \"request_id\": \"c638eed1-e8f2-49ee-8142-786043008a22\"}', name='internet_search', id='92da397c-e083-4863-97ef-25bd2df6f56f', tool_call_id='call_89580e7dd35048f5a3921112'),\n  AIMessage(content='**LangGraph ‚Äì A Polished Overview**\\n\\n---\\n\\n### 1. Executive Summary\\nLangGraph is an **open‚Äësource orchestration framework** that lets developers build, deploy, and manage **stateful, long‚Äërunning AI agents** as graph‚Äëbased workflows. Created by the team behind **LangChain**, it provides low‚Äëlevel primitives for durable execution, human‚Äëin‚Äëthe‚Äëloop control, memory, streaming, and debugging. Because it models agent logic as a graph, LangGraph makes complex, multi‚Äëstep, and multi‚Äëagent processes transparent, scalable, and easy to reason about.\\n\\n&gt; *‚ÄúLangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L4„Äë  \\n\\n---\\n\\n### 2. Core Concepts\\n\\n| Concept | What It Is | Why It Matters |\\n|---------|------------|----------------|\\n| **Nodes** | Individual components or ‚Äúactors‚Äù (e.g., an LLM call, a tool, a data fetcher). | Represent the atomic steps of a workflow; can be reused, swapped, or extended. |\\n| **Edges** | Directed connections that decide **which node runs next** (conditional or fixed). | Enable flexible routing, branching, and loops within the agent. |\\n| **State (Graph Memory)** | A persistent key‚Äëvalue store that travels with the workflow, recording past actions, observations, and intermediate results. | Provides ‚Äúmemory‚Äù across steps, supports debugging, and enables reflection (agents reviewing their own outputs). |\\n| **Graph Architecture** | The overall directed graph (often cyclic) that defines the execution topology. | Allows complex control flows (single‚Äëagent, multi‚Äëagent, hierarchical, sequential) while keeping the system **stateful and durable**. |\\n| **Durable Execution** | Execution that survives crashes, restarts, or back‚Äëpressure; the runtime can resume exactly where it left off. | Guarantees reliability for long‚Äërunning tasks (e.g., multi‚Äëday data pipelines). |\\n| **Human‚Äëin‚Äëthe‚ÄëLoop (HITL)** | Points in the graph where a human can inspect, approve, or modify the agent‚Äôs state. | Adds safety and oversight for high‚Äëstakes decisions. |\\n| **Streaming** | Token‚Äëby‚Äëtoken or step‚Äëby‚Äëstep output that can be shown to users in real time. | Improves UX for interactive agents (chatbots, planning assistants). |\\n| **Memory** | Built‚Äëin mechanisms for short‚Äëterm working memory and long‚Äëterm persistent memory. | Enables context‚Äëaware agents that remember past interactions across sessions. |\\n\\n&gt; *‚ÄúNodes represent individual components or agents within an AI workflow‚Ä¶ Edges are a function within Python that determines which node to execute next based on the current state.‚Äù* ‚Äì LangChain documentation„Äê2‚Ä†L1-L4„Äë  \\n\\n---\\n\\n### 3. Key Benefits\\n\\n| Benefit | Description |\\n|---------|-------------|\\n| **Transparency & Observability** | The graph visualizes every step; state can be inspected at any point. |\\n| **Scalability** | Graph‚Äëbased design allows horizontal scaling and parallel execution of independent branches. |\\n| **Modularity** | Nodes and edges are interchangeable; developers can plug in new models, tools, or data sources without rewriting the whole workflow. |\\n| **Production‚ÄëReady** | Integrates with **LangSmith** for tracing, evaluation, and deployment; supports durable execution and checkpointing. |\\n| **Human‚ÄëCentric Control** | Built‚Äëin HITL checkpoints let humans intervene, approve, or redirect agents. |\\n| **Open‚ÄëSource & Free** | MIT‚Äëlicensed; no runtime overhead; can be used standalone or alongside LangChain. |\\n| **Multi‚ÄëAgent Support** | Facilitates creation of networks of specialized agents that collaborate on complex problems. |\\n\\n&gt; *‚ÄúLangGraph provides low‚Äëlevel supporting infrastructure for any long‚Äërunning, stateful workflow or agent‚Ä¶ It does not abstract prompts or architecture, and provides the following central benefits: durable execution, human‚Äëin‚Äëthe‚Äëloop, comprehensive memory, debugging with LangSmith, production‚Äëready deployment.‚Äù* ‚Äì LangChain overview„Äê2‚Ä†L1-L5„Äë  \\n\\n---\\n\\n### 4. Typical Use Cases\\n\\n| Use Case | Example |\\n|----------|---------|\\n| **Chatbots & Virtual Assistants** | Build a vacation‚Äëplanning bot that remembers user preferences, loops for clarification, and streams responses token‚Äëby‚Äëtoken. |\\n| **Agent Systems** | Deploy autonomous agents for robotics, autonomous vehicles, or game NPCs that can pause for human review. |\\n| **Retrieval‚ÄëAugmented Generation (RAG)** | Combine external document retrieval with LLM reasoning inside a graph where retrieval nodes feed into generation nodes. |\\n| **Multi‚ÄëAgent Collaboration** | Orchestrate teams of specialized agents (e.g., ‚Äúresearcher‚Äù, ‚Äúwriter‚Äù, ‚Äúeditor‚Äù) that pass artifacts back and forth. |\\n| **Enterprise Workflow Automation** | Automate multi‚Äëstep business processes (e.g., invoice approval, onboarding) with persistent state and audit trails. |\\n| **Experimentation & Prototyping** | Quickly prototype new agent topologies in a visual IDE (LangGraph Studio) before moving to production. |\\n\\n&gt; *‚ÄúLangGraph is also built on several key technologies, including LangChain‚Ä¶ It provides a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent‚Äëbased systems.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L3„Äë  \\n\\n---\\n\\n### 5. Relationship to LangChain & the Broader Ecosystem\\n\\n| Component | Role |\\n|-----------|------|\\n| **LangChain** | High‚Äëlevel library that supplies ready‚Äëmade agents, tool‚Äëwrappers, and prompt templates. LangGraph sits **underneath** LangChain, offering the durable execution and state‚Äëmanagement backbone. |\\n| **LangSmith** | Observability platform for tracing, evaluating, and monitoring LangGraph‚Äëbased agents in production. |\\n| **LangGraph Studio** | Visual, drag‚Äëand‚Äëdrop IDE that lets users design graphs without writing code; useful for rapid prototyping and collaboration. |\\n| **Community Integrations** | Projects like **CrewAI**, **AutoGen**, and **MetaGPT** can be built on top of LangGraph, leveraging its graph primitives for multi‚Äëagent orchestration. |\\n| **Deployment Options** | Can be run locally, in a container, or as a managed service via LangSmith; supports scaling on Kubernetes, cloud VMs, or edge devices. |\\n\\n&gt; *‚ÄúLangGraph can be used standalone, but it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents.‚Äù* ‚Äì LangChain docs„Äê2‚Ä†L1-L2„Äë  \\n\\n---\\n\\n### 6. Getting Started (Practical Steps)\\n\\n1. **Install**  \\n   ```bash\\n   pip install -U langgraph\\n   ```\\n2. **Create a Simple Graph** (from LangChain docs)  \\n   ```python\\n   from langgraph.graph import StateGraph, MessagesState, START, END\\n\\n   class State(MessagesState):\\n       pass\\n\\n   def mock_llm(state: State):\\n       return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\n   graph = StateGraph(State)\\n   graph.add_node(mock_llm)\\n   graph.add_edge(START, \"mock_llm\")\\n   graph.add_edge(\"mock_llm\", END)\\n   app = graph.compile()\\n\\n   print(app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]))\\n   ```\\n3. **Add Memory & Persistence** ‚Äì Use built‚Äëin checkpointing or integrate with a database (e.g., SQLite, Redis).  \\n4. **Add Human‚Äëin‚Äëthe‚ÄëLoop** ‚Äì Insert `interrupt()` calls to pause and present the current state to a user for approval.  \\n5. **Deploy** ‚Äì Wrap the compiled graph in a FastAPI/Flask endpoint, or deploy via **LangSmith** for managed scaling and observability.  \\n\\n&gt; *The official LangGraph Quickstart guide provides step‚Äëby‚Äëstep tutorials, example notebooks, and a ‚ÄúHello‚ÄëWorld‚Äù template.* ‚Äì LangChain documentation„Äê2‚Ä†L1-L3„Äë\\n\\n---\\n\\n### 7. Future Roadmap (as of early‚Äë2026)\\n\\n| Planned Enhancement | Expected Impact |\\n|---------------------|-----------------|\\n| **Advanced NLP** ‚Äì richer language understanding and generation capabilities. | More natural, context‚Äëaware interactions. |\\n| **Improved ML Integration** ‚Äì auto‚Äëlearning loops that refine agent policies over time. | Continuous performance gains without manual retraining. |\\n| **New Platform Support** ‚Äì mobile, edge, and IoT runtimes. | Bring agent workflows to constrained environments. |\\n| **Enhanced Debugging & Visualization** ‚Äì richer state‚Äëtravel and ‚Äútime‚Äëmachine‚Äù debugging. | Faster issue resolution and higher reliability. |\\n| **Standardized Multi‚ÄëAgent Protocols** ‚Äì built‚Äëin patterns for coordination, negotiation, and conflict resolution. | Simplify creation of large‚Äëscale multi‚Äëagent systems. |\\n\\n&gt; *‚ÄúFuture developments include enhanced NLP, improved machine learning, and support for new platforms such as mobile and edge computing.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L5„Äë\\n\\n---\\n\\n### 8. Key Takeaways\\n\\n* **LangGraph = Graph‚Äëbased orchestration for AI agents** ‚Äì it turns complex, stateful workflows into visual, durable graphs.  \\n* **Built for production** ‚Äì offers durable execution, human‚Äëin‚Äëthe‚Äëloop checkpoints, persistent memory, and seamless integration with LangSmith for observability.  \\n* **Open‚Äësource & flexible** ‚Äì MIT‚Äëlicensed, can be used alone or as the execution engine behind LangChain‚Äôs higher‚Äëlevel agents.  \\n* **Wide adoption** ‚Äì Companies such as Klarna, Replit, Elastic, Norwegian Cruise Line, and IBM‚Äôs own watsonx.ai use LangGraph to power chatbots, multi‚Äëagent systems, and enterprise automation.  \\n* **Getting started is easy** ‚Äì a few `pip install` commands and a short Python snippet get you a working graph; visual tools (LangGraph Studio) make design accessible to non‚Äëprogrammers.  \\n\\n---\\n\\n### 9. References\\n\\n1. **IBM Think ‚Äì ‚ÄúWhat is LangGraph?‚Äù** ‚Äì Overview of LangGraph‚Äôs purpose, architecture, and use cases.  \\n2. **LangChain Documentation ‚Äì ‚ÄúLangGraph overview‚Äù** ‚Äì Core concepts, installation, and production‚Äëready features.  \\n3. **Reddit ‚Äì r/AI_Agents ‚ÄúWhy use LangGraph?‚Äù** ‚Äì Community perspective on graph‚Äëbased workflow advantages.  \\n4. **LangChain.com ‚Äì Product page for LangGraph** ‚Äì Marketing overview, customer stories, and FAQ.  \\n5. **GitHub ‚Äì langchain-ai/langgraph** ‚Äì Source repository, release notes, and contribution activity.  \\n\\n--- \\n\\n*Prepared as a concise, structured briefing for anyone seeking a clear understanding of what LangGraph is, how it works, and why it matters in the modern AI‚Äëagent landscape.*', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2592, 'prompt_tokens': 31532, 'total_tokens': 34124, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 243, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771589005-ywGQGvGvLv8XbqFplapK', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c7aee-f0c1-7c50-927d-d827373b12a0-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 31532, 'output_tokens': 2592, 'total_tokens': 34124, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 243}})]}\n\n\n\n# Print the agent's response\nprint(result[\"messages\"][-1].content)\n\n**LangGraph ‚Äì A Polished Overview**\n\n---\n\n### 1. Executive Summary\nLangGraph is an **open‚Äësource orchestration framework** that lets developers build, deploy, and manage **stateful, long‚Äërunning AI agents** as graph‚Äëbased workflows. Created by the team behind **LangChain**, it provides low‚Äëlevel primitives for durable execution, human‚Äëin‚Äëthe‚Äëloop control, memory, streaming, and debugging. Because it models agent logic as a graph, LangGraph makes complex, multi‚Äëstep, and multi‚Äëagent processes transparent, scalable, and easy to reason about.\n\n&gt; *‚ÄúLangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L4„Äë  \n\n---\n\n### 2. Core Concepts\n\n| Concept | What It Is | Why It Matters |\n|---------|------------|----------------|\n| **Nodes** | Individual components or ‚Äúactors‚Äù (e.g., an LLM call, a tool, a data fetcher). | Represent the atomic steps of a workflow; can be reused, swapped, or extended. |\n| **Edges** | Directed connections that decide **which node runs next** (conditional or fixed). | Enable flexible routing, branching, and loops within the agent. |\n| **State (Graph Memory)** | A persistent key‚Äëvalue store that travels with the workflow, recording past actions, observations, and intermediate results. | Provides ‚Äúmemory‚Äù across steps, supports debugging, and enables reflection (agents reviewing their own outputs). |\n| **Graph Architecture** | The overall directed graph (often cyclic) that defines the execution topology. | Allows complex control flows (single‚Äëagent, multi‚Äëagent, hierarchical, sequential) while keeping the system **stateful and durable**. |\n| **Durable Execution** | Execution that survives crashes, restarts, or back‚Äëpressure; the runtime can resume exactly where it left off. | Guarantees reliability for long‚Äërunning tasks (e.g., multi‚Äëday data pipelines). |\n| **Human‚Äëin‚Äëthe‚ÄëLoop (HITL)** | Points in the graph where a human can inspect, approve, or modify the agent‚Äôs state. | Adds safety and oversight for high‚Äëstakes decisions. |\n| **Streaming** | Token‚Äëby‚Äëtoken or step‚Äëby‚Äëstep output that can be shown to users in real time. | Improves UX for interactive agents (chatbots, planning assistants). |\n| **Memory** | Built‚Äëin mechanisms for short‚Äëterm working memory and long‚Äëterm persistent memory. | Enables context‚Äëaware agents that remember past interactions across sessions. |\n\n&gt; *‚ÄúNodes represent individual components or agents within an AI workflow‚Ä¶ Edges are a function within Python that determines which node to execute next based on the current state.‚Äù* ‚Äì LangChain documentation„Äê2‚Ä†L1-L4„Äë  \n\n---\n\n### 3. Key Benefits\n\n| Benefit | Description |\n|---------|-------------|\n| **Transparency & Observability** | The graph visualizes every step; state can be inspected at any point. |\n| **Scalability** | Graph‚Äëbased design allows horizontal scaling and parallel execution of independent branches. |\n| **Modularity** | Nodes and edges are interchangeable; developers can plug in new models, tools, or data sources without rewriting the whole workflow. |\n| **Production‚ÄëReady** | Integrates with **LangSmith** for tracing, evaluation, and deployment; supports durable execution and checkpointing. |\n| **Human‚ÄëCentric Control** | Built‚Äëin HITL checkpoints let humans intervene, approve, or redirect agents. |\n| **Open‚ÄëSource & Free** | MIT‚Äëlicensed; no runtime overhead; can be used standalone or alongside LangChain. |\n| **Multi‚ÄëAgent Support** | Facilitates creation of networks of specialized agents that collaborate on complex problems. |\n\n&gt; *‚ÄúLangGraph provides low‚Äëlevel supporting infrastructure for any long‚Äërunning, stateful workflow or agent‚Ä¶ It does not abstract prompts or architecture, and provides the following central benefits: durable execution, human‚Äëin‚Äëthe‚Äëloop, comprehensive memory, debugging with LangSmith, production‚Äëready deployment.‚Äù* ‚Äì LangChain overview„Äê2‚Ä†L1-L5„Äë  \n\n---\n\n### 4. Typical Use Cases\n\n| Use Case | Example |\n|----------|---------|\n| **Chatbots & Virtual Assistants** | Build a vacation‚Äëplanning bot that remembers user preferences, loops for clarification, and streams responses token‚Äëby‚Äëtoken. |\n| **Agent Systems** | Deploy autonomous agents for robotics, autonomous vehicles, or game NPCs that can pause for human review. |\n| **Retrieval‚ÄëAugmented Generation (RAG)** | Combine external document retrieval with LLM reasoning inside a graph where retrieval nodes feed into generation nodes. |\n| **Multi‚ÄëAgent Collaboration** | Orchestrate teams of specialized agents (e.g., ‚Äúresearcher‚Äù, ‚Äúwriter‚Äù, ‚Äúeditor‚Äù) that pass artifacts back and forth. |\n| **Enterprise Workflow Automation** | Automate multi‚Äëstep business processes (e.g., invoice approval, onboarding) with persistent state and audit trails. |\n| **Experimentation & Prototyping** | Quickly prototype new agent topologies in a visual IDE (LangGraph Studio) before moving to production. |\n\n&gt; *‚ÄúLangGraph is also built on several key technologies, including LangChain‚Ä¶ It provides a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent‚Äëbased systems.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L3„Äë  \n\n---\n\n### 5. Relationship to LangChain & the Broader Ecosystem\n\n| Component | Role |\n|-----------|------|\n| **LangChain** | High‚Äëlevel library that supplies ready‚Äëmade agents, tool‚Äëwrappers, and prompt templates. LangGraph sits **underneath** LangChain, offering the durable execution and state‚Äëmanagement backbone. |\n| **LangSmith** | Observability platform for tracing, evaluating, and monitoring LangGraph‚Äëbased agents in production. |\n| **LangGraph Studio** | Visual, drag‚Äëand‚Äëdrop IDE that lets users design graphs without writing code; useful for rapid prototyping and collaboration. |\n| **Community Integrations** | Projects like **CrewAI**, **AutoGen**, and **MetaGPT** can be built on top of LangGraph, leveraging its graph primitives for multi‚Äëagent orchestration. |\n| **Deployment Options** | Can be run locally, in a container, or as a managed service via LangSmith; supports scaling on Kubernetes, cloud VMs, or edge devices. |\n\n&gt; *‚ÄúLangGraph can be used standalone, but it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents.‚Äù* ‚Äì LangChain docs„Äê2‚Ä†L1-L2„Äë  \n\n---\n\n### 6. Getting Started (Practical Steps)\n\n1. **Install**  \n   ```bash\n   pip install -U langgraph\n   ```\n2. **Create a Simple Graph** (from LangChain docs)  \n   ```python\n   from langgraph.graph import StateGraph, MessagesState, START, END\n\n   class State(MessagesState):\n       pass\n\n   def mock_llm(state: State):\n       return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n\n   graph = StateGraph(State)\n   graph.add_node(mock_llm)\n   graph.add_edge(START, \"mock_llm\")\n   graph.add_edge(\"mock_llm\", END)\n   app = graph.compile()\n\n   print(app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]))\n   ```\n3. **Add Memory & Persistence** ‚Äì Use built‚Äëin checkpointing or integrate with a database (e.g., SQLite, Redis).  \n4. **Add Human‚Äëin‚Äëthe‚ÄëLoop** ‚Äì Insert `interrupt()` calls to pause and present the current state to a user for approval.  \n5. **Deploy** ‚Äì Wrap the compiled graph in a FastAPI/Flask endpoint, or deploy via **LangSmith** for managed scaling and observability.  \n\n&gt; *The official LangGraph Quickstart guide provides step‚Äëby‚Äëstep tutorials, example notebooks, and a ‚ÄúHello‚ÄëWorld‚Äù template.* ‚Äì LangChain documentation„Äê2‚Ä†L1-L3„Äë\n\n---\n\n### 7. Future Roadmap (as of early‚Äë2026)\n\n| Planned Enhancement | Expected Impact |\n|---------------------|-----------------|\n| **Advanced NLP** ‚Äì richer language understanding and generation capabilities. | More natural, context‚Äëaware interactions. |\n| **Improved ML Integration** ‚Äì auto‚Äëlearning loops that refine agent policies over time. | Continuous performance gains without manual retraining. |\n| **New Platform Support** ‚Äì mobile, edge, and IoT runtimes. | Bring agent workflows to constrained environments. |\n| **Enhanced Debugging & Visualization** ‚Äì richer state‚Äëtravel and ‚Äútime‚Äëmachine‚Äù debugging. | Faster issue resolution and higher reliability. |\n| **Standardized Multi‚ÄëAgent Protocols** ‚Äì built‚Äëin patterns for coordination, negotiation, and conflict resolution. | Simplify creation of large‚Äëscale multi‚Äëagent systems. |\n\n&gt; *‚ÄúFuture developments include enhanced NLP, improved machine learning, and support for new platforms such as mobile and edge computing.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L5„Äë\n\n---\n\n### 8. Key Takeaways\n\n* **LangGraph = Graph‚Äëbased orchestration for AI agents** ‚Äì it turns complex, stateful workflows into visual, durable graphs.  \n* **Built for production** ‚Äì offers durable execution, human‚Äëin‚Äëthe‚Äëloop checkpoints, persistent memory, and seamless integration with LangSmith for observability.  \n* **Open‚Äësource & flexible** ‚Äì MIT‚Äëlicensed, can be used alone or as the execution engine behind LangChain‚Äôs higher‚Äëlevel agents.  \n* **Wide adoption** ‚Äì Companies such as Klarna, Replit, Elastic, Norwegian Cruise Line, and IBM‚Äôs own watsonx.ai use LangGraph to power chatbots, multi‚Äëagent systems, and enterprise automation.  \n* **Getting started is easy** ‚Äì a few `pip install` commands and a short Python snippet get you a working graph; visual tools (LangGraph Studio) make design accessible to non‚Äëprogrammers.  \n\n---\n\n### 9. References\n\n1. **IBM Think ‚Äì ‚ÄúWhat is LangGraph?‚Äù** ‚Äì Overview of LangGraph‚Äôs purpose, architecture, and use cases.  \n2. **LangChain Documentation ‚Äì ‚ÄúLangGraph overview‚Äù** ‚Äì Core concepts, installation, and production‚Äëready features.  \n3. **Reddit ‚Äì r/AI_Agents ‚ÄúWhy use LangGraph?‚Äù** ‚Äì Community perspective on graph‚Äëbased workflow advantages.  \n4. **LangChain.com ‚Äì Product page for LangGraph** ‚Äì Marketing overview, customer stories, and FAQ.  \n5. **GitHub ‚Äì langchain-ai/langgraph** ‚Äì Source repository, release notes, and contribution activity.  \n\n--- \n\n*Prepared as a concise, structured briefing for anyone seeking a clear understanding of what LangGraph is, how it works, and why it matters in the modern AI‚Äëagent landscape.*"
  },
  {
    "objectID": "L01/01_agents.html#key-takeaways",
    "href": "L01/01_agents.html#key-takeaways",
    "title": "Agents in LangChain",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThree key methods for models: invoke, stream, and batch.\nLLMs can be configured to responsd in a structured format\nAgent = Model + Tools\nModels (LLMs) are the brain-power of agents\nTools are simply names and agruments of defined Python functions"
  },
  {
    "objectID": "L01/01_agents.html#activity",
    "href": "L01/01_agents.html#activity",
    "title": "Agents in LangChain",
    "section": "Activity",
    "text": "Activity\nOver to you: create an Agent that is able to answer questions, with an added internet search capability."
  },
  {
    "objectID": "L01/05_RAG_agent.html",
    "href": "L01/05_RAG_agent.html",
    "title": "Build a RAG agent with LangChain",
    "section": "",
    "text": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\n\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n\n\nWe will cover the following concepts:\n\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\n\n\n\n\n\n\nNote\n\n\n\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.\nIf your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\n\n\n\n\n\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nimport bs4\nfrom langchain.agents import AgentState, create_agent\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.messages import MessageLikeRepresentation\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load and chunk contents of the blog\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nall_splits = text_splitter.split_documents(docs)\n\n# Index chunks\n_ = vector_store.add_documents(documents=all_splits)\n\n# Construct a tool for retrieving context\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n\ntools = [retrieve_context]\n# If desired, specify custom instructions\nprompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\nCheck out the LangSmith trace."
  },
  {
    "objectID": "L01/05_RAG_agent.html#overview",
    "href": "L01/05_RAG_agent.html#overview",
    "title": "Build a RAG agent with LangChain",
    "section": "",
    "text": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\n\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n\n\nWe will cover the following concepts:\n\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\n\n\n\n\n\n\nNote\n\n\n\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.\nIf your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\n\n\n\n\n\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nimport bs4\nfrom langchain.agents import AgentState, create_agent\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.messages import MessageLikeRepresentation\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load and chunk contents of the blog\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nall_splits = text_splitter.split_documents(docs)\n\n# Index chunks\n_ = vector_store.add_documents(documents=all_splits)\n\n# Construct a tool for retrieving context\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n\ntools = [retrieve_context]\n# If desired, specify custom instructions\nprompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\nCheck out the LangSmith trace."
  },
  {
    "objectID": "L01/05_RAG_agent.html#setup",
    "href": "L01/05_RAG_agent.html#setup",
    "title": "Build a RAG agent with LangChain",
    "section": "Setup",
    "text": "Setup\n\nInstallation\nThis tutorial requires these langchain dependencies:\nuv add langchain langchain-text-splitters langchain-community bs4\nFor more details, see our Installation guide.\n\n\nComponents\nWe will need to select three components from LangChain‚Äôs suite of integrations.\n\n1. Select a chat model\nüëâ Read the OpenAI chat model integration docs\nuv add \"langchain[openai]\"\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# We use OpenRouter for the agent ‚Äî set OPENROUTER_API_KEY in .env\n# Get your key at https://openrouter.ai/keys\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    raise RuntimeError(\n        \"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:\\n\"\n        \"OPENROUTER_API_KEY=your-openrouter-api-key\"\n    )\n\n\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\n2. Select an embeddings model\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n\n\n3. Select a vector store\nuv add \"langchain-core\"\n\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nvector_store = InMemoryVectorStore(embeddings)"
  },
  {
    "objectID": "L01/05_RAG_agent.html#indexing",
    "href": "L01/05_RAG_agent.html#indexing",
    "title": "Build a RAG agent with LangChain",
    "section": "1. Indexing",
    "text": "1. Indexing\n\n\n\n\n\n\nNote\n\n\n\nThis section is an abbreviated version of the content in the semantic search tutorial.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you‚Äôre comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\n\n\nIndexing commonly works as follows:\n\nLoad: First we need to load our data. This is done with Document Loaders.\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n\n\n\n\nIndex Diagram\n\n\n\nLoading documents\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\nIn this case we‚Äôll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -&gt; text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\n\nimport bs4\nfrom langchain_community.document_loaders import WebBaseLoader\n\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs={\"parse_only\": bs4_strainer},\n)\ndocs = loader.load()\n\nassert len(docs) == 1\nprint(f\"Total characters: {len(docs[0].page_content)}\")\n\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\n\n\nTotal characters: 43047\n\n\n\nprint(docs[0].page_content[:500])\n\n\n\n      LLM Powered Autonomous Agents\n    \nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n\n\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\n\n\nGo deeper\nDocumentLoader: Object that loads data from a source as list of Documents.\n\nIntegrations: 160+ integrations to choose from.\nBaseLoader: API reference for the base interface.\n\n\n\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # chunk size (characters)\n    chunk_overlap=200,  # chunk overlap (characters)\n    add_start_index=True,  # track index in original document\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\n\nSplit blog post into 63 sub-documents.\n\n\nGo deeper\nTextSplitter: Object that splits a list of Document objects into smaller chunks for storage and retrieval.\n\nIntegrations\nInterface: API reference for the base interface.\n\n\n\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\n\ndocument_ids = vector_store.add_documents(documents=all_splits)\n\nprint(document_ids[:3])\n\n['7140256e-d3e0-4174-8abe-6941b0698ff3', 'b8079c9c-1552-4add-b4d0-c3bc454e258f', '5df27ff9-ff55-4f6c-8642-931608b5a21c']\n\n\nGo deeper\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\n\nIntegrations: 30+ integrations to choose from.\nInterface: API reference for the base interface.\n\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\n\nIntegrations: 40+ integrations to choose from.\nInterface: API reference for the base interface.\n\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question."
  },
  {
    "objectID": "L01/05_RAG_agent.html#retrieval-and-generation",
    "href": "L01/05_RAG_agent.html#retrieval-and-generation",
    "title": "Build a RAG agent with LangChain",
    "section": "2. Retrieval and generation",
    "text": "2. Retrieval and generation\nRAG applications commonly work as follows:\n\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\n\n\n\n\nRetrieval Diagram\n\n\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\n\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n\nRAG agents\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\n\nfrom langchain.tools import tool\n\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n\n\n\n\n\n\n\nTip\n\n\n\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n\n\n\n\n\n\n\n\nTip\n\n\n\nRetrieval tools are not limited to a single string query argument, as in the above example. You can force the LLM to specify additional search parameters by adding arguments‚Äî for example, a category:\nfrom typing import Literal\n\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\n\n\nGiven our tool, we can construct the agent:\n\nfrom langchain.agents import create_agent\n\ntools = [retrieve_context]\n# If desired, specify custom instructions\nprompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)\n\nLet‚Äôs test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\n\nquery = (\n    \"What is the standard method for Task Decomposition?\\n\\n\"\n    \"Once you get the answer, look up common extensions of that method.\"\n)\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    event[\"messages\"][-1].pretty_print()\n\n\n================================ Human Message =================================\n\n\n\nWhat is the standard method for Task Decomposition?\n\n\n\nOnce you get the answer, look up common extensions of that method.\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  retrieve_context (call_5ekkYX8vMgmb4Rzv4p2jFdAl)\n\n Call ID: call_5ekkYX8vMgmb4Rzv4p2jFdAl\n\n  Args:\n\n    query: standard method for Task Decomposition\n\n================================= Tool Message =================================\n\nName: retrieve_context\n\n\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n\nContent: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into ‚ÄúProblem PDDL‚Äù, then (2) requests a classical planner to generate a PDDL plan based on an existing ‚ÄúDomain PDDL‚Äù, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n\nSelf-Reflection#\n\n\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n\nContent: Component One: Planning#\n\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n\nTask Decomposition#\n\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\n\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  retrieve_context (call_p3OgKqTVTa0P51eT1Rlzy48U)\n\n Call ID: call_p3OgKqTVTa0P51eT1Rlzy48U\n\n  Args:\n\n    query: extensions of standard task decomposition methods\n\n================================= Tool Message =================================\n\nName: retrieve_context\n\n\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n\nContent: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into ‚ÄúProblem PDDL‚Äù, then (2) requests a classical planner to generate a PDDL plan based on an existing ‚ÄúDomain PDDL‚Äù, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n\nSelf-Reflection#\n\n\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n\nContent: Component One: Planning#\n\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n\nTask Decomposition#\n\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\n\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n\n================================== Ai Message ==================================\n\n\n\nThe standard method for Task Decomposition is the Chain of Thought (CoT) prompting technique. In CoT, the model is instructed to ‚Äúthink step by step‚Äù to break down complex tasks into smaller and simpler steps, aiding both task execution and interpretability of the reasoning process.\n\n\n\nCommon extensions of Chain of Thought include:\n\n\n\n1. Tree of Thoughts (ToT): This method extends CoT by exploring multiple reasoning possibilities at each step. The problem is decomposed into multiple ‚Äúthought steps,‚Äù and several alternative thoughts are generated for each, forming a tree structure. Search strategies like breadth-first search (BFS) or depth-first search (DFS) are used, with states evaluated using classifiers or majority voting.\n\n\n\n2. LLM + Planner (LLM+P): In this extension, long-horizon planning is handled by an external classical planner. The LLM first generates a problem description in Planning Domain Definition Language (PDDL), which a planner uses to generate a solution plan, then translates the plan back into natural language. This approach leverages domain-specific planning outside of the LLM.\n\n\n\nThese extensions allow for greater flexibility, systematic reasoning, and usage of external tools for sophisticated planning beyond the basic step-by-step CoT method.\n\n\n\n\nNote that the agent:\n\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\n\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n\n\n\n\n\n\nTip\n\n\n\nYou can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph‚Äôs Agentic RAG tutorial for more advanced formulations."
  },
  {
    "objectID": "L01/05_RAG_agent.html#overview-1",
    "href": "L01/05_RAG_agent.html#overview-1",
    "title": "Build a RAG agent with LangChain",
    "section": "Overview",
    "text": "Overview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\n\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n\nConcepts\nWe will cover the following concepts:\n\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\n\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.\nIf your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\n\n\n\nPreview\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\n ```python theme={null} import bs4 from langchain.agents import AgentState, create_agent from langchain_community.document_loaders import WebBaseLoader from langchain.messages import MessageLikeRepresentation from langchain_text_splitters import RecursiveCharacterTextSplitter\n# Load and chunk contents of the blog loader = WebBaseLoader( web_paths=(‚Äúhttps://lilianweng.github.io/posts/2023-06-23-agent/‚Äù,), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, ‚Äúpost-header‚Äù) ) ), ) docs = loader.load()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) all_splits = text_splitter.split_documents(docs)\n# Index chunks _ = vector_store.add_documents(documents=all_splits)\n# Construct a tool for retrieving context @tool(response_format=‚Äúcontent_and_artifact‚Äù) def retrieve_context(query: str): ‚Äú‚Äú‚ÄúRetrieve information to help answer a query.‚Äù‚Äú‚Äù retrieved_docs = vector_store.similarity_search(query, k=2) serialized = ‚Äú‚Äù.join( (f‚ÄùSource: {doc.metadata}: {doc.page_content}‚Äú) for doc in retrieved_docs ) return serialized, retrieved_docs\ntools = [retrieve_context] # If desired, specify custom instructions prompt = ( ‚ÄúYou have access to a tool that retrieves context from a blog post.‚Äù ‚ÄúUse the tool to help answer user queries.‚Äù ) agent = create_agent(model, tools, system_prompt=prompt) ```\npython  theme={null}   query = \"What is task decomposition?\"   for step in agent.stream(       {\"messages\": [{\"role\": \"user\", \"content\": query}]},       stream_mode=\"values\",   ):       step[\"messages\"][-1].pretty_print()\n================================ Human Message =================================\n\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\n  Args:\n    query: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\n\nTask decomposition refers to...\nCheck out the LangSmith trace."
  },
  {
    "objectID": "L01/05_RAG_agent.html#setup-1",
    "href": "L01/05_RAG_agent.html#setup-1",
    "title": "Build a RAG agent with LangChain",
    "section": "Setup",
    "text": "Setup\n\nInstallation\nThis tutorial requires these langchain dependencies:\n bash pip theme={null}   pip install langchain langchain-text-splitters langchain-community bs4\nbash uv theme={null}   uv add langchain langchain-text-splitters langchain-community bs4 \nFor more details, see our Installation guide.\n\n\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nshell  theme={null} export LANGSMITH_TRACING=\"true\" export LANGSMITH_API_KEY=\"...\"\nOr, set them in Python:\n```python theme={null} import getpass import os\nos.environ[‚ÄúLANGSMITH_TRACING‚Äù] = ‚Äútrue‚Äù os.environ[‚ÄúLANGSMITH_API_KEY‚Äù] = getpass.getpass()\n\n### Components\n\nWe will need to select three components from LangChain's suite of integrations.\n\nSelect a chat model:\n\n&lt;Tabs&gt;\n  &lt;Tab title=\"OpenAI\"&gt;\n    üëâ Read the [OpenAI chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"gpt-4.1\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import ChatOpenAI\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = ChatOpenAI(model=\"gpt-4.1\")\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Anthropic\"&gt;\n    üëâ Read the [Anthropic chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/anthropic/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[anthropic]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_anthropic import ChatAnthropic\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Azure\"&gt;\n    üëâ Read the [Azure chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/azure_chat_openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = init_chat_model(\n          \"azure_openai:gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import AzureChatOpenAI\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = AzureChatOpenAI(\n          model=\"gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n      )\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Google Gemini\"&gt;\n    üëâ Read the [Google GenAI chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[google-genai]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_google_genai import ChatGoogleGenerativeAI\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"AWS Bedrock\"&gt;\n    üëâ Read the [AWS Bedrock chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/bedrock/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[aws]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      from langchain.chat_models import init_chat_model\n\n      # Follow the steps here to configure your credentials:\n      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      model = init_chat_model(\n          \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n          model_provider=\"bedrock_converse\",\n      )\n      ```\n\n      ```python Model Class theme={null}\n      from langchain_aws import ChatBedrock\n\n      model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"HuggingFace\"&gt;\n    üëâ Read the [HuggingFace chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/huggingface/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[huggingface]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n      model = init_chat_model(\n          \"microsoft/Phi-3-mini-4k-instruct\",\n          model_provider=\"huggingface\",\n          temperature=0.7,\n          max_tokens=1024,\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\n      os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n      llm = HuggingFaceEndpoint(\n          repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n          temperature=0.7,\n          max_length=1024,\n      )\n      model = ChatHuggingFace(llm=llm)\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n&lt;/Tabs&gt;\n\nSelect an embeddings model:\n\n&lt;Tabs&gt;\n  &lt;Tab title=\"OpenAI\"&gt;\n    ```shell  theme={null}\n    pip install -U \"langchain-openai\"\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"OPENAI_API_KEY\"):\n        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n\n    from langchain_openai import OpenAIEmbeddings\n\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Azure\"&gt;\n    ```shell  theme={null}\n    pip install -U \"langchain-openai\"\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n        os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\n\n    from langchain_openai import AzureOpenAIEmbeddings\n\n    embeddings = AzureOpenAIEmbeddings(\n        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n        azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n        openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Google Gemini\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-google-genai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"GOOGLE_API_KEY\"):\n        os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n\n    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Google Vertex\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-google-vertexai\n    ```\n\n    ```python  theme={null}\n    from langchain_google_vertexai import VertexAIEmbeddings\n\n    embeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"AWS\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-aws\n    ```\n\n    ```python  theme={null}\n    from langchain_aws import BedrockEmbeddings\n\n    embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"HuggingFace\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-huggingface\n    ```\n\n    ```python  theme={null}\n    from langchain_huggingface import HuggingFaceEmbeddings\n\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Ollama\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-ollama\n    ```\n\n    ```python  theme={null}\n    from langchain_ollama import OllamaEmbeddings\n\n    embeddings = OllamaEmbeddings(model=\"llama3\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Cohere\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-cohere\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"COHERE_API_KEY\"):\n        os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\n\n    from langchain_cohere import CohereEmbeddings\n\n    embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"MistralAI\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-mistralai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"MISTRALAI_API_KEY\"):\n        os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\n\n    from langchain_mistralai import MistralAIEmbeddings\n\n    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Nomic\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-nomic\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"NOMIC_API_KEY\"):\n        os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\n\n    from langchain_nomic import NomicEmbeddings\n\n    embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"NVIDIA\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-nvidia-ai-endpoints\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"NVIDIA_API_KEY\"):\n        os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n\n    from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n\n    embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Voyage AI\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-voyageai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"VOYAGE_API_KEY\"):\n        os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n\n    from langchain-voyageai import VoyageAIEmbeddings\n\n    embeddings = VoyageAIEmbeddings(model=\"voyage-3\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"IBM watsonx\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-ibm\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"WATSONX_APIKEY\"):\n        os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\n\n    from langchain_ibm import WatsonxEmbeddings\n\n    embeddings = WatsonxEmbeddings(\n        model_id=\"ibm/slate-125m-english-rtrvr\",\n        url=\"https://us-south.ml.cloud.ibm.com\",\n        project_id=\"&lt;WATSONX PROJECT_ID&gt;\",\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Fake\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-core\n    ```\n\n    ```python  theme={null}\n    from langchain_core.embeddings import DeterministicFakeEmbedding\n\n    embeddings = DeterministicFakeEmbedding(size=4096)\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Isaacus\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-isaacus\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"ISAACUS_API_KEY\"):\n    os.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\n\n    from langchain_isaacus import IsaacusEmbeddings\n\n    embeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\n    ```\n  &lt;/Tab&gt;\n&lt;/Tabs&gt;\n\nSelect a vector store:\n\n&lt;Tabs&gt;\n  &lt;Tab title=\"In-memory\"&gt;\n    ```shell  theme={null}\n    pip install -U \"langchain-core\"\n    ```\n\n    ```python  theme={null}\n    from langchain_core.vectorstores import InMemoryVectorStore\n\n    vector_store = InMemoryVectorStore(embeddings)\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Amazon OpenSearch\"&gt;\n    ```shell  theme={null}\n    pip install -qU  boto3\n    ```\n\n    ```python  theme={null}\n    from opensearchpy import RequestsHttpConnection\n\n    service = \"es\"  # must set the service as 'es'\n    region = \"us-east-2\"\n    credentials = boto3.Session(\n        aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\n    ).get_credentials()\n    awsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\n\n    vector_store = OpenSearchVectorSearch.from_documents(\n        docs,\n        embeddings,\n        opensearch_url=\"host url\",\n        http_auth=awsauth,\n        timeout=300,\n        use_ssl=True,\n        verify_certs=True,\n        connection_class=RequestsHttpConnection,\n        index_name=\"test-index\",\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"AstraDB\"&gt;\n    ```shell  theme={null}\n    pip install -U \"langchain-astradb\"\n    ```\n\n    ```python  theme={null}\n    from langchain_astradb import AstraDBVectorStore\n\n    vector_store = AstraDBVectorStore(\n        embedding=embeddings,\n        api_endpoint=ASTRA_DB_API_ENDPOINT,\n        collection_name=\"astra_vector_langchain\",\n        token=ASTRA_DB_APPLICATION_TOKEN,\n        namespace=ASTRA_DB_NAMESPACE,\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Chroma\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-chroma\n    ```\n\n    ```python  theme={null}\n    from langchain_chroma import Chroma\n\n    vector_store = Chroma(\n        collection_name=\"example_collection\",\n        embedding_function=embeddings,\n        persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"FAISS\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-community faiss-cpu\n    ```\n\n    ```python  theme={null}\n    import faiss\n    from langchain_community.docstore.in_memory import InMemoryDocstore\n    from langchain_community.vectorstores import FAISS\n\n    embedding_dim = len(embeddings.embed_query(\"hello world\"))\n    index = faiss.IndexFlatL2(embedding_dim)\n\n    vector_store = FAISS(\n        embedding_function=embeddings,\n        index=index,\n        docstore=InMemoryDocstore(),\n        index_to_docstore_id={},\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Milvus\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-milvus\n    ```\n\n    ```python  theme={null}\n    from langchain_milvus import Milvus\n\n    URI = \"./milvus_example.db\"\n\n    vector_store = Milvus(\n        embedding_function=embeddings,\n        connection_args={\"uri\": URI},\n        index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"MongoDB\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-mongodb\n    ```\n\n    ```python  theme={null}\n    from langchain_mongodb import MongoDBAtlasVectorSearch\n\n    vector_store = MongoDBAtlasVectorSearch(\n        embedding=embeddings,\n        collection=MONGODB_COLLECTION,\n        index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n        relevance_score_fn=\"cosine\",\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"PGVector\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-postgres\n    ```\n\n    ```python  theme={null}\n    from langchain_postgres import PGVector\n\n    vector_store = PGVector(\n        embeddings=embeddings,\n        collection_name=\"my_docs\",\n        connection=\"postgresql+psycopg://...\",\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"PGVectorStore\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-postgres\n    ```\n\n    ```python  theme={null}\n    from langchain_postgres import PGEngine, PGVectorStore\n\n    pg_engine = PGEngine.from_connection_string(\n        url=\"postgresql+psycopg://...\"\n    )\n\n    vector_store = PGVectorStore.create_sync(\n        engine=pg_engine,\n        table_name='test_table',\n        embedding_service=embeddings\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Pinecone\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-pinecone\n    ```\n\n    ```python  theme={null}\n    from langchain_pinecone import PineconeVectorStore\n    from pinecone import Pinecone\n\n    pc = Pinecone(api_key=...)\n    index = pc.Index(index_name)\n\n    vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Qdrant\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-qdrant\n    ```\n\n    ```python  theme={null}\n    from qdrant_client.models import Distance, VectorParams\n    from langchain_qdrant import QdrantVectorStore\n    from qdrant_client import QdrantClient\n\n    client = QdrantClient(\":memory:\")\n\n    vector_size = len(embeddings.embed_query(\"sample text\"))\n\n    if not client.collection_exists(\"test\"):\n        client.create_collection(\n            collection_name=\"test\",\n            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n        )\n    vector_store = QdrantVectorStore(\n        client=client,\n        collection_name=\"test\",\n        embedding=embeddings,\n    )\n    ```\n  &lt;/Tab&gt;\n&lt;/Tabs&gt;\n\n## 1. Indexing\n\n&lt;Note&gt;\n  **This section is an abbreviated version of the content in the [semantic search tutorial](https://docs.langchain.com/oss/python/langchain/knowledge-base).**\n\n  If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you're comfortable with [document loaders](https://docs.langchain.com/oss/python/langchain/retrieval#document_loaders), [embeddings](https://docs.langchain.com/oss/python/langchain/retrieval#embedding_models), and [vector stores](https://docs.langchain.com/oss/python/langchain/retrieval#vectorstores), feel free to skip to the next section on [retrieval and generation](https://docs.langchain.com/oss/python/langchain/rag#2-retrieval-and-generation).\n&lt;/Note&gt;\n\nIndexing commonly works as follows:\n\n1. **Load**: First we need to load our data. This is done with [Document Loaders](https://docs.langchain.com/oss/python/langchain/retrieval#document_loaders).\n2. **Split**: [Text splitters](https://docs.langchain.com/oss/python/langchain/retrieval#text_splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](https://docs.langchain.com/oss/python/langchain/retrieval#vectorstores) and [Embeddings](https://docs.langchain.com/oss/python/langchain/retrieval#embedding_models) model.\n\n&lt;img src=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=21403ce0d0c772da84dcc5b75cff4451\" alt=\"index_diagram\" data-og-width=\"2583\" width=\"2583\" data-og-height=\"1299\" height=\"1299\" data-path=\"images/rag_indexing.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=bf4eb8255b82a809dbbd2bc2a96d2ed7 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4ebc538b2c4765b609f416025e4dbbda 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=1838328a870c7353c42bf1cc2290a779 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=675f55e100bab5e2904d27db01775ccc 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4b9e544a7a3ec168651558bce854eb60 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=f5aeaaaea103128f374c03b05a317263 2500w\" /&gt;\n\n### Loading documents\n\nWe need to first load the blog post contents. We can use [DocumentLoaders](https://docs.langchain.com/oss/python/langchain/retrieval#document_loaders) for this, which are objects that load in data from a source and return a list of [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects.\n\nIn this case we'll use the [`WebBaseLoader`](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base), which uses `urllib` to load HTML from web URLs and `BeautifulSoup` to parse it to text. We can customize the HTML -&gt; text parsing by passing in parameters into the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we'll remove all others.\n\n```python  theme={null}\nimport bs4\nfrom langchain_community.document_loaders import WebBaseLoader\n\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs={\"parse_only\": bs4_strainer},\n)\ndocs = loader.load()\n\nassert len(docs) == 1\nprint(f\"Total characters: {len(docs[0].page_content)}\")\ntext  theme={null} Total characters: 43131\npython  theme={null} print(docs[0].page_content[:500])\n```text theme={null} LLM Powered Autonomous Agents\nDate: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In\n\n**Go deeper**\n\n`DocumentLoader`: Object that loads data from a source as list of `Documents`.\n\n* [Integrations](https://docs.langchain.com/oss/python/integrations/document_loaders/): 160+ integrations to choose from.\n* [`BaseLoader`](https://reference.langchain.com/python/langchain_core/document_loaders/#langchain_core.document_loaders.BaseLoader): API reference for the base interface.\n\n### Splitting documents\n\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n\nTo handle this we'll split the [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n\nAs in the [semantic search tutorial](https://docs.langchain.com/oss/python/langchain/knowledge-base), we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n\n```python  theme={null}\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # chunk size (characters)\n    chunk_overlap=200,  # chunk overlap (characters)\n    add_start_index=True,  # track index in original document\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\ntext  theme={null} Split blog post into 66 sub-documents.\nGo deeper\nTextSplitter: Object that splits a list of Document objects into smaller chunks for storage and retrieval.\n\nIntegrations\nInterface: API reference for the base interface.\n\n\n\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\n```python theme={null} document_ids = vector_store.add_documents(documents=all_splits)\nprint(document_ids[:3])\n\n```python  theme={null}\n['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\nGo deeper\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\n\nIntegrations: 30+ integrations to choose from.\nInterface: API reference for the base interface.\n\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\n\nIntegrations: 40+ integrations to choose from.\nInterface: API reference for the base interface.\n\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question."
  },
  {
    "objectID": "L01/05_RAG_agent.html#retrieval-and-generation-1",
    "href": "L01/05_RAG_agent.html#retrieval-and-generation-1",
    "title": "Build a RAG agent with LangChain",
    "section": "2. Retrieval and generation",
    "text": "2. Retrieval and generation\nRAG applications commonly work as follows:\n\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\n\n\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\n\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n\nRAG agents\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\n```python theme={null} from langchain.tools import tool\n@tool(response_format=‚Äúcontent_and_artifact‚Äù) def retrieve_context(query: str): ‚Äú‚Äú‚ÄúRetrieve information to help answer a query.‚Äù‚Äú‚Äù retrieved_docs = vector_store.similarity_search(query, k=2) serialized = ‚Äú‚Äù.join( (f‚ÄùSource: {doc.metadata}: {doc.page_content}‚Äú) for doc in retrieved_docs ) return serialized, retrieved_docs\n\n&lt;Tip&gt;\n  Here we use the [tool decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) to configure the tool to attach raw documents as [artifacts](https://docs.langchain.com/oss/python/langchain/messages#param-artifact) to each [ToolMessage](https://docs.langchain.com/oss/python/langchain/messages#tool-message). This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n&lt;/Tip&gt;\n\n&lt;Tip&gt;\n  Retrieval tools are not limited to a single string `query` argument, as in the above example. You can\n  force the LLM to specify additional search parameters by adding arguments‚Äî for example, a category:\n\n  ```python  theme={null}\n  from typing import Literal\n\n  def retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\n\nGiven our tool, we can construct the agent:\n```python theme={null} from langchain.agents import create_agent\ntools = [retrieve_context] # If desired, specify custom instructions prompt = ( ‚ÄúYou have access to a tool that retrieves context from a blog post.‚Äù ‚ÄúUse the tool to help answer user queries.‚Äù ) agent = create_agent(model, tools, system_prompt=prompt)\n\nLet's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\n\n```python  theme={null}\nquery = (\n    \"What is the standard method for Task Decomposition?\\n\\n\"\n    \"Once you get the answer, look up common extensions of that method.\"\n)\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    event[\"messages\"][-1].pretty_print()\n================================ Human Message =================================\n\nWhat is the standard method for Task Decomposition?\n\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\n  Args:\n    query: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\n  Args:\n    query: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\n\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\n\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\n\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n\nYou can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph‚Äôs Agentic RAG tutorial for more advanced formulations.\n\n\n\nRAG chains\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\n\n\n\n\n\n\n‚úÖ Benefits\n‚ö†Ô∏è Drawbacks\n\n\n\n\nSearch only when needed ‚Äì The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls ‚Äì When a search is performed, it requires one call to generate the query and another to produce the final response.\n\n\nContextual search queries ‚Äì By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.\nReduced control ‚Äì The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\n\n\nMultiple searches allowed ‚Äì The LLM can execute several searches in support of a single user query.\n\n\n\n\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\n```python theme={null} from langchain.agents.middleware import dynamic_prompt, ModelRequest\n@dynamic_prompt def prompt_with_context(request: ModelRequest) -&gt; str: ‚Äú‚Äú‚ÄúInject context into state messages.‚Äù‚Äú‚Äù last_query = request.state[‚Äúmessages‚Äù][-1].text retrieved_docs = vector_store.similarity_search(last_query)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n\nsystem_message = (\n    \"You are a helpful assistant. Use the following context in your response:\"\n    f\"\\n\\n{docs_content}\"\n)\n\nreturn system_message\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\n\nLet's try this out:\n\n```python  theme={null}\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n================================ Human Message =================================\n\nWhat is task decomposition?\n================================== Ai Message ==================================\n\nTask decomposition is...\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\n The above RAG chain incorporates retrieved context into a single system message for that run.\nAs in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\n\nAdding a key to the state to store the retrieved documents\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\n\n```python theme={null} from typing import Any from langchain_core.documents import Document from langchain.agents.middleware import AgentMiddleware, AgentState\nclass State(AgentState): context: list[Document]\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]): state_schema = State\n  def before_model(self, state: AgentState) -&gt; dict[str, Any] | None:\n      last_message = state[\"messages\"][-1]\n      retrieved_docs = vector_store.similarity_search(last_message.text)\n\n      docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n\n      augmented_message_content = (\n          f\"{last_message.text}\\n\\n\"\n          \"Use the following context to answer the query:\\n\"\n          f\"{docs_content}\"\n      )\n      return {\n          \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\n          \"context\": retrieved_docs,\n      }\nagent = create_agent( model, tools=[], middleware=[RetrieveDocumentsMiddleware()], ) ```"
  },
  {
    "objectID": "L01/05_RAG_agent.html#next-steps",
    "href": "L01/05_RAG_agent.html#next-steps",
    "title": "Build a RAG agent with LangChain",
    "section": "Next steps",
    "text": "Next steps\nNow that we‚Äôve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\n\nStream tokens and other information for responsive user experiences\nAdd conversational memory to support multi-turn interactions\nAdd long-term memory to support memory across conversational threads\nAdd structured responses\nDeploy your application with LangSmith Deployment"
  },
  {
    "objectID": "L01/03_how_agents_work.html",
    "href": "L01/03_how_agents_work.html",
    "title": "LLMs and augmentations",
    "section": "",
    "text": "Workflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs."
  },
  {
    "objectID": "L01/03_how_agents_work.html#structured-output",
    "href": "L01/03_how_agents_work.html#structured-output",
    "title": "LLMs and augmentations",
    "section": "Structured Output",
    "text": "Structured Output\nStructured outputs are a way to extract structured data from the LLM‚Äôs response.\n\n# Schema for structured output\nfrom pydantic import BaseModel, Field\n\n\nclass SearchQuery(BaseModel):\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n    justification: str = Field(\n        None, description=\"Why this query is relevant to the user's request.\"\n    )\n\n# Augment the LLM with schema for structured output\nstructured_llm = llm.with_structured_output(SearchQuery)\n\n# Invoke the augmented LLM\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=SearchQuery(search_query=...-to-date explanations.'), input_type=SearchQuery])\n  return self.__pydantic_serializer__.to_python(\n\n\n\nprint(\"search_query:\", output.search_query)\nprint(\"justification:\", output.justification)\n\nsearch_query: Calcium CT score relationship with high cholesterol atherosclerosis risk\njustification: The user wants to understand how a coronary artery calcium (CAC) score derived from CT imaging connects to elevated blood cholesterol levels. This requires information on the pathophysiology of atherosclerotic plaque formation, the role of lipid disorders, and how CAC scoring is used clinically to assess cardiovascular risk in the context of cholesterol. The query targets medical literature and guidelines that discuss these relationships, making it the most relevant search term for retrieving accurate, up-to-date explanations."
  },
  {
    "objectID": "L01/03_how_agents_work.html#tool-calling",
    "href": "L01/03_how_agents_work.html#tool-calling",
    "title": "LLMs and augmentations",
    "section": "Tool Calling",
    "text": "Tool Calling\nTools connect the LLM to the outside world.\n\nfrom langchain.tools import tool\n\n# Define a tool\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiplies two numbers.\"\"\"\n    return a * b\n\n\n# Bind (potentially multiple) tools to the model\nllm_with_tools = llm.bind_tools([multiply])\n\nStep 1: Model generates tool calls\n\nmessages = [{\"role\": \"user\", \"content\": \"What is 2 times 3\"}]\nai_msg = llm_with_tools.invoke(messages)\nmessages.append(ai_msg)\n\n\nmessages\n\n[{'role': 'user', 'content': 'What is 2 times 3'},\n AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 295, 'total_tokens': 414, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 82, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771681856-uclSTv2ef3GhPvPUeA98', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c8077-b908-7fd1-aa62-f083b42d90f6-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_fb2407be30fa4f989ea9a4c6', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 295, 'output_tokens': 119, 'total_tokens': 414, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 82}})]\n\n\n\n# Get the tool call\nmessages[-1].tool_calls\n\n[{'name': 'multiply',\n  'args': {'a': 2, 'b': 3},\n  'id': 'call_fb2407be30fa4f989ea9a4c6',\n  'type': 'tool_call'}]\n\n\nStep 2: Execute tools and collect results\n\nfor tool_call in ai_msg.tool_calls:\n    # Execute the tool with the generated arguments\n    tool_result = multiply.invoke(tool_call)\n    messages.append(tool_result)\n\n\nmessages\n\n[{'role': 'user', 'content': 'What is 2 times 3'},\n AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 295, 'total_tokens': 414, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 82, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771681856-uclSTv2ef3GhPvPUeA98', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c8077-b908-7fd1-aa62-f083b42d90f6-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_fb2407be30fa4f989ea9a4c6', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 295, 'output_tokens': 119, 'total_tokens': 414, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 82}}),\n ToolMessage(content='6', name='multiply', tool_call_id='call_fb2407be30fa4f989ea9a4c6')]\n\n\nStep 3: Pass results back to model for final response\n\nfinal_response = llm_with_tools.invoke(messages)\nprint(final_response.text)\n\nThe product of 2 and 3 is 6. \n\n&lt;final_answer&gt;\n6\n&lt;/final_answer&gt;\n\n\nEssentially, this is what the implementation of the agent resulting from create_agent, in langchain is all about."
  },
  {
    "objectID": "L01/07_workflows.html",
    "href": "L01/07_workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "The Functional API allows you to add LangGraph‚Äôs key features ‚Äî persistence, memory, human-in-the-loop, and streaming ‚Äî to your applications with minimal changes to your existing code.\nIt is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\nThe Functional API uses two key building blocks:\n\n@entrypoint ‚Äì Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\n@task ‚Äì Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\n\nThis provides a minimal abstraction for building workflows with state management and streaming."
  },
  {
    "objectID": "L01/07_workflows.html#understanding-and-using-functional-api",
    "href": "L01/07_workflows.html#understanding-and-using-functional-api",
    "title": "Workflows",
    "section": "",
    "text": "The Functional API allows you to add LangGraph‚Äôs key features ‚Äî persistence, memory, human-in-the-loop, and streaming ‚Äî to your applications with minimal changes to your existing code.\nIt is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\nThe Functional API uses two key building blocks:\n\n@entrypoint ‚Äì Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\n@task ‚Äì Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\n\nThis provides a minimal abstraction for building workflows with state management and streaming."
  },
  {
    "objectID": "L01/07_workflows.html#entrypoint",
    "href": "L01/07_workflows.html#entrypoint",
    "title": "Workflows",
    "section": "Entrypoint",
    "text": "Entrypoint\nThe @entrypoint decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.\n\nInjectable parameters\nWhen declaring an entrypoint, you can request access to additional parameters that will be injected automatically at runtime. These parameters include:\n\n\n\nParameter\nDescription\n\n\n\n\nprevious\nAccess the state associated with the previous checkpoint for the given thread. See short-term-memory.\n\n\nstore\nAn instance of [BaseStore][langgraph.store.base.BaseStore]. Useful for long-term memory.\n\n\nwriter\nUse to access the StreamWriter when working with Async Python &lt; 3.11. See streaming with functional API for details.\n\n\nconfig\nFor accessing run time configuration. See RunnableConfig for information.\n\n\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.func import entrypoint\nfrom langgraph.store.base import BaseStore\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import StreamWriter\n\nin_memory_checkpointer = InMemorySaver(...)\nin_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory\n\n@entrypoint(\n    checkpointer=in_memory_checkpointer,  # Specify the checkpointer\n    store=in_memory_store  # Specify the store\n)\ndef my_workflow(\n    some_input: dict,  # The input (e.g., passed via `invoke`)\n    *,\n    previous: Any = None, # For short-term memory\n    store: BaseStore,  # For long-term memory\n    writer: StreamWriter,  # For streaming custom data\n    config: RunnableConfig  # For accessing the configuration passed to the entrypoint\n) -&gt; ...:\n\n\nShort-term memory\nWhen an entrypoint is defined with a checkpointer, it stores information between successive invocations on the same thread id in checkpoints.\nThis allows accessing the state from the previous invocation using the previous parameter.\nBy default, the previous parameter is the return value of the previous invocation.\n\nfrom typing import Any\nfrom langgraph.func import entrypoint\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Create a checkpointer\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(number: int, *, previous: Any = None) -&gt; int:\n    previous = previous or 0\n    return number + previous\n\nNote: the * in the function signature marks the end of positional arguments and the start of keyword-only arguments.\nInvocation with config.configurable.thread_id which allows for consecutive calls to be grouped under the same ‚Äúconversation‚Äù or ‚Äúsession‚Äù.\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\n\nmy_workflow.invoke(1, config=config)  # 1 (previous was None)\n\n1\n\n\n\nmy_workflow.invoke(2, config=config)  # 3 (previous was 1 from the previous \n\n3\n\n\nThe checkpointer saves the state of the workflow between invocations.\n\n\nLong-term memory\n‚Ä¶\n\n\nStreaming\n‚Ä¶"
  },
  {
    "objectID": "L01/07_workflows.html#task",
    "href": "L01/07_workflows.html#task",
    "title": "Workflows",
    "section": "Task",
    "text": "Task\nA task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\n\nAsynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\nCheckpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details).\n\nfrom langgraph.func import task\n\n@task()\ndef slow_computation(input_value):\n    # Simulate a long-running operation\n    ...\n    return result\n\n\n\n\n\n\nWarning\n\n\n\nSerialization: The outputs of tasks must be JSON-serializable to support checkpointing.\n\n\n\nExecution\nTasks can only be called from within an entrypoint, another task, or a state graph node.\nTasks cannot be called directly from the main application code.\nWhen you call a task, it returns immediately with a future object. A future is a placeholder for a result that will be available later.\nTo obtain the result of a task, you can either wait for it synchronously (using result()) or await it asynchronously (using await).\nWait for the result synchronously:\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(some_input: int) -&gt; int:\n    future = slow_computation(some_input)\n    result = future.result()\n    return result\nAwait result asynchronously:\n@entrypoint(checkpointer=checkpointer)\nasync def my_workflow(some_input: int) -&gt; int:\n    future = slow_computation(some_input)\n    result = await future\n    return result\n\n\n## When to use a task\n\n**Tasks** are useful in the following scenarios:\n\n**Checkpointing**: When you need to save the result of a long-running operation to a checkpoint, so you don't need to recompute it when resuming the workflow.\n\n**Human-in-the-loop**: If you're building a workflow that requires human intervention, you MUST use **tasks** to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the [determinism](#determinism) section for more details.\n\n**Parallel Execution**: For I/O-bound tasks, **tasks** enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\n\n**Observability**: Wrapping operations in **tasks** provides a way to track the progress of the workflow and monitor the execution of individual operations using [LangSmith](https://docs.langchain.com/langsmith/home).\n\n**Retryable Work**: When work needs to be retried to handle failures or inconsistencies, **tasks** provide a way to encapsulate and manage the retry logic.\n\n# Using the Functional API\n\n## Creating a simple workflow\n\nWhen defining an `entrypoint`, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.\n\n::: {#2f80fe08 .cell execution_count=15}\n``` {.python .cell-code}\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -&gt; int:\n    value = inputs[\"value\"]\n    another_value = inputs[\"another_value\"]\n    \n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\",\n    }\n}\n\nmy_workflow.invoke({\"value\": 1, \"another_value\": 2}, config=config)\n:::\nExtended example: simple workflow\n\nimport uuid\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Task that checks if a number is even\n@task\ndef is_even(number: int) -&gt; bool:\n    return number % 2 == 0\n\n# Task that formats a message\n@task\ndef format_message(is_even: bool) -&gt; str:\n    return \"The number is even.\" if is_even else \"The number is odd.\"\n\n# Create a checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(inputs: dict) -&gt; str:\n    \"\"\"Simple workflow to classify a number.\"\"\"\n    even = is_even(inputs[\"number\"]).result()\n    return format_message(even).result()\n\n# Run the workflow with a unique thread ID\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nresult = workflow.invoke({\"number\": 7}, config=config)\nprint(result)\n\nThis example demonstrates how to use the@task and @entrypoint decorators syntactically. Given that a checkpointer is provided, the workflow results will be persisted in the checkpointer.\n\nimport uuid\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nmodel = init_chat_model('gpt-3.5-turbo')\n\n# Task: generate essay using an LLM\n@task\ndef compose_essay(topic: str) -&gt; str:\n    \"\"\"Generate an essay about the given topic.\"\"\"\n    return model.invoke([\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes essays.\"},\n        {\"role\": \"user\", \"content\": f\"Write an essay about {topic}.\"}\n    ]).content\n\n# Create a checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(topic: str) -&gt; str:\n    \"\"\"Simple workflow that generates an essay with an LLM.\"\"\"\n    result = compose_essay(topic).result()\n    return result\n\n# Execute the workflow\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nresult = workflow.invoke(\"the history of flight\", config=config)\nprint(result)"
  },
  {
    "objectID": "L01/07_workflows.html#parallel-execution",
    "href": "L01/07_workflows.html#parallel-execution",
    "title": "Workflows",
    "section": "Parallel execution",
    "text": "Parallel execution\nTasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).\n\n@task\ndef add_one(number: int) -&gt; int:\n    return number + 1\n\n@entrypoint(checkpointer=checkpointer)\ndef graph(numbers: list[int]) -&gt; list[str]:\n    futures = [add_one(i) for i in numbers]\n    return [f.result() for f in futures]\n\nThis example demonstrates how to run multiple LLM calls in parallel using @task. Each call generates a paragraph on a different topic, and results are joined into a single text output.\n\nimport uuid\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Initialize the LLM model\nmodel = init_chat_model(\"gpt-3.5-turbo\")\n\n# Task that generates a paragraph about a given topic\n@task\ndef generate_paragraph(topic: str) -&gt; str:\n    response = model.invoke([\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes educational paragraphs.\"},\n        {\"role\": \"user\", \"content\": f\"Write a paragraph about {topic}.\"}\n    ])\n    return response.content\n\n# Create a checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(topics: list[str]) -&gt; str:\n    \"\"\"Generates multiple paragraphs in parallel and combines them.\"\"\"\n    futures = [generate_paragraph(topic) for topic in topics]\n    paragraphs = [f.result() for f in futures]\n    return \"\\n\\n\".join(paragraphs)\n\n# Run the workflow\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nresult = workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config=config)\nprint(result)"
  },
  {
    "objectID": "L01/08_patterns.html",
    "href": "L01/08_patterns.html",
    "title": "Workflows and Agents",
    "section": "",
    "text": "This guide reviews common workflow and agent patterns.\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment."
  },
  {
    "objectID": "L01/08_patterns.html#setup",
    "href": "L01/08_patterns.html#setup",
    "title": "Workflows and Agents",
    "section": "Setup",
    "text": "Setup\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\n\nInstall dependencies:\n\nuv add langchain_core langchain-anthropic langgraph\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# We use OpenRouter for the agent ‚Äî set OPENROUTER_API_KEY in .env\n# Get your key at https://openrouter.ai/keys\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    raise RuntimeError(\n        \"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:\\n\"\n        \"OPENROUTER_API_KEY=your-openrouter-api-key\"\n    )\n\n\nInitialize the LLM:\n\n\nfrom langchain_openai import ChatOpenAI\n\n# https://openrouter.ai/openai/gpt-5-nano\n# model_gpt5_nano = ChatOpenAI(\n#     model=\"openai/gpt-5-nano\",\n#     temperature=0,\n#     base_url=\"https://openrouter.ai/api/v1\",\n#     api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n# )\n\n# https://openrouter.ai/nvidia/nemotron-3-nano-30b-a3b:free\nllm = ChatOpenAI(\n    model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n    temperature=0,\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n)\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "L01/08_patterns.html#prompt-chaining",
    "href": "L01/08_patterns.html#prompt-chaining",
    "title": "Workflows and Agents",
    "section": "Prompt chaining",
    "text": "Prompt chaining\nPrompt chaining is when each LLM call processes the output of the previous call. It‚Äôs often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\n\nTranslating documents into different languages\nVerifying generated content for consistency\n\n\n\nfrom langgraph.func import task\n\n\n# Tasks\n@task\ndef generate_joke(topic: str):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\n    return msg.content\n\n\ndef check_punchline(joke: str):\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\n    # Simple check - does the joke contain \"?\" or \"!\"\n    if \"?\" in joke or \"!\" in joke:\n        return \"Fail\"\n\n    return \"Pass\"\n\n\n@task\ndef improve_joke(joke: str):\n    \"\"\"Second LLM call to improve the joke\"\"\"\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\n    return msg.content\n\n\n@task\ndef polish_joke(joke: str):\n    \"\"\"Third LLM call for final polish\"\"\"\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\n    return msg.content\n\n\nfrom langgraph.func import entrypoint\n\n@entrypoint()\ndef prompt_chaining_workflow(topic: str):\n    original_joke = generate_joke(topic).result()\n    if check_punchline(original_joke) == \"Pass\":\n        return original_joke\n\n    improved_joke = improve_joke(original_joke).result()\n    return polish_joke(improved_joke).result()\n\n\n# Invoke\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n\n{'generate_joke': 'Here\\'s a short, purr-fect joke for you:  \\n\\n&gt; *My cat knocked over my coffee.  \\n&gt; It was purr-fect.* üò∏  \\n\\n*(Bonus: It‚Äôs short, uses a cat pun, and the \"purr-fect\" twist lands in 5 words!)*'}\n\n\n{'improve_joke': '**My cat knocked over my coffee‚Äîtalk about a *purr‚Äëfect* disaster!**  \\nNow I‚Äôm *espresso‚Äëly* cat‚Äëastrophic. ‚òïüò∏  \\n\\n*(Wordplay added: ‚Äúpurr‚Äëfect‚Äù ‚Üí perfect, ‚Äúespresso‚Äëly‚Äù ‚Üí especially, ‚Äúcat‚Äëastrophic‚Äù ‚Üí catastrophic.)*'}\n\n\n{'polish_joke': '**My cat knocked over my coffee‚Äîtalk about a *purr‚Äëfect* disaster!**  \\nNow I‚Äôm *espresso‚Äëly* cat‚Äëastrophic. ‚òïüò∏  \\n\\n*But here‚Äôs the twist:* the little furball didn‚Äôt just spill the brew‚Äîhe **re‚Äëprogrammed the coffee maker to dispense catnip instead of caffeine**.  \\n\\nSo now every time I reach for a pick‚Äëme‚Äëup, I‚Äôm actually getting a **‚Äúpurr‚Äëcasso‚Äù** of espresso‚Äëinfused catnip, and the cat‚Äôs proudly serving it up with a side of whisker‚Äëtwitching swagger.  \\n\\n*Bottom line:* I‚Äôm not just *cat‚Äëastrophic* anymore‚ÄîI‚Äôm **caffeinated‚Äëand‚Äëcat‚Äëified**. üêæ‚ú®'}\n\n\n{'prompt_chaining_workflow': '**My cat knocked over my coffee‚Äîtalk about a *purr‚Äëfect* disaster!**  \\nNow I‚Äôm *espresso‚Äëly* cat‚Äëastrophic. ‚òïüò∏  \\n\\n*But here‚Äôs the twist:* the little furball didn‚Äôt just spill the brew‚Äîhe **re‚Äëprogrammed the coffee maker to dispense catnip instead of caffeine**.  \\n\\nSo now every time I reach for a pick‚Äëme‚Äëup, I‚Äôm actually getting a **‚Äúpurr‚Äëcasso‚Äù** of espresso‚Äëinfused catnip, and the cat‚Äôs proudly serving it up with a side of whisker‚Äëtwitching swagger.  \\n\\n*Bottom line:* I‚Äôm not just *cat‚Äëastrophic* anymore‚ÄîI‚Äôm **caffeinated‚Äëand‚Äëcat‚Äëified**. üêæ‚ú®'}"
  },
  {
    "objectID": "L01/08_patterns.html#parallelization",
    "href": "L01/08_patterns.html#parallelization",
    "title": "Workflows and Agents",
    "section": "Parallelization",
    "text": "Parallelization\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\n\nSplit up subtasks and run them in parallel, which increases speed\nRun tasks multiple times to check for different outputs, which increases confidence\n\nSome examples include:\n\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\n\n\n\n@task\ndef call_llm_1(topic: str):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n    msg = llm.invoke(f\"Write a joke about {topic}\")\n    return msg.content\n\n\n@task\ndef call_llm_2(topic: str):\n    \"\"\"Second LLM call to generate story\"\"\"\n    msg = llm.invoke(f\"Write a story about {topic}\")\n    return msg.content\n\n\n@task\ndef call_llm_3(topic):\n    \"\"\"Third LLM call to generate poem\"\"\"\n    msg = llm.invoke(f\"Write a poem about {topic}\")\n    return msg.content\n\n\n@task\ndef aggregator(topic, joke, story, poem):\n    \"\"\"Combine the joke and story into a single output\"\"\"\n\n    combined = f\"Here's a story, joke, and poem about {topic}!\\n\\n\"\n    combined += f\"STORY:\\n{story}\\n\\n\"\n    combined += f\"JOKE:\\n{joke}\\n\\n\"\n    combined += f\"POEM:\\n{poem}\"\n    return combined\n\n\n# Build workflow\n@entrypoint()\ndef parallel_workflow(topic: str):\n    joke_fut = call_llm_1(topic)\n    story_fut = call_llm_2(topic)\n    poem_fut = call_llm_3(topic)\n    return aggregator(\n        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\n    ).result()\n\n\n# Invoke\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n\n{'call_llm_3': '**Whiskers in the Moonlight**\\n\\nIn the hush of night‚Äôs soft sigh,  \\nA shadow slips on velvet paws‚Äî  \\nEyes like amber lanterns high,  \\nA silent hunter, caught in awe.\\n\\nShe curls around the world‚Äôs warm seam,  \\nA purr that rolls like rolling tide;  \\nEach ripple sings a secret dream,  \\nA lullaby where hearts can hide.\\n\\nShe stalks the sunbeams on the sill,  \\nA tiger in a tuxedoed coat;  \\nShe leaps, she lands, she never will‚Äî  \\nMiss a beat, she owns the float.\\n\\nHer tail, a question mark, unfurls,  \\nA comet tracing lazy arcs;  \\nShe paints the air with silent swirls,  \\nAnd leaves a trail of quiet sparks.\\n\\nWhen dawn awakes with amber glow,  \\nShe stretches, yawns, and claims the day;  \\nA regal queen of softest glow,  \\nShe rules the world in whiskered sway.\\n\\nSo here‚Äôs to cats‚Äîboth shy and bold‚Äî  \\nThe poets of the feline kind;  \\nIn every purr, a story told,  \\nA mystery we‚Äôll never fully find.'}\n\n\n{'call_llm_1': \"Here's a purr-fectly simple one for you:  \\n\\n&gt; *Why did the cat get kicked out of the party?*  \\n&gt; *Because it kept knocking over the punch bowl... and then *paw*-tying on the floor!* üò∏  \\n\\n*(Bonus groan: It was a *cat*-astrophe!)*\"}\n\n\n{'call_llm_2': '\\n**The Midnight Library**\\n\\nWhen the clock struck twelve in the sleepy town of Willowbrook, the old stone library on Main Street began to hum with a sound no one could quite place. It wasn‚Äôt the creak of the ancient wooden floorboards, nor the whisper of the wind through the cracked stained‚Äëglass windows. It was a soft, rhythmic purring that seemed to rise from the very shelves themselves.\\n\\nThe source of the purring was a sleek, silver‚Äëtabby cat named **Mira**. She had appeared one foggy evening a month earlier, slipping through the cracked door of the library as if she owned the place. The townsfolk had watched her with a mixture of curiosity and amusement as she padded between the rows of books, her tail flicking in time with the rustle of pages. She never knocked anything over, never scratched a single tome‚Äîshe simply settled herself on a high stool near the reference desk and began to read.\\n\\nMira‚Äôs eyes were a deep amber, and they seemed to glow whenever she turned a page. She would stare at the words as if they were tiny constellations, tracing their shapes with a paw that hovered just above the paper. The librarians, Mrs. Penelope Hargrove and her grandson, Theo, soon realized that Mira was not just any cat. She could understand the stories she read, and more astonishingly, she could *write* them.\\n\\nOne rainy night, as thunder rattled the panes, a stray kitten named **Pip** slipped into the library, shivering and soaked. Pip was a tiny, mottled gray furball with oversized ears that twitched at every sound. He tried to hide behind a stack of encyclopedias, but Mira‚Äôs gentle nudge guided him toward a warm spot on a plush armchair. She lowered her head and brushed her whiskers against his cheek, as if saying, ‚ÄúYou‚Äôre safe here.‚Äù\\n\\nThe next morning, Mrs. Hargrove found a handwritten note tucked between the pages of *The Secret Garden*. It read:\\n\\n&gt; *‚ÄúThe garden is not just a place of flowers, but a sanctuary for those who listen. Come, little one, and hear the stories the wind tells.‚Äù*\\n\\nShe looked up to see Mira perched on the arm of the chair, her tail curled around Pip, who was now curled up, eyes half‚Äëclosed, listening to the soft rustle of pages. The kitten‚Äôs ears perked up whenever a new sentence was spoken aloud, as if the words themselves were a lullaby.\\n\\nFrom that day on, the Midnight Library became a haven for more than just books. Animals of all kinds‚Äîsquirrels with bright eyes, a shy hedgehog named Quill, even an old barn owl that occasionally swooped in through the open window‚Äîfound their way to the quiet sanctuary. Each creature was greeted by Mira with a soft purr and a gentle nudge toward a spot where they could curl up and listen.\\n\\nMira‚Äôs true talent, however, was not just in reading or comforting. She could *weave* stories from the thoughts and feelings that swirled in the hearts of those who entered. When a child cried over a lost toy, Mira would curl up beside them and, with a flick of her tail, conjure a tale of a brave mouse who embarked on a daring rescue mission. When an elderly man sighed with nostalgia, she would settle on his lap and spin a yarn about a distant sea voyage that seemed to echo his own memories.\\n\\nOne evening, as the town prepared for its annual Harvest Festival, a sudden storm rolled in, threatening to cancel the celebrations. The townsfolk gathered in the library, worried that the rain would wash away their plans. Mira leapt onto the central reading table, her paws landing softly on a stack of old maps. She stared at the ceiling, then at the anxious faces around her, and began to purr‚Äîa deep, resonant sound that seemed to vibrate through the very walls.\\n\\nAs the purring grew louder, the lights flickered, and a soft glow began to emanate from the books themselves. The pages fluttered, and words rose off the paper like fireflies, forming a luminous tapestry across the ceiling. The story that unfolded was one of a brave cat who, during a storm, guided a lost flock of birds back to safety by leading them through a hidden tunnel beneath the town. The tale ended with a promise: *‚ÄúWhen the rain falls, the heart of the library shines brighter than any lantern.‚Äù*\\n\\nThe next morning, the storm had passed, and the sky cleared to a brilliant sunrise. The townspeople emerged to find the streets glistening, but more importantly, they found a renewed sense of hope. The Harvest Festival went ahead, brighter than ever, with lanterns hanging from the library‚Äôs windows, each one reflecting the story Mira had told.\\n\\nFrom that night on, Mira was no longer just a cat who liked to read. She became the **Guardian of Stories**, a silent protector who ensured that every heart that entered the library left with a tale to carry forward. And whenever a new creature‚Äîbe it a trembling kitten, a weary traveler, or a curious squirrel‚Äîstepped through the doors, they would find a warm spot, a gentle purr, and perhaps, if they listened closely, a story waiting to be written.\\n\\nAnd so, the Midnight Library continued to hum with the soft purring of a silver‚Äëtabby cat, its shelves alive with whispered adventures, and its heart forever open to the magic that only stories‚Äîand a few well‚Äëplaced purrs‚Äîcan bring.'}\n\n\n{'aggregator': \"Here's a story, joke, and poem about cats!\\n\\nSTORY:\\n\\n**The Midnight Library**\\n\\nWhen the clock struck twelve in the sleepy town of Willowbrook, the old stone library on Main Street began to hum with a sound no one could quite place. It wasn‚Äôt the creak of the ancient wooden floorboards, nor the whisper of the wind through the cracked stained‚Äëglass windows. It was a soft, rhythmic purring that seemed to rise from the very shelves themselves.\\n\\nThe source of the purring was a sleek, silver‚Äëtabby cat named **Mira**. She had appeared one foggy evening a month earlier, slipping through the cracked door of the library as if she owned the place. The townsfolk had watched her with a mixture of curiosity and amusement as she padded between the rows of books, her tail flicking in time with the rustle of pages. She never knocked anything over, never scratched a single tome‚Äîshe simply settled herself on a high stool near the reference desk and began to read.\\n\\nMira‚Äôs eyes were a deep amber, and they seemed to glow whenever she turned a page. She would stare at the words as if they were tiny constellations, tracing their shapes with a paw that hovered just above the paper. The librarians, Mrs. Penelope Hargrove and her grandson, Theo, soon realized that Mira was not just any cat. She could understand the stories she read, and more astonishingly, she could *write* them.\\n\\nOne rainy night, as thunder rattled the panes, a stray kitten named **Pip** slipped into the library, shivering and soaked. Pip was a tiny, mottled gray furball with oversized ears that twitched at every sound. He tried to hide behind a stack of encyclopedias, but Mira‚Äôs gentle nudge guided him toward a warm spot on a plush armchair. She lowered her head and brushed her whiskers against his cheek, as if saying, ‚ÄúYou‚Äôre safe here.‚Äù\\n\\nThe next morning, Mrs. Hargrove found a handwritten note tucked between the pages of *The Secret Garden*. It read:\\n\\n&gt; *‚ÄúThe garden is not just a place of flowers, but a sanctuary for those who listen. Come, little one, and hear the stories the wind tells.‚Äù*\\n\\nShe looked up to see Mira perched on the arm of the chair, her tail curled around Pip, who was now curled up, eyes half‚Äëclosed, listening to the soft rustle of pages. The kitten‚Äôs ears perked up whenever a new sentence was spoken aloud, as if the words themselves were a lullaby.\\n\\nFrom that day on, the Midnight Library became a haven for more than just books. Animals of all kinds‚Äîsquirrels with bright eyes, a shy hedgehog named Quill, even an old barn owl that occasionally swooped in through the open window‚Äîfound their way to the quiet sanctuary. Each creature was greeted by Mira with a soft purr and a gentle nudge toward a spot where they could curl up and listen.\\n\\nMira‚Äôs true talent, however, was not just in reading or comforting. She could *weave* stories from the thoughts and feelings that swirled in the hearts of those who entered. When a child cried over a lost toy, Mira would curl up beside them and, with a flick of her tail, conjure a tale of a brave mouse who embarked on a daring rescue mission. When an elderly man sighed with nostalgia, she would settle on his lap and spin a yarn about a distant sea voyage that seemed to echo his own memories.\\n\\nOne evening, as the town prepared for its annual Harvest Festival, a sudden storm rolled in, threatening to cancel the celebrations. The townsfolk gathered in the library, worried that the rain would wash away their plans. Mira leapt onto the central reading table, her paws landing softly on a stack of old maps. She stared at the ceiling, then at the anxious faces around her, and began to purr‚Äîa deep, resonant sound that seemed to vibrate through the very walls.\\n\\nAs the purring grew louder, the lights flickered, and a soft glow began to emanate from the books themselves. The pages fluttered, and words rose off the paper like fireflies, forming a luminous tapestry across the ceiling. The story that unfolded was one of a brave cat who, during a storm, guided a lost flock of birds back to safety by leading them through a hidden tunnel beneath the town. The tale ended with a promise: *‚ÄúWhen the rain falls, the heart of the library shines brighter than any lantern.‚Äù*\\n\\nThe next morning, the storm had passed, and the sky cleared to a brilliant sunrise. The townspeople emerged to find the streets glistening, but more importantly, they found a renewed sense of hope. The Harvest Festival went ahead, brighter than ever, with lanterns hanging from the library‚Äôs windows, each one reflecting the story Mira had told.\\n\\nFrom that night on, Mira was no longer just a cat who liked to read. She became the **Guardian of Stories**, a silent protector who ensured that every heart that entered the library left with a tale to carry forward. And whenever a new creature‚Äîbe it a trembling kitten, a weary traveler, or a curious squirrel‚Äîstepped through the doors, they would find a warm spot, a gentle purr, and perhaps, if they listened closely, a story waiting to be written.\\n\\nAnd so, the Midnight Library continued to hum with the soft purring of a silver‚Äëtabby cat, its shelves alive with whispered adventures, and its heart forever open to the magic that only stories‚Äîand a few well‚Äëplaced purrs‚Äîcan bring.\\n\\nJOKE:\\nHere's a purr-fectly simple one for you:  \\n\\n&gt; *Why did the cat get kicked out of the party?*  \\n&gt; *Because it kept knocking over the punch bowl... and then *paw*-tying on the floor!* üò∏  \\n\\n*(Bonus groan: It was a *cat*-astrophe!)*\\n\\nPOEM:\\n**Whiskers in the Moonlight**\\n\\nIn the hush of night‚Äôs soft sigh,  \\nA shadow slips on velvet paws‚Äî  \\nEyes like amber lanterns high,  \\nA silent hunter, caught in awe.\\n\\nShe curls around the world‚Äôs warm seam,  \\nA purr that rolls like rolling tide;  \\nEach ripple sings a secret dream,  \\nA lullaby where hearts can hide.\\n\\nShe stalks the sunbeams on the sill,  \\nA tiger in a tuxedoed coat;  \\nShe leaps, she lands, she never will‚Äî  \\nMiss a beat, she owns the float.\\n\\nHer tail, a question mark, unfurls,  \\nA comet tracing lazy arcs;  \\nShe paints the air with silent swirls,  \\nAnd leaves a trail of quiet sparks.\\n\\nWhen dawn awakes with amber glow,  \\nShe stretches, yawns, and claims the day;  \\nA regal queen of softest glow,  \\nShe rules the world in whiskered sway.\\n\\nSo here‚Äôs to cats‚Äîboth shy and bold‚Äî  \\nThe poets of the feline kind;  \\nIn every purr, a story told,  \\nA mystery we‚Äôll never fully find.\"}\n\n\n{'parallel_workflow': \"Here's a story, joke, and poem about cats!\\n\\nSTORY:\\n\\n**The Midnight Library**\\n\\nWhen the clock struck twelve in the sleepy town of Willowbrook, the old stone library on Main Street began to hum with a sound no one could quite place. It wasn‚Äôt the creak of the ancient wooden floorboards, nor the whisper of the wind through the cracked stained‚Äëglass windows. It was a soft, rhythmic purring that seemed to rise from the very shelves themselves.\\n\\nThe source of the purring was a sleek, silver‚Äëtabby cat named **Mira**. She had appeared one foggy evening a month earlier, slipping through the cracked door of the library as if she owned the place. The townsfolk had watched her with a mixture of curiosity and amusement as she padded between the rows of books, her tail flicking in time with the rustle of pages. She never knocked anything over, never scratched a single tome‚Äîshe simply settled herself on a high stool near the reference desk and began to read.\\n\\nMira‚Äôs eyes were a deep amber, and they seemed to glow whenever she turned a page. She would stare at the words as if they were tiny constellations, tracing their shapes with a paw that hovered just above the paper. The librarians, Mrs. Penelope Hargrove and her grandson, Theo, soon realized that Mira was not just any cat. She could understand the stories she read, and more astonishingly, she could *write* them.\\n\\nOne rainy night, as thunder rattled the panes, a stray kitten named **Pip** slipped into the library, shivering and soaked. Pip was a tiny, mottled gray furball with oversized ears that twitched at every sound. He tried to hide behind a stack of encyclopedias, but Mira‚Äôs gentle nudge guided him toward a warm spot on a plush armchair. She lowered her head and brushed her whiskers against his cheek, as if saying, ‚ÄúYou‚Äôre safe here.‚Äù\\n\\nThe next morning, Mrs. Hargrove found a handwritten note tucked between the pages of *The Secret Garden*. It read:\\n\\n&gt; *‚ÄúThe garden is not just a place of flowers, but a sanctuary for those who listen. Come, little one, and hear the stories the wind tells.‚Äù*\\n\\nShe looked up to see Mira perched on the arm of the chair, her tail curled around Pip, who was now curled up, eyes half‚Äëclosed, listening to the soft rustle of pages. The kitten‚Äôs ears perked up whenever a new sentence was spoken aloud, as if the words themselves were a lullaby.\\n\\nFrom that day on, the Midnight Library became a haven for more than just books. Animals of all kinds‚Äîsquirrels with bright eyes, a shy hedgehog named Quill, even an old barn owl that occasionally swooped in through the open window‚Äîfound their way to the quiet sanctuary. Each creature was greeted by Mira with a soft purr and a gentle nudge toward a spot where they could curl up and listen.\\n\\nMira‚Äôs true talent, however, was not just in reading or comforting. She could *weave* stories from the thoughts and feelings that swirled in the hearts of those who entered. When a child cried over a lost toy, Mira would curl up beside them and, with a flick of her tail, conjure a tale of a brave mouse who embarked on a daring rescue mission. When an elderly man sighed with nostalgia, she would settle on his lap and spin a yarn about a distant sea voyage that seemed to echo his own memories.\\n\\nOne evening, as the town prepared for its annual Harvest Festival, a sudden storm rolled in, threatening to cancel the celebrations. The townsfolk gathered in the library, worried that the rain would wash away their plans. Mira leapt onto the central reading table, her paws landing softly on a stack of old maps. She stared at the ceiling, then at the anxious faces around her, and began to purr‚Äîa deep, resonant sound that seemed to vibrate through the very walls.\\n\\nAs the purring grew louder, the lights flickered, and a soft glow began to emanate from the books themselves. The pages fluttered, and words rose off the paper like fireflies, forming a luminous tapestry across the ceiling. The story that unfolded was one of a brave cat who, during a storm, guided a lost flock of birds back to safety by leading them through a hidden tunnel beneath the town. The tale ended with a promise: *‚ÄúWhen the rain falls, the heart of the library shines brighter than any lantern.‚Äù*\\n\\nThe next morning, the storm had passed, and the sky cleared to a brilliant sunrise. The townspeople emerged to find the streets glistening, but more importantly, they found a renewed sense of hope. The Harvest Festival went ahead, brighter than ever, with lanterns hanging from the library‚Äôs windows, each one reflecting the story Mira had told.\\n\\nFrom that night on, Mira was no longer just a cat who liked to read. She became the **Guardian of Stories**, a silent protector who ensured that every heart that entered the library left with a tale to carry forward. And whenever a new creature‚Äîbe it a trembling kitten, a weary traveler, or a curious squirrel‚Äîstepped through the doors, they would find a warm spot, a gentle purr, and perhaps, if they listened closely, a story waiting to be written.\\n\\nAnd so, the Midnight Library continued to hum with the soft purring of a silver‚Äëtabby cat, its shelves alive with whispered adventures, and its heart forever open to the magic that only stories‚Äîand a few well‚Äëplaced purrs‚Äîcan bring.\\n\\nJOKE:\\nHere's a purr-fectly simple one for you:  \\n\\n&gt; *Why did the cat get kicked out of the party?*  \\n&gt; *Because it kept knocking over the punch bowl... and then *paw*-tying on the floor!* üò∏  \\n\\n*(Bonus groan: It was a *cat*-astrophe!)*\\n\\nPOEM:\\n**Whiskers in the Moonlight**\\n\\nIn the hush of night‚Äôs soft sigh,  \\nA shadow slips on velvet paws‚Äî  \\nEyes like amber lanterns high,  \\nA silent hunter, caught in awe.\\n\\nShe curls around the world‚Äôs warm seam,  \\nA purr that rolls like rolling tide;  \\nEach ripple sings a secret dream,  \\nA lullaby where hearts can hide.\\n\\nShe stalks the sunbeams on the sill,  \\nA tiger in a tuxedoed coat;  \\nShe leaps, she lands, she never will‚Äî  \\nMiss a beat, she owns the float.\\n\\nHer tail, a question mark, unfurls,  \\nA comet tracing lazy arcs;  \\nShe paints the air with silent swirls,  \\nAnd leaves a trail of quiet sparks.\\n\\nWhen dawn awakes with amber glow,  \\nShe stretches, yawns, and claims the day;  \\nA regal queen of softest glow,  \\nShe rules the world in whiskered sway.\\n\\nSo here‚Äôs to cats‚Äîboth shy and bold‚Äî  \\nThe poets of the feline kind;  \\nIn every purr, a story told,  \\nA mystery we‚Äôll never fully find.\"}"
  },
  {
    "objectID": "L01/08_patterns.html#routing",
    "href": "L01/08_patterns.html#routing",
    "title": "Workflows and Agents",
    "section": "Routing",
    "text": "Routing\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\n\n\nfrom typing_extensions import Literal\nfrom pydantic import BaseModel\nfrom langchain.messages import HumanMessage, SystemMessage\n\n\n# Schema for structured output to use as routing logic\nclass Route(BaseModel):\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n        None, description=\"The next step in the routing process\"\n    )\n\n# Augment the LLM with schema for structured output\nrouter = llm.with_structured_output(Route)\n\ndef llm_call_router(input_: str):\n    \"\"\"Route the input to the appropriate node\"\"\"\n    # Run the augmented LLM with structured output to serve as routing logic\n    decision = router.invoke(\n        [\n            SystemMessage(\n                content=\"Route the input to story, joke, or poem based on the user's request.\"\n            ),\n            HumanMessage(content=input_),\n        ]\n    )\n    return decision.step\n\n\n@task\ndef llm_call_1(input_: str):\n    \"\"\"Write a story\"\"\"\n    result = llm.invoke(input_)\n    return result.content\n\n\n@task\ndef llm_call_2(input_: str):\n    \"\"\"Write a joke\"\"\"\n    result = llm.invoke(input_)\n    return result.content\n\n\n@task\ndef llm_call_3(input_: str):\n    \"\"\"Write a poem\"\"\"\n    result = llm.invoke(input_)\n    return result.content\n\n\n# Create workflow\n@entrypoint()\ndef router_workflow(input_: str):\n    next_step = llm_call_router(input_)\n    if next_step == \"story\":\n        llm_call = llm_call_1\n    elif next_step == \"joke\":\n        llm_call = llm_call_2\n    elif next_step == \"poem\":\n        llm_call = llm_call_3\n\n    return llm_call(input_).result()\n\n\n# Invoke\nfor step in router_workflow.stream(\"Tell me a joke about cats\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=Route(step='joke'), input_type=Route])\n  return self.__pydantic_serializer__.to_python(\n\n\n{'llm_call_2': \"Here's a classic cat joke that‚Äôs purr-fect for any cat lover:  \\n\\n&gt; **Why did the cat sit on the computer?**  \\n&gt; *Because it wanted to keep an eye on the mouse!* üòº  \\n\\n*(Bonus groan: Because it heard the mouse was *running* the system!)*  \\n\\nHope that gives you a little *purr* of laughter! üêæ\"}\n\n\n{'router_workflow': \"Here's a classic cat joke that‚Äôs purr-fect for any cat lover:  \\n\\n&gt; **Why did the cat sit on the computer?**  \\n&gt; *Because it wanted to keep an eye on the mouse!* üòº  \\n\\n*(Bonus groan: Because it heard the mouse was *running* the system!)*  \\n\\nHope that gives you a little *purr* of laughter! üêæ\"}"
  },
  {
    "objectID": "L01/08_patterns.html#orchestrator-worker",
    "href": "L01/08_patterns.html#orchestrator-worker",
    "title": "Workflows and Agents",
    "section": "Orchestrator-worker",
    "text": "Orchestrator-worker\nIn an orchestrator-worker configuration, the orchestrator:\n\nBreaks down tasks into subtasks\nDelegates subtasks to workers\nSynthesizes worker outputs into a final result\n\n\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\n\nfrom typing import List\n\n\n# Schema for structured output to use in planning\nclass Section(BaseModel):\n    name: str = Field(\n        description=\"Name for this section of the report.\",\n    )\n    description: str = Field(\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n    )\n\n\nclass Sections(BaseModel):\n    sections: List[Section] = Field(\n        description=\"Sections of the report.\",\n    )\n\n\n# Augment the LLM with schema for structured output\nplanner = llm.with_structured_output(Sections)\n\n\n@task\ndef orchestrator(topic: str):\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n    # Generate queries\n    report_sections = planner.invoke(\n        [\n            SystemMessage(content=\"Generate a plan for the report.\"),\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\n        ]\n    )\n\n    return report_sections.sections\n\n\n@task\ndef llm_call(section: Section):\n    \"\"\"Worker writes a section of the report\"\"\"\n\n    # Generate section\n    result = llm.invoke(\n        [\n            SystemMessage(content=\"Write a report section.\"),\n            HumanMessage(\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\n            ),\n        ]\n    )\n\n    # Write the updated section to completed sections\n    return result.content\n\n\n@task\ndef synthesizer(completed_sections: list[str]):\n    \"\"\"Synthesize full report from sections\"\"\"\n    final_report = \"\\n\\n---\\n\\n\".join(completed_sections)\n    return final_report\n\n\n@entrypoint()\ndef orchestrator_worker(topic: str):\n    sections = orchestrator(topic).result()\n    section_futures = [llm_call(section) for section in sections]\n    final_report = synthesizer(\n        [section_fut.result() for section_fut in section_futures]\n    ).result()\n    return final_report\n\n\n# Invoke\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=Sections(sections=[Sectio...ary of abbreviations')]), input_type=Sections])\n  return self.__pydantic_serializer__.to_python(\n\n\n\nfrom IPython.display import Markdown\n\n\nMarkdown(report)\n\nExecutive Summary\nPurpose\nThis report provides a comprehensive analysis of the current market landscape for renewable energy adoption in emerging economies, evaluates the performance of key policy initiatives, and assesses the financial viability of proposed investment strategies. Its primary objective is to equip policymakers, investors, and development agencies with actionable insights that can accelerate the transition to sustainable energy systems while fostering economic growth.\nKey Findings\n- Rapid Growth Potential: Emerging markets collectively possess an estimated 1.2‚ÄØTW of untapped renewable capacity, with solar and wind accounting for 68‚ÄØ% of the projected expansion.\n- Policy Impact: Countries that have implemented stable feed‚Äëin tariffs and streamlined permitting processes have seen a 35‚ÄØ% increase in renewable project completions within two years, compared with a 12‚ÄØ% rise in nations lacking such frameworks.\n- Economic Benefits: Transitioning to a 30‚ÄØ% renewable energy mix could generate up to 4.5‚ÄØmillion new jobs, reduce energy import bills by $18‚ÄØbillion annually, and lower CO‚ÇÇ emissions by 1.1‚ÄØGt‚ÄØCO‚ÇÇe per year.\n- Financial Viability: The levelized cost of electricity (LCOE) for utility‚Äëscale solar has fallen to $0.028‚ÄØ/kWh, making it competitive with fossil‚Äëfuel generation in 14 of the 20 studied economies.\n- Barriers to Scale: Limited grid infrastructure, fragmented financing mechanisms, and insufficient local technical expertise remain the most significant obstacles to scaling up renewable projects.\nRecommendations\n1. Establish Predictable Policy Frameworks: Governments should adopt long‚Äëterm renewable energy targets, stable feed‚Äëin tariffs, and transparent permitting processes to attract private capital.\n2. Mobilize Blended Finance: Leverage public‚Äësector guarantees and concessional loans to de‚Äërisk private investments, particularly in early‚Äëstage projects and emerging technologies such as storage and green hydrogen.\n3. Strengthen Grid Resilience: Prioritize investments in transmission upgrades and smart‚Äëgrid technologies to integrate variable renewable sources and ensure reliable supply.\n4. Build Local Capacity: Implement training programs and incentives for domestic firms to develop expertise in renewable installation, operation, and maintenance, thereby creating a self‚Äësustaining industry ecosystem.\n5. Promote Regional Cooperation: Facilitate cross‚Äëborder power trade and joint research initiatives to share best practices, reduce costs, and maximize resource utilization across neighboring economies.\nBy implementing these targeted actions, stakeholders can unlock the full economic and environmental potential of renewable energy in emerging markets, driving sustainable development and fostering inclusive prosperity.\n\n1. Introduction and Description: Context and Motivation for Studying LLM Scaling Laws; Objectives and Scope\n\n\n1.1. Background and Motivation\nThe performance of large language models (LLMs) exhibits a remarkably predictable dependence on three principal scaling factors: model size (parameter count), dataset size, and compute budget (often measured in FLOPs). Empirical studies‚Äîmost notably the ‚Äúscaling laws‚Äù first formalized by Kaplan et‚ÄØal. (2020) and subsequently refined by a growing body of work‚Äîhave demonstrated that, within certain regimes, the error of a model scales as a power‚Äëlaw function of these variables. This regularity has profound implications:\n\nPredictive Power: It enables researchers and practitioners to forecast the resources required to achieve a target level of performance, guiding efficient allocation of compute and data.\n\nDesign Guidance: Scaling laws inform architectural decisions (e.g., depth vs.¬†width, token‚Äëmix strategies) and help prioritize research directions such as sparsity, mixture‚Äëof‚Äëexperts, or curriculum learning.\n\nEconomic & Ethical Considerations: Understanding the cost‚Äëperformance trade‚Äëoffs is essential for responsible deployment, budgeting, and assessing the environmental footprint of ever‚Äëlarger models.\n\nDespite their utility, existing scaling‚Äëlaw analyses are often limited to specific model families, training regimes, or evaluation metrics. Moreover, the rapid emergence of new model architectures (e.g., transformer‚Äëbased diffusion language models, retrieval‚Äëaugmented generators) and training paradigms (e.g., multi‚Äëtask fine‚Äëtuning, reinforcement learning from human feedback) raises questions about the generality and robustness of traditional scaling relationships.\n\n\n1.2. Objectives\nThe primary objective of this report is to systematically investigate the scaling behavior of contemporary LLMs across a broad spectrum of model sizes, data regimes, and compute budgets. Specifically, we aim to:\n\nQuantify Scaling Relationships ‚Äì Derive empirical power‚Äëlaw exponents for loss, downstream task performance, and inference latency as functions of parameter count, training token count, and FLOPs, respectively.\n\nAssess Regime Boundaries ‚Äì Identify the transition points between the pre‚Äëtraining, scaling, and post‚Äëtraining regimes, and examine how factors such as token‚Äëtype distribution, optimizer choice, and regularization affect these boundaries.\n\nEvaluate Generalization Across Architectures ‚Äì Test whether the identified scaling laws hold for diverse model families (e.g., dense transformers, sparsely‚Äëgated mixture‚Äëof‚Äëexperts, retrieval‚Äëaugmented models) and for a variety of downstream tasks (language modeling, reasoning, code generation, multilingual benchmarks).\n\nProvide Practical Recommendations ‚Äì Translate the findings into actionable guidance for model selection, data collection, and compute budgeting under fixed performance targets.\n\n\n\n1.3. Scope\nThe scope of this report is deliberately bounded to ensure depth and reproducibility:\n\n\n\n\n\n\n\n\nDimension\nInclusion\nExclusion\n\n\n\n\nModel Families\nDense transformer decoders (GPT‚Äëstyle) up to ~1‚ÄØT parameters; sparsely‚Äëgated MoE variants with up to ~10‚ÄØB active parameters; retrieval‚Äëaugmented generators with external knowledge bases.\nNon‚Äëtransformer architectures (e.g., recurrent, convolutional) and models that rely on fundamentally different tokenization schemes (e.g., byte‚Äëpair encoding vs.¬†character‚Äëlevel).\n\n\nTraining Regimes\nPre‚Äëtraining on curated web‚Äëscale corpora (English‚Äëcentric and multilingual); multi‚Äëtask fine‚Äëtuning; RLHF fine‚Äëtuning for alignment.\nTraining on proprietary, non‚Äëpublic datasets that are unavailable for audit; on‚Äëdevice continual learning beyond the pre‚Äëtraining phase.\n\n\nCompute & Data Metrics\nParameter count, total FLOPs, token count, and effective compute (measured in PF‚Äëdays).\nEnergy consumption beyond FLOP accounting, hardware‚Äëspecific latency measurements (unless explicitly tied to FLOP equivalence).\n\n\nEvaluation Metrics\nPer‚Äëtoken cross‚Äëentropy loss, perplexity, and a curated suite of downstream benchmarks (e.g., MMLU, GSM‚Äë8K, BIG‚ÄëBench, XGLUE).\nProprietary enterprise metrics that require confidential data or are not publicly benchmarked.\n\n\nTemporal Horizon\nModels released up to June‚ÄØ2024 (including publicly disclosed checkpoints).\nFuture models or those released after this date, unless they are open‚Äësource and meet the inclusion criteria.\n\n\n\nAll experiments reported herein will be reproducible using publicly available checkpoints and standard training scripts (e.g., Hugging Face Transformers, DeepSpeed, FairScale). Where proprietary data is used for illustrative purposes, we will provide synthetic proxies that preserve the statistical properties of the original corpora.\n\n\n1.4. Structure of the Report\nThe remainder of the report is organized as follows:\n\nRelated Work ‚Äì A review of seminal scaling‚Äëlaw studies, recent extensions, and gaps in the literature.\n\nExperimental Methodology ‚Äì Details on model configurations, data pipelines, training schedules, and evaluation protocols.\n\nEmpirical Findings ‚Äì Presentation and analysis of scaling exponents, regime transitions, and cross‚Äëarchitecture comparisons.\n\nDiscussion ‚Äì Interpretation of results, implications for model design and deployment, and limitations of the current study.\n\nConclusions and Recommendations ‚Äì Summary of key insights and actionable guidance for researchers and practitioners.\n\nBy systematically characterizing how performance scales with model size, data, and compute, this report seeks to provide a comprehensive, empirically grounded roadmap for leveraging scaling laws as a predictive tool in the development of next‚Äëgeneration LLMs.\n\n2. Background and Description\n\n\n\n2.1. Evolution of Large Language Models\nLarge language models (LLMs) are a class of neural‚Äënetwork‚Äëbased systems that have dramatically reshaped natural‚Äëlanguage processing (NLP) and, more broadly, artificial intelligence (AI) over the past decade. Their evolution can be traced through three interrelated milestones:\n\n\n\n\n\n\n\n\n\nMilestone\nYear\nModel / Architecture\nKey Advances\n\n\n\n\nEarly Distributed Representations\n2013‚Äë2015\nWord2Vec, GloVe, FastText\nIntroduced dense, context‚Äëaware embeddings that made vector‚Äëspace semantics tractable for downstream tasks.\n\n\nTransformer Paradigm\n2017\nAttention Is All You Need (Vaswani et‚ÄØal.)\nReplaced recurrent and convolutional layers with self‚Äëattention, enabling parallel computation and scalable context handling.\n\n\nPre‚Äëtraining at Scale\n2018‚Äë2020\nOpenAI GPT‚Äë1/2, Google BERT, Microsoft Turing‚ÄëNLG\nDemonstrated that massive unsupervised pre‚Äëtraining on heterogeneous text corpora yields emergent linguistic abilities that transfer to a wide range of downstream tasks.\n\n\nMassive Parameter Regimes\n2020‚Äë2023\nGPT‚Äë3 (175‚ÄØB), Megatron‚ÄëTuring‚ÄëNLG (530‚ÄØB), PaLM‚Äë2 (up to 540‚ÄØB)\nShowed that increasing model size‚Äîboth in parameters and training compute‚Äîproduces systematic gains in few‚Äëshot learning, reasoning, and multilingual competence.\n\n\nMultimodal & Structured Integration\n2023‚Äëpresent\nGPT‚Äë4‚ÄëV, LLaMA‚Äë2‚ÄëChat, Gemini, Claude‚Äë3\nExtends LLMs beyond pure text to incorporate images, code, tables, and structured knowledge, while refining alignment and safety mechanisms.\n\n\n\nThe trajectory is characterized not merely by a quantitative increase in parameter count, but by a qualitative shift in capability: from models that excel at narrow, supervised tasks to systems that exhibit emergent properties such as chain‚Äëof‚Äëthought reasoning, code synthesis, and cross‚Äëmodal understanding. This shift has been enabled by three synergistic developments:\n\nData‚Äëcentric scaling ‚Äì curated, high‚Äëquality corpora (e.g., The Pile, Common Crawl, filtered Wikipedia) that provide richer linguistic diversity.\n\nCompute‚Äëefficient training ‚Äì techniques such as mixed‚Äëprecision arithmetic, gradient checkpointing, and optimizer variants (e.g., AdamW) that make training billions of parameters feasible on commodity hardware clusters.\n\nArchitectural refinements ‚Äì layer‚Äënorm variants, rotary positional embeddings, and sparsity‚Äëaware attention mechanisms that improve stability and reduce memory footprints.\n\nCollectively, these advances have positioned LLMs as the foundational substrate for a new generation of AI‚Äëdriven applications, ranging from conversational agents and content generation to scientific discovery and automated reasoning.\n\n\n\n2.2. Definition of Scaling Laws\nScaling laws are empirical relationships that describe how the performance of a neural‚Äënetwork model‚Äîtypically measured by a downstream benchmark metric‚Äîimproves as a function of three controllable resources:\n\nModel size ‚Äì usually expressed in terms of the number of parameters, (N).\n\nTraining compute ‚Äì the total amount of floating‚Äëpoint operations (FLOPs) expended during training, (C).\n\nDataset size ‚Äì the number of training tokens or examples, (D).\n\nIn their simplest form, scaling laws can be written as:\n[ (N, C, D) A , N^{-} , C^{-} , D^{-}, ]\nwhere () denotes the loss (or error) on a held‚Äëout validation set, and (A, , , ) are positive constants estimated from experimental data. More commonly, researchers express error (e.g., perplexity) as a power‚Äëlaw function of the effective compute per parameter:\n[ ()^{-}, ]\nwith () representing the scaling exponent that captures the diminishing returns of adding more compute.\nKey properties of these laws include:\n\nPower‚Äëlaw behavior: Performance improves smoothly and predictably as a function of scale, rather than exhibiting abrupt phase transitions.\n\nOptimal allocation: Given a fixed budget (B = C N), the error is minimized when compute and model size are balanced according to the exponents (, ).\n\nGeneralization to new tasks: Scaling laws observed on language‚Äëmodel pre‚Äëtraining loss often transfer to downstream few‚Äëshot performance, suggesting that the same underlying resource‚Äìerror relationship governs both pre‚Äëtraining and fine‚Äëtuning regimes.\n\nThese empirical regularities have become a guiding principle for research planning, allowing practitioners to forecast the trade‚Äëoffs between model size, data collection, and compute allocation before committing to expensive training runs.\n\n\n\n2.3. Historical Perspective: Power‚ÄëLaw Relationships in AI\nThe notion that complex systems exhibit power‚Äëlaw scaling predates modern deep learning and has recurrently surfaced across AI subfields:\n\n\n\n\n\n\n\n\n\nEra\nDomain\nPower‚ÄëLaw Manifestation\nInsight Gained\n\n\n\n\n1970s‚Äì1980s\nStatistical Physics\nDistribution of energy states in Ising models\nIntroduced the concept of scale‚Äëfree behavior, later adapted to characterize parameter distributions in neural networks.\n\n\n1990s\nConnectionist Learning\nScaling of required training examples with network depth\nEarly work on capacity showed that the number of trainable parameters must grow polynomially with task complexity.\n\n\n2000s\nSpeech Recognition\nRelationship between acoustic model size and word error rate\nDemonstrated that larger acoustic models reduced error roughly as a power of model size, foreshadowing later LLM scaling.\n\n\n2010s\nImage Classification\nAccuracy vs.¬†number of layers / filters\nEmpirical studies (e.g., Krizhevsky et‚ÄØal., 2012) revealed diminishing error improvements with additional layers, prompting the adoption of residual connections and deeper architectures.\n\n\n2020s\nLarge Language Models\nLoss vs.¬†parameters, tokens, and FLOPs\nSystematic studies (e.g., Kaplan et‚ÄØal., 2020; Hoffmann et‚ÄØal., 2022) quantified scaling exponents, establishing that model performance follows a predictable power‚Äëlaw with respect to each resource dimension.\n\n\n\nThe historical thread linking these observations is the recurring pattern that error or error‚Äërelevant metrics decrease as a power of the underlying resource. In early AI, this manifested as a need for exponentially more training data to achieve linear gains in accuracy. With the advent of deep, over‚Äëparameterized networks, the relationship softened to a polynomial (often square‚Äëroot) scaling, enabling more efficient utilization of compute.\nThe modern scaling law literature formalizes this intuition:\n\nKaplan et‚ÄØal.¬†(2020) introduced a simple power‚Äëlaw model linking loss to model size, dataset size, and compute, showing that optimal performance is achieved when (N C^{1/2}) and (D N).\n\nHoffmann et‚ÄØal.¬†(2022) extended the analysis to the Chinchilla regime, proving that beyond a certain point, allocating more compute to data yields greater returns than enlarging the model.\n\nChinchilla & PaLM‚Äë2 studies empirically validated that training a 70‚ÄØB‚Äëparameter model on 1.4‚ÄØ√ó‚ÄØthe data used for a 175‚ÄØB model yields comparable downstream performance, underscoring the practical relevance of scaling‚Äëlaw‚Äëguided resource allocation.\n\nThese historical insights collectively illustrate a unifying principle: the performance of AI systems obeys power‚Äëlaw scaling with respect to the fundamental resources of model capacity, data, and compute. Recognizing and leveraging this principle has become a cornerstone of contemporary AI research, informing everything from architecture design to budgeting of large‚Äëscale training campaigns.\n\nThe above subsections synthesize the current scholarly understanding of how large language models have evolved, how scaling laws formalize the relationship between resources and performance, and how power‚Äëlaw scaling has recurred throughout the broader history of artificial intelligence.\n\n3. Theoretical Foundations and Description\nThe performance of complex engineered and natural systems is frequently observed to obey scaling relationships that can be captured succinctly by power‚Äëlaw functions. In this section we lay out the mathematical scaffolding that underpins our analysis, beginning with the formulation of power‚Äëlaw models for performance versus resource metrics, followed by a systematic derivation of the associated scaling exponents, and finally by situating these results within the broader frameworks of statistical mechanics and information theory.\n\n\n\n3.1. Power‚Äëlaw Modeling of Performance vs.¬†Resource Metrics\nLet (P) denote a performance indicator (e.g., throughput, error rate, energy consumption) and let (R) represent a measurable resource input (e.g., number of processing nodes, bandwidth, material stock). Empirical observations across a wide class of systems reveal that, over a broad intermediate regime, the relationship can be approximated by\n[ P(R) ;; C,R^{},, ]\nwhere\n\n(C&gt;0) is a system‚Äëspecific prefactor that encapsulates baseline efficiency, design constants, or normalization factors, and\n\n() is the scaling exponent that quantifies how sensitively performance responds to changes in the resource pool.\n\nEquation (3.1) is deliberately generic; specific instantiations may involve logarithmic corrections, cut‚Äëoffs, or multi‚Äëscale regimes, but the power‚Äëlaw form remains the leading-order approximation in the asymptotic limit of large (R). The logarithm of both sides yields a linear relationship amenable to regression:\n[ P = C + R . ]\nThus, a log‚Äìlog plot of (P) versus (R) should exhibit a straight line with slope () in the scaling window, providing a straightforward diagnostic for power‚Äëlaw behavior.\n\n\n\n3.2. Derivation of Scaling Exponents\nTo extract () analytically, we consider a representative stochastic growth process that is known to generate power‚Äëlaw asymptotics. Suppose the incremental improvement (P) obtained by adding a marginal amount (R) of resource follows a scale‚Äëinvariant rule\n[ P ;; (R)^{},, ]\nwith () a characteristic exponent of the underlying dynamics. In a continuous limit, the differential form\n[ ;; R^{} ]\nintegrates to\n[ P(R) ;; ^{R} R‚Äô^{},dR‚Äô ;; R^{},, ]\nprovided the integration starts from a non‚Äëzero lower bound and the upper bound lies within the asymptotic regime. Consequently, the scaling exponent governing the performance‚Äìresource relationship is simply\n[ . ]\nIn many models‚Äîsuch as preferential attachment, self‚Äëorganized criticality, or queueing networks with heavy‚Äëtailed service times‚Äî() can be derived from first principles. For instance, in a preferential‚Äëattachment process where the probability of acquiring additional resources is proportional to the current performance, one obtains (= ), leading to (= ). In queueing systems with Poisson arrivals and exponential service times, the exponent often emerges as (= 1 - ) where (k) is the shape parameter of the service‚Äëtime distribution. These derivations illustrate how the exponent is not an empirical fitting parameter per se, but rather a fingerprint of the underlying microscopic dynamics.\n\n\n\n3.3. Connection to Statistical Mechanics and Information Theory\nThe power‚Äëlaw form (3.1) resonates deeply with concepts from statistical mechanics and information theory, where scale invariance and entropy maximization give rise to analogous scaling laws.\n\nStatistical Mechanics Perspective ‚Äì Near critical points, macroscopic observables often exhibit power‚Äëlaw dependencies on control parameters (e.g., magnetization vs.¬†temperature). The renormalization‚Äëgroup (RG) framework explains that such dependencies are universal, arising from the fixed‚Äëpoint structure of the RG flow. By mapping the resource variable (R) onto a temperature‚Äëlike control parameter and the performance variable (P) onto an order parameter, the exponent () can be identified with a critical exponent associated with a relevant RG eigenvalue. This viewpoint justifies the robustness of power‚Äëlaw scaling across disparate domains: the same universality class yields the same () irrespective of microscopic details.\nInformation‚ÄëTheoretic Perspective ‚Äì From the standpoint of Shannon entropy, the distribution of resource allocations that maximizes entropy under constraints of fixed mean and variance is a power‚Äëlaw (Pareto) distribution. When performance is interpreted as a function of the entropy of the underlying stochastic process, the scaling exponent () can be linked to the exponent governing the tail of this entropy distribution. Moreover, the Kolmogorov‚ÄìSinai entropy of a dynamical system quantifies the rate of information production; in systems where information production scales sub‚Äëlinearly with resource consumption, the exponent () emerges as the ratio of information‚Äëproduction rate to resource‚Äëconsumption rate. Thus, () can be interpreted as a measure of efficiency of information processing in the system.\n\nThese connections provide a unifying lens: the power‚Äëlaw exponent is not merely a phenomenological fit but a manifestation of deep structural properties‚Äîscale invariance, critical fluctuations, and optimal information encoding‚Äîthat are common to many complex systems.\n\nSummary ‚Äì Section‚ÄØ3.1 introduced the generic power‚Äëlaw ansatz (P(R)=C R^{}) and highlighted its diagnostic utility via log‚Äìlog linearization. Section‚ÄØ3.2 demonstrated how () can be derived from scale‚Äëinvariant growth dynamics, establishing a direct link to microscopic exponents (). Finally, Section‚ÄØ3.3 situated these results within the theoretical constructs of statistical mechanics (critical phenomena, renormalization‚Äëgroup universality) and information theory (entropy maximization, information‚Äëproduction rates), underscoring the profound conceptual underpinnings of the observed scaling behavior.\nThese foundations set the stage for the empirical analysis presented in the subsequent sections, where we validate the power‚Äëlaw predictions against experimental data and explore the implications of the derived exponents for system design and optimization.\n\n4. Empirical Evidence and Description\nThe empirical foundation of this study rests on a systematic exploration of how three core axes of model design‚Äîtraining compute, model size, and data characteristics‚Äîinteract with downstream performance across a spectrum of benchmark tasks. The evidence presented below draws on a curated set of experiments that span from controlled ablations to large‚Äëscale case studies of contemporary foundation models. Each subsection details the methodology, key observations, and their implications for scaling laws and practical deployment.\n\n\n\n4.1. Training Compute vs.¬†Validation Loss Curves\nObjective. To quantify the relationship between the total amount of compute expended during pre‚Äëtraining (measured in FLOPs) and the achievable validation loss on a held‚Äëout dataset.\nMethodology.\n- A series of transformer‚Äëbased models were trained from scratch on the same base corpus (e.g., a 300‚ÄØB‚Äëtoken English text collection).\n- Compute budgets were selected to span three orders of magnitude: 10‚Åπ, 10¬π‚Å∞, 10¬π¬π, 10¬π¬≤, and 10¬π¬≥ FLOPs.\n- For each budget, training was run until either a fixed number of epochs or a target loss plateau was reached; early‚Äëstopping was applied based on a moving‚Äëaverage of validation loss.\n- Validation loss was recorded at regular intervals (every 0.1‚ÄØ% of total compute) to generate smooth loss curves.\nKey Findings.\n| Compute (FLOPs) | Validation Loss (perplexity) | Observed Trend | |‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî‚Äî-| | 10‚Åπ | 150‚ÄØ√ó | High variance, unstable training | | 10¬π‚Å∞ | 45‚ÄØ√ó | Rapid initial improvement, diminishing returns after ~5‚ÄØB tokens | | 10¬π¬π | 22‚ÄØ√ó | Near‚Äëlinear reduction in loss up to ~10‚ÄØB tokens | | 10¬π¬≤ | 12‚ÄØ√ó | Plateau begins; additional compute yields &lt;0.5‚ÄØ√ó loss reduction | | 10¬π¬≥ | 11‚ÄØ√ó | Marginal gain; marginal cost increase &gt;10√ó |\n\nPower‚Äëlaw behavior: The log‚Äëlog plot of validation loss versus compute follows a slope of approximately ‚Äì0.07, consistent with prior scaling‚Äëlaw analyses (e.g., Kaplan et al., 2020).\n\nDiminishing returns: Beyond ~10¬π¬≤ FLOPs, each additional 10√ó compute translates to less than a 0.2√ó reduction in loss, indicating a saturation point for the given data distribution.\n\nStability considerations: Higher compute regimes exhibited lower gradient variance, enabling larger batch sizes and more stable optimizer schedules, which further contributed to smoother loss curves.\n\nImplications. The compute‚Äëloss relationship suggests that, for a fixed dataset, there exists an ‚Äúoptimal‚Äù compute budget where marginal gains are outweighed by diminishing returns. Practitioners can therefore allocate resources more efficiently by targeting compute levels that bring loss below a task‚Äëspecific threshold rather than pursuing maximal compute indiscriminately.\n\n\n\n4.2. Model Size vs.¬†Downstream Benchmark Performance\nObjective. To assess how scaling model parameters influences performance on a suite of downstream benchmarks (e.g., GLUE, SuperGLUE, BIG‚ÄëBench, and domain‚Äëspecific QA/translation tasks).\nMethodology.\n- Five model families were constructed with parameter counts ranging from 125‚ÄØM to 175‚ÄØB, keeping architecture (depth, width, attention heads) proportional.\n- All models were trained for an identical number of tokens (‚âà300‚ÄØB) using the same optimizer and learning‚Äërate schedule.\n- After pre‚Äëtraining, each model was fine‚Äëtuned on each benchmark for a fixed budget (e.g., 10‚ÄØk steps) and evaluated using the standard metric for that task.\nObserved Patterns.\n1. Monotonic improvement: Across almost all benchmarks, performance increased monotonically with model size, with a median relative gain of ~12‚ÄØ% when moving from 1‚ÄØB to 10‚ÄØB parameters.\n2. Task‚Äëspecific scaling exponents: Certain tasks displayed steeper scaling curves (e.g., multi‚Äëhop reasoning tasks exhibited exponent ‚âà0.35, whereas lexical classification tasks showed ‚âà0.15).\n3. Saturation thresholds: For a subset of benchmarks (e.g., natural language inference), performance plateaued around 70‚ÄØB parameters, suggesting that additional capacity yields negligible gains beyond this point.\n4. Cross‚Äëtask transfer: Larger models demonstrated superior zero‚Äëshot transfer to out‚Äëof‚Äëdistribution tasks, often outperforming smaller fine‚Äëtuned baselines by &gt;20‚ÄØ% absolute accuracy.\nStatistical Analysis.\n- A mixed‚Äëeffects regression model was fitted with size (log‚Äëparameter count) as a fixed effect and task as a random effect. The estimated coefficient for size was 0.28 (SE‚ÄØ=‚ÄØ0.02), confirming a statistically significant positive relationship (p‚ÄØ&lt;‚ÄØ0.001).\n- The marginal R¬≤ of the model was 0.42, indicating that size explains a substantial but not exhaustive portion of performance variance; task difficulty and data quality also contributed significantly.\nPractical Takeaway. Deploying a model whose parameter count aligns with the most demanding downstream task yields the greatest overall utility. However, for resource‚Äëconstrained settings, a ‚Äúsweet‚Äëspot‚Äù model (‚âà10‚Äì30‚ÄØB parameters) often balances performance gains with inference cost, especially when the target tasks are not heavily reasoning‚Äëintensive.\n\n\n\n4.3. Dataset Size and Data Quality Effects\nObjective. To disentangle the impact of raw dataset volume from the intrinsic quality of the data on downstream performance.\nExperimental Design.\n- Starting from a base corpus of 300‚ÄØB tokens, we constructed three variants:\n1. Low‚Äëquality, high‚Äëvolume ‚Äì duplicated and noisy web crawl (‚âà1.2‚ÄØT tokens, 30‚ÄØ% duplicate, 15‚ÄØ% profanity).\n2. Medium‚Äëquality, moderate‚Äëvolume ‚Äì filtered to remove exact duplicates and low‚Äëquality HTML (‚âà600‚ÄØB tokens).\n3. High‚Äëquality, low‚Äëvolume ‚Äì curated, human‚Äëannotated text (‚âà150‚ÄØB tokens, &gt;95‚ÄØ% clean).\n- Each variant was used to pre‚Äëtrain a 1.3‚ÄØB‚Äëparameter model for the same compute budget (‚âà10¬π¬π FLOPs).\n- Downstream evaluation was performed on a standardized benchmark suite (e.g., ARC, PIQA, and a domain‚Äëspecific medical QA set).\nFindings.\n| Dataset Variant | Validation Perplexity | Avg. Benchmark Accuracy | |‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì| | Low‚Äëquality, high‚Äëvolume | 18.4 | 68‚ÄØ% | | Medium‚Äëquality, moderate‚Äëvolume | 13.2 | 74‚ÄØ% | | High‚Äëquality, low‚Äëvolume | 11.7 | 78‚ÄØ% |\n\nQuality dominates quantity: Even when the high‚Äëquality set was four times smaller, the resulting model outperformed the low‚Äëquality counterpart by 10‚ÄØ% absolute accuracy.\n\nNoise mitigation: Models trained on noisy data exhibited higher variance in fine‚Äëtuning, leading to poorer calibration and higher error rates on out‚Äëof‚Äëdistribution prompts.\n\nCurriculum effects: When a progressive cleaning pipeline was applied (starting from noisy data and gradually adding higher‚Äëquality subsets), performance improved smoothly, suggesting that controlled exposure to increasing quality can yield synergistic benefits.\n\nInterpretation. These results reinforce the notion that data hygiene is a critical lever for scaling efficiency. Investing in filtering, deduplication, and domain‚Äëspecific curation can reduce the compute needed to achieve a target performance level, especially for tasks that demand precise linguistic understanding.\n\n\n\n4.4. Case Studies\n\n4.4.1. GPT‚Äë2 ‚Üí GPT‚Äë3\n\nScale jump: Parameter count increased from 1.5‚ÄØB (GPT‚Äë2) to 175‚ÄØB (GPT‚Äë3), accompanied by a 3,125√ó increase in training tokens (from 3‚ÄØB to 570‚ÄØB).\n\nEmpirical outcome: GPT‚Äë3 achieved state‚Äëof‚Äëthe‚Äëart zero‚Äëshot performance on 45‚ÄØ% of BIG‚ÄëBench tasks, a 20‚ÄØ% absolute gain over the best fine‚Äëtuned GPT‚Äë2 variants.\n\nKey insight: The scaling law exponent for loss versus compute remained stable (‚âà‚Äì0.07), but the effective downstream benefit per additional parameter rose sharply due to the richer data mixture and longer training horizon.\n\n\n\n4.4.2. PaLM (540‚ÄØB)\n\nTraining regime: 780‚ÄØB tokens, 1.5‚ÄØ√ó‚ÄØ10¬≤‚Å¥ FLOPs, using a mixture of web text, books, and code.\n\nPerformance: Demonstrated emergent capabilities (e.g., multi‚Äëstep arithmetic, few‚Äëshot reasoning) that were absent in smaller siblings. Benchmarks such as TriviaQA and Natural Questions saw relative improvements of 15‚Äì30‚ÄØ% over the 100‚ÄØB‚Äëparameter baseline.\n\nObservation: The model exhibited a double‚Äëdescent curve in terms of compute vs.¬†validation loss, where a temporary increase in loss was observed when moving from 100‚ÄØB to 300‚ÄØB parameters before the final descent at 540‚ÄØB.\n\n\n\n4.4.3. LLaMA (7‚ÄØB, 13‚ÄØB, 33‚ÄØB, 65‚ÄØB)\n\nUniform architecture: All sizes shared the same token embedding dimension scaling rule, facilitating direct size comparisons.\n\nDownstream results: On the MMLU benchmark, accuracy scaled roughly as 0.5‚ÄØ% per 10‚ÄØB parameter increase, with the 65‚ÄØB variant reaching 57‚ÄØ% average accuracy.\n\nData efficiency: When trained on a 1‚ÄëT‚Äëtoken filtered corpus, the 13‚ÄØB model matched the 33‚ÄØB model‚Äôs performance on several tasks, underscoring the importance of high‚Äëquality data.\n\n\n\n4.4.4. GPT‚Äë4 (estimated &gt;1‚ÄØT parameters)\n\nLimited public details: While exact compute figures are undisclosed, external analyses suggest &gt;10‚Å¥‚ÄØPF‚Äëdays of training and a token budget exceeding 13‚ÄØT.\n\nEmpirical evidence: GPT‚Äë4 achieved near‚Äëhuman performance on a broad set of professional exams (e.g., bar, medical licensing) and demonstrated unprecedented few‚Äëshot reasoning on novel tasks.\n\nScaling implications: The observed loss curve plateaued at a perplexity of ~9, indicating that further compute yields diminishing returns unless accompanied by richer data modalities (e.g., multimodal embeddings).\n\nSynthesis. Across these case studies, a consistent pattern emerges: scale amplifies capability, but the magnitude of improvement is mediated by three intertwined factors‚Äîtraining compute, model architecture, and data curation. The most pronounced gains arise when larger compute budgets are coupled with high‚Äëquality, diverse data, enabling emergent behaviors that cannot be predicted from smaller‚Äëscale experiments.\n\n\n\n\n4.5. Summary\n\nCompute‚Äëloss curves reveal a power‚Äëlaw relationship with diminishing returns beyond ~10¬π¬≤ FLOPs for a fixed dataset.\n\nModel size scaling yields monotonic improvements on most benchmarks, yet the rate of gain is task‚Äëdependent and often plateaus around 70‚Äì100‚ÄØB parameters for certain tasks.\n\nData quality can outweigh raw volume; curated, low‚Äënoise corpora produce markedly better downstream performance even when smaller in size.\n\nCase studies from GPT‚Äë2 ‚Üí GPT‚Äë3, PaLM, LLaMA, and GPT‚Äë4 illustrate how coordinated scaling of compute, parameters, and data leads to both incremental and emergent capabilities.\n\nThese empirical observations provide a quantitative backbone for the design of future foundation models, guiding resource allocation toward regimes where marginal gains are maximized while mitigating the costs associated with over‚Äëparameterization or data noise.\n\n5. Practical Implications and Description\nThis section translates the technical findings of the study into concrete actions that practitioners, decision‚Äëmakers, and budgeting teams can apply when selecting, deploying, and operating machine‚Äëlearning systems.\n\n\n\n5.1. Cost‚ÄëEfficiency Trade‚Äëoffs\n\n\n\n\n\n\n\n\n\nDimension\nTypical Trade‚Äëoff\nPractical Consequence\nMitigation Strategies\n\n\n\n\nModel Accuracy vs.¬†Compute Cost\nHigher‚Äëcapacity architectures (e.g., deep transformers, large ensembles) often yield marginal gains in predictive performance but require exponentially more GPU/TPU cycles, memory, and energy.\nDiminishing returns on accuracy can quickly outpace budget constraints, especially for inference‚Äëheavy workloads.\n‚Ä¢ Use progressive model scaling ‚Äì start with a baseline model and only upgrade when the marginal gain exceeds a predefined cost‚Äëbenefit threshold.‚Ä¢ Apply knowledge distillation to compress large models into smaller, cheaper variants.\n\n\nTraining Time vs.¬†Data Utilization\nLonger training epochs improve convergence but increase electricity, cloud‚Äëinstance hours, and labor costs.\nExtended timelines delay product releases and inflate operational expenses.\n‚Ä¢ Adopt early‚Äëstopping and learning‚Äërate schedules that stop training once validation improvement falls below a cost‚Äësensitivity parameter.‚Ä¢ Leverage mixed‚Äëprecision training and gradient checkpointing to cut compute without sacrificing final accuracy.\n\n\nModel Size vs.¬†Deployment Footprint\nLarger models improve performance on complex tasks but increase latency, storage, and memory requirements on edge devices.\nMay necessitate expensive hardware upgrades or limit deployment to data‚Äëcenter environments only.\n‚Ä¢ Prioritize parameter‚Äëefficient architectures (e.g., MobileNet‚ÄëV3, TinyBERT).‚Ä¢ Use quantization (int8/float16) and pruning to shrink model size while preserving accuracy.\n\n\nEnergy Consumption vs.¬†Sustainability Goals\nHigh‚Äëperformance training consumes significant electricity, affecting carbon footprints and potentially incurring carbon‚Äëtax penalties.\nDirect cost impact and reputational risk for environmentally‚Äëconscious organizations.\n‚Ä¢ Schedule training during off‚Äëpeak renewable‚Äëenergy windows.‚Ä¢ Employ carbon‚Äëaware scheduling tools that select low‚Äëcarbon cloud regions.\n\n\n\nKey Takeaway:\nCost‚Äëefficiency is not a single metric but a multi‚Äëdimensional balance. Decision‚Äëmakers should quantify the marginal utility of each additional unit of accuracy, latency, or energy consumption and compare it against the associated financial and ecological costs. A disciplined, data‚Äëdriven cost‚Äëbenefit analysis prevents over‚Äëengineering and ensures that resources are allocated where they deliver the greatest net value.\n\n\n\n5.2. Implications for Model Selection and Deployment\n\nPerformance‚ÄëFirst vs.¬†Cost‚ÄëFirst Paradigms\n\nPerformance‚Äëfirst approaches (e.g., selecting the highest‚Äëaccuracy model regardless of cost) are appropriate when the model is a core differentiator (e.g., proprietary recommendation engine).\n\nCost‚Äëfirst approaches dominate in commodity use‚Äëcases (e.g., fraud detection at scale) where marginal gains are negligible but operational expenses dominate.\n\nModel‚Äëas‚Äëa‚ÄëService (MaaS) Considerations\n\nDeploying models via APIs introduces inference‚Äëcost scaling: each request incurs compute, network, and storage charges.\n\nSelecting a model with a favorable accuracy‚Äëper‚Äëinference‚Äëcost ratio can dramatically improve ROI.\n\nUse dynamic scaling (e.g., serverless functions) and request batching to amortize fixed costs across many queries.\n\nVersioning, Monitoring, and Retraining Pipelines\n\nDeployed models require continuous monitoring for drift, which can trigger costly retraining cycles.\n\nImplement automated drift detection with thresholds tuned to the organization‚Äôs budget tolerance; only retrain when the expected loss in performance exceeds the projected cost of a new training run.\n\nHardware‚ÄëSpecific Optimizations\n\nCertain models (e.g., transformer‚Äëbased language models) are highly optimized on specific accelerators (e.g., NVIDIA GPUs, Google TPUs).\n\nAlign model architecture with the hardware portfolio of the deployment environment to minimize conversion overhead and maximize throughput.\n\nRegulatory and Compliance Constraints\n\nIn regulated domains (e.g., healthcare, finance), model interpretability and auditability may impose additional computational overhead (e.g., post‚Äëhoc explanation layers).\n\nFactor these compliance‚Äërelated costs into the selection matrix early to avoid surprise budget overruns later.\n\n\n\n\n\n5.3. Guidance for Resource Allocation and Budgeting\n\n\n\n\n\n\n\n\nBudgetary Element\nRecommended Allocation Principle\nPractical Implementation\n\n\n\n\nCompute Infrastructure\nAllocate 70‚ÄØ% of compute spend to steady‚Äëstate inference and 30‚ÄØ% to training/experimentation.\n‚Ä¢ Use spot instances or pre‚Äëemptible VMs for training workloads.‚Ä¢ Reserve dedicated instances for latency‚Äëcritical inference services.\n\n\nPersonnel\nReserve 40‚ÄØ% of data‚Äëscience/ML engineering capacity for model optimization (distillation, quantization) and 40‚ÄØ% for pipeline reliability (monitoring, CI/CD). The remaining 20‚ÄØ% supports research & innovation.\n‚Ä¢ Adopt DevOps‚Äëstyle MLops practices: automated testing, version control, and rollback mechanisms.\n\n\nCloud Services\nApply a cost‚Äëcenter tagging strategy; tag all resources by project, environment, and model version to enable granular spend analysis.\n‚Ä¢ Leverage reserved instances for predictable workloads.‚Ä¢ Use budget alerts that trigger when projected monthly spend exceeds a predefined threshold.\n\n\nEnergy & Sustainability\nInclude a carbon‚Äëcost factor (e.g., $/kg‚ÄØCO‚ÇÇ) in the cost model for high‚Äëenergy training jobs.\n‚Ä¢ Schedule heavy training jobs during periods of low grid carbon intensity.‚Ä¢ Purchase green‚Äëenergy credits where feasible to offset unavoidable emissions.\n\n\nContingency Reserve\nMaintain a 10‚Äë15‚ÄØ% contingency fund for unexpected retraining, emergency scaling, or security patches.\n‚Ä¢ Review and adjust the reserve quarterly based on historical variance in training job durations and inference traffic spikes.\n\n\n\nStrategic Checklist for Budget Planning\n\nDefine Success Metrics ‚Äì Establish clear, quantifiable targets (e.g., ‚Äúmaintain inference latency ‚â§‚ÄØ50‚ÄØms at ‚â§‚ÄØ$0.02 per 1‚ÄØk requests‚Äù).\n\nModel‚ÄëCost Matrix ‚Äì Build a spreadsheet that maps each candidate model to:\n\nExpected accuracy / performance.\n\nTraining compute (GPU‚Äëhours, memory).\n\nInference compute (CPU/GPU cycles, memory).\n\nStorage and network egress costs.\n\nEstimated annual operating expense.\n\n\nRun Sensitivity Analyses ‚Äì Vary key parameters (e.g., batch size, quantization level) to see how cost curves respond.\n\nPrioritize ‚ÄúLow‚ÄëHanging Fruit‚Äù ‚Äì Implement quick wins such as model pruning or switching to a cheaper inference backend before committing to large‚Äëscale infrastructure upgrades.\n\nDocument Assumptions ‚Äì Record all cost assumptions (e.g., cloud‚Äëprovider pricing, expected request volume) and revisit them quarterly as market rates evolve.\n\nBottom Line:\nEffective resource allocation hinges on a disciplined, data‚Äëdriven view of both technical performance and financial impact. By embedding cost‚Äëefficiency considerations into every stage‚Äîfrom model selection through to production monitoring‚Äîorganizations can maximize the return on their AI investments while staying within budgetary and sustainability constraints.\n\n6. Limitations and Open Questions\nThe empirical findings presented in this work illuminate several important trends, yet they also expose a set of constraints and unresolved issues that merit further investigation. The subsection below enumerates the principal limitations and outlines the key open questions that arise from each.\n\n\n\n6.1. Deviations from Ideal Power‚ÄëLaw Behavior\n\nEmpirical deviations. In several experimental regimes the observed scaling deviates systematically from the theoretically predicted power‚Äëlaw exponent. Specifically, for input distributions with heavy tails, the exponent appears to saturate at a lower value than anticipated, suggesting the presence of hidden bottlenecks that are not captured by the baseline model.\n\nFinite‚Äësize effects. The power‚Äëlaw regime is only observable over a limited range of scales; beyond this range, discretization and boundary effects dominate, leading to curvature in log‚Äëlog plots. Quantifying the size of the ‚Äúasymptotic window‚Äù remains an open analytical challenge.\n\nModel dependence. The deviations are sensitive to the choice of regularization and initialization strategies. While certain initialization schemes restore power‚Äëlaw behavior, they introduce additional hyper‚Äëparameters whose optimal settings are not yet fully understood.\n\nOpen question: Can a unified theoretical framework be developed that predicts the conditions under which power‚Äëlaw scaling breaks down, and that provides principled remedies (e.g., adaptive regularization) to recover the ideal exponent?\n\n\n\n6.2. Generalization Beyond the Studied Regimes\n\nOut‚Äëof‚Äëdistribution (OOD) inputs. The current experiments focus on a narrow band of input statistics (e.g., Gaussian, low‚Äëfrequency sinusoids). Preliminary tests on OOD datasets reveal a marked degradation in performance, indicating that the learned representations may be over‚Äëfitted to the training distribution.\n\nTemporal and dynamical extensions. Although the static analysis suffices for the present scope, extending the methodology to time‚Äëvarying or sequential inputs raises questions about stability, memory retention, and the emergence of recurrent dynamics.\n\nMulti‚Äëmodal interactions. The interplay between heterogeneous modalities (e.g., vision‚Äëlanguage, multimodal sensor fusion) has not been examined. Preliminary observations suggest that cross‚Äëmodal correlations may either amplify or suppress the power‚Äëlaw signatures observed in unimodal settings.\n\nOpen question: What architectural or training modifications are necessary to ensure robust generalization to unseen input distributions and to maintain power‚Äëlaw scaling in more complex, dynamic, or multimodal contexts?\n\n\n\n6.3. Role of Architectural Innovations and Sparsity\n\nSparse connectivity patterns. While sparse weight matrices have been shown to improve computational efficiency, their impact on the statistical properties of the learned representations is still ambiguous. In some cases, sparsity leads to a flattening of the power‚Äëlaw tail, whereas in others it accentuates it.\n\nNon‚Äëstandard layer designs. Recent architectural innovations‚Äîsuch as gated residual pathways, adaptive activation functions, and hierarchical attention mechanisms‚Äîintroduce additional nonlinearities that can perturb the scaling behavior. Systematic ablation studies are required to isolate which components are responsible for observed deviations.\n\nScalability limits. Scaling these innovations to larger model families (e.g., billions of parameters) may introduce new regimes where the assumptions underlying the power‚Äëlaw analysis no longer hold, particularly concerning memory bandwidth and communication constraints.\n\nOpen question: How can architectural design be guided by scaling laws to deliberately shape the statistical structure of representations, and what trade‚Äëoffs arise when moving from sparse, low‚Äëdimensional prototypes to high‚Äëcapacity, sparsely activated networks?\n\n\n\n6.4. Ethical and Environmental Considerations\n\nEnergy consumption. Training models that exhibit pronounced power‚Äëlaw scaling often requires extensive computational resources, leading to substantial electricity usage and associated carbon emissions. Quantifying the environmental footprint of such training pipelines and exploring energy‚Äëefficient alternatives is an emerging priority.\n\nBias amplification. The statistical regularities captured by power‚Äëlaw models can inadvertently reinforce existing societal biases present in the training data. For instance, skewed frequency distributions may cause over‚Äërepresentation of certain subpopulations, leading to disparate impacts in downstream applications.\n\nTransparency and accountability. The opaque nature of scaling relationships can hinder interpretability, making it difficult to audit model behavior or to certify compliance with fairness and safety standards. Developing explainable metrics that link scaling exponents to ethical outcomes is an open research avenue.\n\nOpen question: What principled frameworks can reconcile the pursuit of improved scaling performance with sustainability goals and ethical safeguards, and how can such frameworks be operationalized in model development pipelines?\n\nSummary. Addressing the limitations and open questions outlined above will be essential for advancing both the theoretical understanding and practical deployment of power‚Äëlaw‚Äëguided methodologies. Future work should aim to (i) refine the theoretical foundations that predict scaling breakdowns, (ii) extend empirical validation to richer input spaces, (iii) systematically dissect the influence of architectural choices and sparsity, and (iv) embed ethical and environmental considerations into the design and evaluation process. Only through a coordinated effort across these dimensions can the full potential of scaling laws be realized in a responsible and sustainable manner.\n\n7. Future Directions\nThe rapid evolution of large‚Äëscale language models has exposed both the promise and the limits of current scaling paradigms. Anticipating the next generation of research and deployment requires a shift from purely empirical growth toward more principled, data‚Äëcentric, and predictive frameworks. The following subsections outline three interrelated avenues that are poised to reshape how we design, evaluate, and operationalize future models.\n\n\n\n7.1. Emerging Scaling Regimes (e.g., Multimodal, Reasoning‚ÄëFocused Models)\n\nMultimodal Integration\n\nConcept: Extending the parameter‚Äëcentric paradigm to incorporate heterogeneous data streams‚Äîtext, vision, audio, and structured knowledge‚Äîwithin a unified architecture.\n\nImplications: Scaling laws must now account for cross‚Äëmodal token budgets and alignment costs (e.g., joint embedding layers, contrastive pre‚Äëtraining). Early evidence suggests that effective model size grows sub‚Äëlinearly with raw parameter count when modalities are balanced, prompting a re‚Äëexamination of ‚Äúbigger‚Äëis‚Äëbetter‚Äù heuristics.\n\nResearch Frontiers: Development of modular token‚Äëfusion mechanisms, dynamic modality weighting, and curriculum‚Äëdriven data mixing strategies that preserve scalability while enhancing multimodal reasoning.\n\nReasoning‚ÄëFocused Architectures\n\nConcept: Designing models whose capacity is explicitly allocated to structured inference (e.g., chain‚Äëof‚Äëthought, symbolic manipulation, program synthesis) rather than merely memorizing surface patterns.\n\nImplications: Scaling regimes shift from ‚Äúparameter‚Äëheavy‚Äù to ‚Äúcompute‚Äëheavy‚Äù regimes, where effective model size is measured in reasoning steps per token and depth of latent deliberation. This gives rise to sparse scaling laws that penalize unnecessary breadth but reward depth.\n\nResearch Frontiers: Exploration of self‚Äëgenerated reasoning traces, reinforcement‚Äëlearning‚Äëfrom‚Äëhuman‚Äëfeedback (RLHF) on logical consistency, and neuro‚Äësymbolic hybrids that can be scaled predictably.\n\n\n\n\n\n7.2. Alternative Formulation of Scaling Laws (e.g., Data‚ÄëAware Scaling)\n\nFrom Parameter‚ÄëCentric to Data‚ÄëCentric Metrics\n\nTraditional scaling laws relate model performance (P) to parameter count (N) and dataset size (D) as (P N^{} D^{}). Recent work proposes data‚Äëefficiency indices that weight each token by its informational gain (e.g., novelty, difficulty, semantic richness).\n\nThis yields a data‚Äëaware scaling law: (P _{i=1}^{D} w_i f(N_i)), where (w_i) encodes token importance and (f) captures diminishing returns of additional parameters on high‚Äëvalue data.\n\nIncorporating Compute Budgets and Training Dynamics\n\nBy treating effective compute (C) (FLOPs) as a third axis, we can express performance as (P = g(N, D, C)) with budget‚Äëaware exponents that reflect optimal allocation across pre‚Äëtraining, fine‚Äëtuning, and in‚Äëcontext learning.\n\nEmpirical studies suggest an optimal trade‚Äëoff surface where marginal gains from extra parameters are outpaced by gains from targeted data augmentation or curriculum scheduling.\n\nPredictive Modelling and Generalisation Bounds\n\nLeveraging statistical learning theory, researchers are deriving generalisation bounds that tie scaling exponents to covering numbers of the data manifold. Such bounds enable pre‚Äëemptive predictions of required (N) and (D) for a target error tolerance, reducing costly trial‚Äëand‚Äëerror experiments.\n\n\n\n\n\n7.3. Potential for Predictive Tools and Automated Scaling\n\nAutomated Scaling Pipelines\n\nToolkits: Emerging frameworks (e.g., ScaleAI, MetaScale) integrate Bayesian optimization, multi‚Äëfidelity simulation, and differentiable architecture search to propose optimal ((N, D, C)) configurations given a performance target and resource constraints.\n\nWorkflow: Users specify a utility function (e.g., cost‚Äëweighted accuracy), and the system iteratively samples scaling configurations, evaluates them on proxy tasks, and refines its policy via reinforcement learning.\n\nPredictive Modelling of Scaling Behaviour\n\nNeural‚Äëaugmented regressors: Models trained on historic scaling experiments can predict the slope of performance curves for unseen model families, enabling early‚Äëstage forecasting of breakpoint behaviours (e.g., transition from data‚Äëlimited to compute‚Äëlimited regimes).\n\nUncertainty Quantification: Probabilistic models (e.g., Gaussian processes with hierarchical priors) provide confidence intervals around predicted scaling exponents, allowing stakeholders to assess risk before committing to massive training runs.\n\nEthical and Operational Implications\n\nPredictive scaling tools democratize access to high‚Äëperforming models by allowing smaller labs to leverage the same scaling insights previously reserved for industry giants.\n\nHowever, they also raise concerns about over‚Äëreliance on extrapolation, potential bias amplification if historical data reflect inequities, and the need for transparent accounting of assumptions (e.g., distribution shift, hardware constraints).\n\n\n\nSummary\nFuture directions in scaling research are converging on three synergistic thrusts: (1) redefining what it means to scale by embedding multimodal and reasoning capabilities into the model fabric; (2) recasting scaling laws to be explicitly data‚Äëaware, compute‚Äëaware, and statistically grounded; and (3) building automated, predictive tooling that can guide resource allocation with quantified uncertainty. Together, these advances promise a more principled and efficient pathway toward the next generation of large‚Äëscale AI systems.\n\n8. Conclusion and Description: Synthesis of Key Insights and Final Take‚Äëaways\n\n\n\n1. Overview of Core Findings\n\nInterdisciplinary Convergence: The project demonstrated that integrating [Domain A], [Domain B], and [Domain C] yields a synergistic framework that outperforms siloed approaches.\n\nEvidence‚ÄëBased Impact: Quantitative metrics (e.g., a 23‚ÄØ% increase in efficiency, 15‚ÄØ% reduction in error rates) and qualitative feedback from stakeholders confirm the tangible benefits of the proposed solution.\n\nScalability & Transferability: The methodology proved adaptable across [Context 1], [Context 2], and [Context 3], suggesting strong potential for broader deployment in similar environments.\n\n\n\n2. Key Insights\n\n\n\n\n\n\n\n\nInsight\nDescription\nImplication\n\n\n\n\n1. Process Alignment\nAligning workflow stages with [specific principle] eliminated bottlenecks.\nStreamlined operations and reduced cycle time by X‚ÄØ%.\n\n\n2. Data‚ÄëDriven Decision Making\nLeveraging real‚Äëtime analytics enabled proactive adjustments.\nImproved predictive accuracy from Y‚ÄØ% ‚Üí Z‚ÄØ%.\n\n\n3. Stakeholder Engagement\nEarly involvement of end‚Äëusers fostered ownership and reduced resistance.\nAdoption rate rose to 85‚ÄØ% within the first quarter.\n\n\n4. Continuous Improvement Loop\nEmbedding feedback mechanisms sustains iterative refinement.\nEstablished a quarterly review cadence that drives ongoing enhancements.\n\n\n\n\n\n3. Final Take‚Äëaways\n\nStrategic Integration Is Critical ‚Äì Combining complementary strengths across disciplines creates a multiplier effect that single‚Äëdomain solutions cannot achieve.\n\nMetrics‚ÄëCentric Approach Enhances Credibility ‚Äì Quantifiable outcomes provide a clear business case for continued investment and replication.\n\nHuman‚ÄëCentric Design Drives Adoption ‚Äì Engaging end‚Äëusers from the outset translates technical gains into practical, sustainable results.\n\nScalable Frameworks Enable Future Growth ‚Äì The modular architecture allows for incremental expansion into new markets or use‚Äëcases without major redesign.\n\nContinuous Feedback Is Non‚ÄëNegotiable ‚Äì Embedding mechanisms for ongoing learning ensures the solution remains relevant amid evolving constraints and opportunities.\n\n\n\n4. Recommendations for Next Steps\n\nPilot Expansion: Deploy the framework in [Target Region/Department] to validate scalability under varied operational conditions.\n\nResource Allocation: Secure additional [budget/skill‚Äëset] to accelerate implementation phases and support training initiatives.\n\nPerformance Monitoring: Establish a dashboard of KPIs (e.g., throughput, error rate, user satisfaction) to track long‚Äëterm impact.\n\nKnowledge Transfer: Develop a playbook documenting best practices, lessons learned, and configuration templates for future teams.\n\nStakeholder Communication: Maintain a regular cadence of updates to keep sponsors, partners, and end‚Äëusers aligned with progress and outcomes.\n\n\nBottom Line: The synthesis of insights confirms that a coordinated, data‚Äëinformed, and stakeholder‚Äëfocused approach not only delivers measurable performance gains but also establishes a resilient foundation for future innovation. By institutionalizing the identified best practices and scaling the framework responsibly, the organization is well positioned to achieve sustained competitive advantage.\n\n9. References and Description\nComprehensive list of peer‚Äëreviewed papers, technical reports, and credible web sources.\n\n\n\n9.1 Purpose\nThe References and Description section serves three primary objectives:\n\nTransparency ‚Äì Provide readers with a clear audit trail of every scholarly and technical source that informed the research, analysis, or design presented in this report.\n\nCredibility ‚Äì Demonstrate that all factual statements, data sets, models, and design decisions are grounded in vetted, peer‚Äëreviewed literature or reputable institutional publications.\n\nReproducibility ‚Äì Enable other researchers to locate, retrieve, and, where appropriate, replicate the underlying evidence that supports the findings and recommendations.\n\n\n\n\n9.2 Scope of Sources\n\n\n\n\n\n\n\n\nCategory\nTypical Content\nExample Sources\n\n\n\n\nPeer‚Äëreviewed journal articles\nOriginal research findings, literature reviews, meta‚Äëanalyses, theoretical frameworks.\nIEEE Transactions on Neural Networks, Journal of Machine Learning Research, Nature Communications\n\n\nConference proceedings\nCutting‚Äëedge results presented at major scientific or engineering conferences.\nProceedings of the International Conference on Machine Learning (ICML), ACM SIGGRAPH\n\n\nTechnical reports & white papers\nIn‚Äëdepth studies from government agencies, industry research labs, or standards bodies.\nNASA Technical Report, Microsoft Research Technical Report, ISO/IEC 27001\n\n\nBooks & book chapters\nAuthoritative syntheses, historical context, or comprehensive theory.\nPattern Recognition and Machine Learning (Bishop), Deep Learning (Goodfellow, Bengio & Courville)\n\n\nCredible web resources\nData repositories, open‚Äësource code bases, authoritative databases, and policy documents.\nUCI Machine Learning Repository, World Health Organization (WHO) Fact Sheets, NASA Earthdata\n\n\nStandards & regulations\nMandatory or widely‚Äëadopted specifications that shape methodology or implementation.\nISO/IEC 17025, IEEE 802.11, EU General Data Protection Regulation (GDPR)\n\n\n\nOnly sources that meet the following criteria are included:\n\nPeer‚Äëreviewed (for journal articles and conference papers) or formally reviewed (for technical reports and standards).\n\nPublicly accessible (or available through institutional subscriptions) and citable with a stable identifier (DOI, URL, or report number).\n\nDirectly relevant to the objectives, methodology, or data used in the current work.\n\n\n\n\n9.3 Organization of the Reference List\nThe references are organized alphabetically by the first author‚Äôs surname (or by the responsible organization for reports). Each entry follows the APA 7th edition format, with the following supplemental fields added to aid navigation:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nDOI / URL\nPersistent identifier or direct link to the source.\n\n\nAccess Date\nDate on which the source was retrieved (required for dynamic web content).\n\n\nVersion / Retrieval Note\nFor datasets or code repositories, the specific version number or commit hash used.\n\n\nKey Findings / Relevance\nA one‚Äësentence annotation summarizing why the source is cited in the report.\n\n\n\nExample entry (APA style with annotation):\n\nSmith, J. A., & Lee, K. (2022). Deep reinforcement learning for autonomous navigation in urban environments. IEEE Transactions on Robotics, 38(4), 2150‚Äë2165. https://doi.org/10.1109/TRO.2022.1234567\nProvides the algorithmic framework and benchmark datasets used for the navigation module described in Section‚ÄØ4.2.\n\n\n\n\n9.4 Annotated Bibliography (Sample)\nBelow is a representative sample of the annotated bibliography that will appear in the final report. (The complete list contains ‚âà‚ÄØ150 entries.)\n\n\n\n\n\n\n\n\n#\nReference\nAnnotation\n\n\n\n\n1\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\nClassic textbook that introduces Bayesian inference, graphical models, and variational methods; foundational for the probabilistic models used in Chapter‚ÄØ3.\n\n\n2\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nComprehensive overview of deep learning architectures; consulted for justification of convolutional network choices in Section‚ÄØ5.1.\n\n\n3\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770‚Äë778.\nIntroduces residual connections that inspired the architecture of the image‚Äëclassification pipeline described in Section‚ÄØ5.3.\n\n\n4\nNASA (2023). Earth Observing System Data and Information System (EOSDIS) ‚Äì Data Holdings.\nProvides the multi‚Äëspectral satellite imagery dataset used for the environmental monitoring case study (Section‚ÄØ6.1).\n\n\n5\nWorld Health Organization. (2022). Global Health Estimates 2022.\nSupplies the baseline mortality and disease‚Äëburden statistics cited in the policy‚Äëimpact analysis (Section‚ÄØ7.2).\n\n\n6\nISO/IEC (2021). ISO/IEC 27001:2022 Information security, cybersecurity and privacy protection ‚Äì Information security management systems ‚Äì Requirements.\nGoverns the security controls implemented in the proposed system architecture (Section‚ÄØ4.4).\n\n\n7\nZhang, Y., et al.¬†(2024). Scalable federated learning for edge devices. Nature Machine Intelligence, 6, 1123‚Äë1135.\nPresents the federated learning framework adopted for privacy‚Äëpreserving model updates (Section‚ÄØ3.5).\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nThe full annotated bibliography will be appended as Appendix‚ÄØA.\n\n\n\n9.5 Verification of Source Quality\nEach source was evaluated against the following quality‚Äëassurance checklist:\n\n\n\n\n\n\n\nCriterion\nAssessment\n\n\n\n\nPeer‚Äëreview status\nConfirmed via journal/conference editorial board or editorial statement.\n\n\nAuthoritativeness\nAuthors hold relevant academic or industry credentials; affiliations are reputable.\n\n\nCurrency\nPublication date ‚â§‚ÄØ5‚ÄØyears unless the work is a seminal, foundational contribution.\n\n\nRelevance\nDirectly cited in the text or used to support a methodological choice.\n\n\nAccessibility\nDOI or stable URL available; no pay‚Äëwall restrictions for readers of the report.\n\n\nConflict of interest\nNo evident commercial bias that would compromise objectivity.\n\n\n\nSources that failed any of these criteria were excluded or replaced with an equivalent alternative.\n\n\n\n9.6 How to Use This Section\n\nFor reviewers: Consult the annotated bibliography to verify that every claim is substantiated by a reliable source.\n\nFor readers: Follow the DOI/URL links to retrieve the original documents for deeper exploration.\n\nFor future work: The reference list serves as a curated starting point for anyone wishing to extend, replicate, or critique the present study.\n\n\n\n\n9.7 Limitations\n\nCoverage bias: While every effort was made to include all pertinent literature up to the cut‚Äëoff date (November‚ÄØ2025), some very recent pre‚Äëprints or region‚Äëspecific reports may not be captured.\n\nLanguage restriction: The bibliography focuses primarily on English‚Äëlanguage sources; non‚ÄëEnglish scholarly works that are directly relevant have been deliberately omitted to maintain consistency in annotation.\n\n\n\n\n9.8 Future Updates\nThe reference list will be periodically reviewed (at least annually) to incorporate newly published peer‚Äëreviewed works, emerging standards, and updated data repositories. Updates will be recorded in a version‚Äëcontrolled changelog (Appendix‚ÄØB) to maintain a transparent evolution of the source base.\n\nEnd of Section‚ÄØ9 ‚Äì References and Description.\n\nAppendices and Description\nThe following appendices supplement the main body of the report. They are organized into four distinct parts, each serving a specific purpose to enhance clarity, reproducibility, and completeness of the presented material.\n\n\n\nA. Glossary of Terms\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\nContext of Use\nNotes\n\n\n\n\nANOVA\nAnalysis of Variance\nStatistical test comparing means across multiple groups\nAssumptions: normality, homogeneity of variance\n\n\nCI\nConfidence Interval\nRange of values that likely contain the population parameter\n95‚ÄØ% CI is reported unless otherwise specified\n\n\nFDR\nFalse Discovery Rate\nProportion of false positives among rejected hypotheses\nUsed when controlling for multiple testing\n\n\nICC\nIntraclass Correlation Coefficient\nMeasure of reliability for clustered data\nValues range from 0 to 1; &gt;0.75 indicates high reliability\n\n\nLME\nLinear Mixed‚ÄëEffects Model\nRegression model accounting for both fixed and random effects\nSoftware: lme4 (R) or lmerTest\n\n\np‚Äëvalue\nProbability value\nSignificance test against the null hypothesis\nReported to three decimal places; ‚Äú&lt;0.001‚Äù when appropriate\n\n\nQ‚Äëstatistic\nQuadratic form statistic\nUsed in goodness‚Äëof‚Äëfit tests for multivariate data\nComputed from residual covariance matrix\n\n\nR¬≤\nCoefficient of Determination\nProportion of variance explained by the model\nAdjusted R¬≤ is reported for models with multiple predictors\n\n\nSD\nStandard Deviation\nMeasure of dispersion around the mean\nReported for all continuous variables\n\n\nSE\nStandard Error\nEstimated standard deviation of a sampling distribution\nUsed for confidence‚Äëinterval construction\n\n\nSkewness\nAsymmetry of a distribution\nIndicates whether the distribution is symmetric\nPositive values indicate right‚Äëskewed data\n\n\nKurtosis\n‚ÄúPeakedness‚Äù of a distribution\nMeasures tail heaviness relative to a normal distribution\nExcess kurtosis is reported (normal = 0)\n\n\n\nAll terms are defined at first appearance in the main text; the glossary provides a quick reference for readers who may encounter them out of context.\n\n\n\nB. Detailed Data Tables\n\n\n\n\n\n\n\n\n\nTable\nDescription\nKey Columns\nSample Row (illustrative)\n\n\n\n\nTable‚ÄØA1\nSummary statistics for all variables (n, mean, SD, min, max)\nVariable, Units, N, Mean, SD, Min, Max, Median\nAge (years), 150, 48.2, 12.5, 22, 78, 46\n\n\nTable‚ÄØA2\nCorrelation matrix (Pearson r) among continuous predictors\nVariable‚ÄØ1, Variable‚ÄØ2, r, p‚Äëvalue\nAge, Income, 0.34, 0.001\n\n\nTable‚ÄØA3\nResults of the primary statistical test (e.g., ANOVA)\nSource, df, F, p, Œ∑¬≤\nTreatment, 2, 5.67, 0.004, 0.036\n\n\nTable‚ÄØA4\nModel coefficients for the final mixed‚Äëeffects model\nFixed Effect, Estimate, SE, t, p, 95‚ÄØ% CI\nIntercept, 3.12, 0.45, 6.93, &lt;0.001, 2.24‚Äì4.00\n\n\nTable‚ÄØA5\nSensitivity analysis results (subgroup analyses)\nSubgroup, N, Effect Size, p‚Äëvalue\nAge‚ÄØ&gt;‚ÄØ65, 38, 0.42, 0.02\n\n\nTable‚ÄØA6\nMissing‚Äëdata summary\nVariable, Missing N, % Missing, Imputation Method\nIncome, 5, 3.3‚ÄØ%, Multiple Imputation\n\n\n\nAll tables are presented in LaTeX format in the manuscript and are also provided as separate Excel files (Appendix‚ÄØB.xlsx) for reader convenience.\n\n\n\nC. Additional Plots and Statistical Analyses\n\n\n\n\n\n\n\n\n\nPlot\nPurpose\nDescription of Content\nLocation in Appendix\n\n\n\n\nFigure‚ÄØC1\nResidual diagnostics for the LME\nNormal‚Äëprobability plot, residual vs.¬†fitted scatter, heteroscedasticity check\nPage‚ÄØA‚Äë12\n\n\nFigure‚ÄØC2\nDistribution of the primary outcome across quartiles\nKernel density estimate with overlay of mean and 95‚ÄØ% CI\nPage‚ÄØA‚Äë13\n\n\nFigure‚ÄØC3\nInteraction plot for the treatment √ó covariate effect\nLine graph showing predicted outcomes at low, medium, and high levels of the covariate\nPage‚ÄØA‚Äë14\n\n\nFigure‚ÄØC4\nForest plot of subgroup effects\nSummary estimates with 95‚ÄØ% CI for each predefined subgroup\nPage‚ÄØA‚Äë15\n\n\nFigure‚ÄØC5\nHeatmap of the correlation matrix\nColor‚Äëcoded matrix with hierarchical clustering of variables\nPage‚ÄØA‚Äë16\n\n\nFigure‚ÄØC6\nKaplan‚ÄëMeier survival curves (if applicable)\nCurves for each categorical group with log‚Äërank test statistics\nPage‚ÄØA‚Äë17\n\n\nFigure‚ÄØC7\nSensitivity analysis ‚Äì ROC curves\nArea under the curve (AUC) with 95‚ÄØ% CI for each model variant\nPage‚ÄØA‚Äë18\n\n\n\nAll plots are generated using ggplot2 (R) or Matplotlib (Python) and are saved in both vector (PDF) and raster (PNG, 300‚ÄØdpi) formats. The complete reproducible code is provided in the supplementary GitHub repository (link in the Data Availability statement).\n\n\n\nD. Glossary of Abbreviations\n\n\n\n\n\n\n\n\n\nAbbreviation\nFull Form\nFirst Appearance (Section/Page)\nMeaning in Report\n\n\n\n\nANOVA\nAnalysis of Variance\n3.2, p.‚ÄØ15\nStatistical test for group differences\n\n\nAUC\nArea Under the Curve\n4.1, p.‚ÄØ27\nPerformance metric for binary classifiers\n\n\nCI\nConfidence Interval\n2.1, p.‚ÄØ8\nInterval estimate of a parameter\n\n\ndf\nDegrees of Freedom\n3.4, p.‚ÄØ19\nParameter that quantifies sample information\n\n\nFDR\nFalse Discovery Rate\n5.3, p.‚ÄØ34\nExpected proportion of false positives\n\n\nICC\nIntraclass Correlation Coefficient\n2.5, p.‚ÄØ22\nReliability measure for clustered data\n\n\nIQR\nInter‚ÄëQuartile Range\n2.3, p.‚ÄØ12\nMeasure of statistical dispersion\n\n\nLME\nLinear Mixed‚ÄëEffects Model\n3.5, p.‚ÄØ23\nRegression model with random effects\n\n\nN\nSample Size\nThroughout\nNumber of observations\n\n\np‚Äëvalue\nProbability value\n2.2, p.‚ÄØ9\nSignificance level for hypothesis testing\n\n\nQ‚Äëstatistic\nQuadratic Form Statistic\n4.2, p.‚ÄØ28\nGoodness‚Äëof‚Äëfit test statistic\n\n\nR¬≤\nCoefficient of Determination\n3.1, p.‚ÄØ13\nProportion of variance explained\n\n\nSE\nStandard Error\n2.4, p.‚ÄØ14\nEstimated standard deviation of a statistic\n\n\nSD\nStandard Deviation\n2.3, p.‚ÄØ12\nMeasure of data variability\n\n\nSPSS\nStatistical Package for the Social Sciences\n2.1, p.‚ÄØ7\nSoftware used for initial analyses\n\n\nIQR\nInter‚ÄëQuartile Range\n2.3, p.‚ÄØ12\n25th‚Äì75th percentile range\n\n\nUCL\nUpper Control Limit\n6.1, p.‚ÄØ41\nThreshold for process control charts\n\n\nWHO\nWorld Health Organization\n1.1, p.‚ÄØ1\nInternational health authority\n\n\n\nAbbreviations are defined at first use in the text; the glossary provides a consolidated reference for quick lookup.\n\nEnd of Appendices\nThese supplementary materials are intended to facilitate full transparency of the analytical workflow, enable independent verification of the results, and provide the reader with all necessary context to interpret the findings without over‚Äëburdening the main manuscript."
  },
  {
    "objectID": "L01/08_patterns.html#evaluator-optimizer",
    "href": "L01/08_patterns.html#evaluator-optimizer",
    "title": "Workflows and Agents",
    "section": "Evaluator-optimizer",
    "text": "Evaluator-optimizer\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\nEvaluator-optimizer workflows are commonly used when there‚Äôs particular success criteria for a task, but iteration is required to meet that criteria. For example, there‚Äôs not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\n\n\nfrom pydantic import BaseModel, Field\nfrom typing import Literal \nfrom langgraph.func import entrypoint, task\n\n\n# Schema for structured output to use in evaluation\nclass Feedback(BaseModel):\n    grade: Literal[\"funny\", \"not funny\"] = Field(\n        description=\"Decide if the joke is funny or not.\",\n    )\n    feedback: str = Field(\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\n    )\n\n\n# Augment the LLM with schema for structured output\nevaluator = llm.with_structured_output(Feedback)\n\n\n# Nodes\n@task\ndef llm_call_generator(topic: str, feedback: Feedback):\n    \"\"\"LLM generates a joke\"\"\"\n    if feedback:\n        msg = llm.invoke(\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\n        )\n    else:\n        msg = llm.invoke(f\"Write a joke about {topic}\")\n    return msg.content\n\n\n@task\ndef llm_call_evaluator(joke: str):\n    \"\"\"LLM evaluates the joke\"\"\"\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\n    return feedback\n\n\n@entrypoint()\ndef optimizer_workflow(topic: str):\n    feedback = None\n    while True:\n        joke = llm_call_generator(topic, feedback).result()\n        feedback = llm_call_evaluator(joke).result()\n        if feedback.grade == \"funny\":\n            break\n\n    return joke\n\n\n# Invoke\nfor step in optimizer_workflow.stream(\"mouse\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n\n{'llm_call_generator': 'Why did the mouse get a promotion at the cheese factory?\\n\\nBecause it always delivered the *big* cheese! üê≠üßÄ'}"
  },
  {
    "objectID": "L01/08_patterns.html#agents",
    "href": "L01/08_patterns.html#agents",
    "title": "Workflows and Agents",
    "section": "Agents",
    "text": "Agents\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\n\n\n\n\n\n\n\nNote\n\n\n\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\n\n\n\nfrom langchain.tools import tool\n\n\n# Define tools\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a * b\n\n\n@tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Adds `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a + b\n\n\n@tool\ndef divide(a: int, b: int) -&gt; float:\n    \"\"\"Divide `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a / b\n\n\n# Augment the LLM with tools\ntools = [add, multiply, divide]\ntools_by_name = {tool.name: tool for tool in tools}\nllm_with_tools = llm.bind_tools(tools)\n\n\nfrom langgraph.graph import add_messages\nfrom langchain.messages import (\n    SystemMessage,\n    HumanMessage,\n    ToolCall,\n)\nfrom langchain_core.messages import BaseMessage\n\n\n@task\ndef call_llm(messages: list[BaseMessage]):\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\n    return llm_with_tools.invoke(\n        [\n            SystemMessage(\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n            )\n        ]\n        + messages\n    )\n\n\n@task\ndef call_tool(tool_call: ToolCall):\n    \"\"\"Performs the tool call\"\"\"\n    tool = tools_by_name[tool_call[\"name\"]]\n    return tool.invoke(tool_call)\n\n\n@entrypoint()\ndef agent(messages: list[BaseMessage]):\n    llm_response = call_llm(messages).result()\n\n    while True:\n        if not llm_response.tool_calls:\n            break\n\n        # Execute tools\n        tool_result_futures = [\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\n        ]\n        tool_results = [fut.result() for fut in tool_result_futures]\n        messages = add_messages(messages, [llm_response, *tool_results])\n        llm_response = call_llm(messages).result()\n\n    messages = add_messages(messages, llm_response)\n    return messages\n\n\n# Invoke\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\n    print(chunk)\n    print(\"\\n\")\n\n{'call_llm': AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 530, 'total_tokens': 646, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 83, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771681275-L9PdIbxnKPdh7nLN8k3S', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c806e-db5b-7b43-a06e-af491e6c4ff4-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 4}, 'id': 'call_74b1f662907e4881b16fcdfd', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 530, 'output_tokens': 116, 'total_tokens': 646, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 83}})}\n\n\n{'call_tool': ToolMessage(content='7', name='add', tool_call_id='call_74b1f662907e4881b16fcdfd')}\n\n\n{'call_llm': AIMessage(content='The sum of 3 and 4 is **7**. Let me know if you need help with anything else! üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 109, 'prompt_tokens': 580, 'total_tokens': 689, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 82, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771681278-G987J07cb2jyXzn7GauO', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c806e-eb25-7d03-932f-18261e78fac6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 580, 'output_tokens': 109, 'total_tokens': 689, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 82}})}\n\n\n{'agent': [HumanMessage(content='Add 3 and 4.', additional_kwargs={}, response_metadata={}, id='a272ab84-361b-402a-850f-13f32fea27ff'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 530, 'total_tokens': 646, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 83, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771681275-L9PdIbxnKPdh7nLN8k3S', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c806e-db5b-7b43-a06e-af491e6c4ff4-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 4}, 'id': 'call_74b1f662907e4881b16fcdfd', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 530, 'output_tokens': 116, 'total_tokens': 646, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 83}}), ToolMessage(content='7', name='add', id='ec01cc1d-7857-4d31-b025-c861176c6646', tool_call_id='call_74b1f662907e4881b16fcdfd'), AIMessage(content='The sum of 3 and 4 is **7**. Let me know if you need help with anything else! üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 109, 'prompt_tokens': 580, 'total_tokens': 689, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 82, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771681278-G987J07cb2jyXzn7GauO', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c806e-eb25-7d03-932f-18261e78fac6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 580, 'output_tokens': 109, 'total_tokens': 689, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 82}})]}"
  },
  {
    "objectID": "L01/10_projects.html",
    "href": "L01/10_projects.html",
    "title": "Projects",
    "section": "",
    "text": "See real Use Cases to get ideas: https://docs.langchain.com/oss/python/langgraph/case-studies\nSome ideas:\n\nFAQ + Chat when unsure about the answer. Main idea: use RAG to answer questions, and use a chatbot to answer questions when the answer is not found in the RAG. use-case: https://laws.moj.gov.sa\nAutomated selection of the best candidate for a job. Given a pool of candidates, when the Hiring Partner submits a job description, the system should be able to select the best candidate from the pool. Algorithm: use ELO scoring as in chess, and make a hierarchy to reduce complexity from O(n^2) to O(n log n). The system should be able to handle the case where the pool is large (e.g.¬†10000 candidates).\nDeep research: retrieve, rate and rank curriculums for a specific technical topic on many criteria:\n\nWhat ideas are commonly covered, and what ideas are unique to some courses over others.\nWhat pre-requisites are commonly mentioned for theses courses (and how much depth is required)\nInformation density: depth, volume and timeline.\nWhich takes a bottom-up vs a top-down approach\nWhat type of exercises are used.\nInstructors\n\nWhich courses are co-authored vs single-authored\nIs the instructor an expert in the topic\nHow many years of experience does the instructor have\n\n\nAutomated Customer Service (Voice).\nA chatbot that answers questions based on data scraped from laws.moj.gov.sa (or similar)"
  },
  {
    "objectID": "L01/09_langsmith.html",
    "href": "L01/09_langsmith.html",
    "title": "What is LangSmith?",
    "section": "",
    "text": "What Is LangSmith? Explained in 5 Minutes."
  }
]