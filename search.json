[
  {
    "objectID": "L01/00_intro.html",
    "href": "L01/00_intro.html",
    "title": "Introduction: What is Agentic AI?",
    "section": "",
    "text": "Artificial Intelligence is a field of Computer Science, studying how to automate decision making.\nAgentic AI is where autonomy of the system is at the level of dealing not just with structured tabular data, but with unstructured data such as natural language, speech, visual to inform their decision, in a growingly less supervised manner; and hence, autonomous.\nSpecifically, today‚Äôs Agentic AI systems are driven by Large Language Models (LLMs)."
  },
  {
    "objectID": "L01/00_intro.html#the-evolution-of-agents-from-llms",
    "href": "L01/00_intro.html#the-evolution-of-agents-from-llms",
    "title": "Introduction: What is Agentic AI?",
    "section": "The Evolution of Agents from LLMs",
    "text": "The Evolution of Agents from LLMs\nLarge Language Models (LLMs) are deep neural networks trained to model the most likely token given previous tokens.\nFirst application was: Translation. Given an English sentence, the task is to generate the most likely Arabic equivalent. This is done word-by-word, by repeatedly predicting the next Arabic word, the original sentence plus all predicted words thus far.\nThis technique was able to scale thanks to the Transformers architecture which is based on the concept of Attention, which learns the influence each previous token has in generating the next token, in the translation.\nSame concept applied more simply in Paraphrasing, Summarization, and even Question-Answering.\nTrained on back-and-forth Chat Conversations, models were able to mimic chat-like interactions.\nA hypothesis was proven at the time that Multi-task Model performs better than a single-task model. A more general idea is to train models to ‚ÄúFollow Instructions‚Äù. In which the user Prompts it to do any of the previously mentioned tasks, on-demand. Of course this needed lots of special Data Curation.\nOne task was especially important: JSON mode: in which models produced their output in json format, such that it can be parsed easily by programs.\nFollowing that, an especially key development was the task of Tool Calling: in which a model is required to select from a set of Python funcition signatures (parameters, types, and docstrings) upon instruction. This is where Agents were born.\nAn Agent is based on an LLM in the following sequence:\n\nprogram takes input from user\nprogram feeds this input + available tools, into the LLM\nLLM generates text parsable as a tool call\nprogram parses the tool call\nprogram executes the tool (function)\nprogram feeds the output to the LLM\nLLM generates output\nprogram prints this output to the user\n\n\nChoosing between models\nQuality of models vary.\n\nHumans vote fairly for the best model on websites like Arena.ai.\nYou can also compare pricing, providers, and usage tends on OpenRouter.ai\nAn automatic short list models based on your priorities: Model Recommendation | Artficial Analysis"
  },
  {
    "objectID": "L01/00_intro.html#applications-of-agentic-ai",
    "href": "L01/00_intro.html#applications-of-agentic-ai",
    "title": "Introduction: What is Agentic AI?",
    "section": "Applications of Agentic AI",
    "text": "Applications of Agentic AI\nWhat can Agentic AI do in the real-world? This question is best answered by looking at Case Studies.\nAlso, reading the State of Agent Engineering 2026, a survey of 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents."
  },
  {
    "objectID": "L01/00_intro.html#agent-framework",
    "href": "L01/00_intro.html#agent-framework",
    "title": "Introduction: What is Agentic AI?",
    "section": "Agent Framework",
    "text": "Agent Framework\nLangChain organizes components into these main categories:\n\n\n\nCategory\nPurpose\nKey Components\nUse Cases\n\n\n\n\nModels\nAI reasoning and generation\nChat models, LLMs, Embedding models\nText generation, reasoning, semantic understanding\n\n\nTools\nExternal capabilities\nAPIs, databases, etc.\nWeb search, data access, computations\n\n\nAgents\nOrchestration and reasoning\nReAct agents, tool calling agents\nNondeterministic workflows, decision making\n\n\nMemory\nContext preservation\nMessage history, custom state\nConversations, stateful interactions\n\n\nRetrievers\nInformation access\nVector retrievers, web retrievers\nRAG, knowledge base search\n\n\nDocument processing\nData ingestion\nLoaders, splitters, transformers\nPDF processing, web scraping\n\n\nVector Stores\nSemantic search\nChroma, Pinecone, FAISS\nSimilarity search, embeddings storage\n\n\n\nExamples of other frameworks are:\n\nVercel‚Äôs AI SDK\nCrewAI\nOpenAI Agents SDK\nGoogle ADK\nLlamaIndex"
  },
  {
    "objectID": "L01/00_intro.html#agent-runtime",
    "href": "L01/00_intro.html#agent-runtime",
    "title": "Introduction: What is Agentic AI?",
    "section": "Agent Runtime",
    "text": "Agent Runtime\nRuntimes manage state, and state transitions (orchestration). In other words, building, managing, and deploying long-running, stateful agents. Concretely, things like:\n\nControl-flow: Step by step instructions, conditional execution, and loops.\nPersistence: Thread-level and cross-thread persistence for state management.\nDurable execution: Agents persist through failures and can run for extended periods, resuming from where they left off.\nStreaming: Support for streaming workflows and responses.\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state.\n\nLangGraph builds and runs the flowchart..\n\nRAG (Retrieval-Augmented generation)\n\n\n\n\n\ngraph LR\n    A[User question] --&gt; B[Retriever]\n    B --&gt; C[Relevant docs]\n    C --&gt; D[Chat model]\n    A --&gt; D\n    D --&gt; E[Informed response]\n\n\n\n\n\n\n\n\nAgent with tools\n\n\n\n\n\ngraph LR\n    A[User request] --&gt; B[Agent]\n    B --&gt; C{Need tool?}\n    C --&gt;|Yes| D[Call tool]\n    D --&gt; E[Tool result]\n    E --&gt; B\n    C --&gt;|No| F[Final answer]\n\n\n\n\n\n\n\n\nMulti-agent system\n\n\n\n\n\ngraph LR\n    A[Complex Task] --&gt; B[Supervisor agent]\n    B --&gt; C[Specialist agent 1]\n    B --&gt; D[Specialist agent 2]\n    C --&gt; E[Results]\n    D --&gt; E\n    E --&gt; B\n    B --&gt; F[Coordinated response]\n\n\n\n\n\n\nOther runtimes:\n\nTemporal\nInngest\n\nNote: for the best DX (Developer Experience), we will be using the Functional API."
  },
  {
    "objectID": "L01/00_intro.html#agent-platform",
    "href": "L01/00_intro.html#agent-platform",
    "title": "Introduction: What is Agentic AI?",
    "section": "Agent Platform",
    "text": "Agent Platform\nLangSmith:\n\nDeployment: localhost -&gt; production server\nObservability: tracing, real-time monitoring, alerting and usage.\nEvaluation: testing versions and providing feedback on traces."
  },
  {
    "objectID": "L01/00_intro.html#key-takeways",
    "href": "L01/00_intro.html#key-takeways",
    "title": "Introduction: What is Agentic AI?",
    "section": "Key Takeways",
    "text": "Key Takeways\n\nLangChain is the framework.\nLangGraph is the runtime.\nLangSmith is the platform.\n\n\n\nRead More.\nHelp: Chat with langchain docs"
  },
  {
    "objectID": "L01/tasks.html",
    "href": "L01/tasks.html",
    "title": "Initialize the LLM model",
    "section": "",
    "text": "Tasks can only be called from within an entrypoint, another task, or a state graph node.\n\nTasks cannot be called directly from the main application code.\n\nWhen you call a task, the runtime kick-starts the task, and immediate Future object is returned.\n\nA future is a placeholder for a result that will be available later.\n\nTo obtain the result of a task, you can either:\n\nwait for it synchronously (blocking the thread), using result() or\nawait it asynchronously (non-blocking), using await ## Parallel execution\n\n\nTasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs). from langgraph.func import entrypoint, task\n@task def add_one(number: int) -&gt; int: return number + 1\n@entrypoint() def graph(numbers: list[int]) -&gt; list[str]: futures = [add_one(i) for i in numbers] return [f.result() for f in futures] graph.invoke([10, 20, 30])\nfrom langgraph.func import entrypoint, task from langgraph.checkpoint.memory import InMemorySaver"
  },
  {
    "objectID": "L01/tasks.html#when-to-use-a-task",
    "href": "L01/tasks.html#when-to-use-a-task",
    "title": "Initialize the LLM model",
    "section": "When to use a task",
    "text": "When to use a task\nTasks are useful in the following scenarios:\nCheckpointing: When you need to save the result of a long-running operation to a checkpoint, so you don‚Äôt need to recompute it when resuming the workflow.\nHuman-in-the-loop: If you‚Äôre building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details.\nParallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\nObservability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.\nRetryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic."
  },
  {
    "objectID": "L01/subagents_hil.html",
    "href": "L01/subagents_hil.html",
    "title": "6. Add human-in-the-loop review",
    "section": "",
    "text": "It can be prudent to incorporate human-in-the-loop review of sensitive actions. LangChain includes built-in middleware to review tool calls, in this case the tools invoked by sub-agents.\nLet‚Äôs add human-in-the-loop review to both sub-agents:\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\ncalendar_agent = create_agent(\n    model,\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\"create_calendar_event\": True},\n            description_prefix=\"Calendar event pending approval\",\n        ),\n    ],\n)\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\"send_email\": True},\n            description_prefix=\"Outbound email pending approval\",\n        ),\n    ],\n)\nsupervisor_agent = create_agent(\n    model,\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n    checkpointer=InMemorySaver(),\n)\nLet‚Äôs repeat the query. Note that we gather interrupt events into a list to access downstream:\nquery = (\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n    \"and send them an email reminder about reviewing the new mockups.\"\n)\ninput_ = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n\n# thread_id is needed to later resume the interrupted conversation\nconfig = {\"configurable\": {\"thread_id\": \"6\"}}\n\ninterrupts = []\nfor step in supervisor_agent.stream(input_, config):\n    for update in step.values():\n        if isinstance(update, dict):\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n        else:\n            interrupt_ = update[0]\n            interrupts.append(interrupt_)\n            print(f\"\\nINTERRUPTED: {interrupt_.id}\")\n\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  schedule_event (call_JsP0Ij54jT1X0TcxYafwRcMa)\n\n Call ID: call_JsP0Ij54jT1X0TcxYafwRcMa\n\n  Args:\n\n    request: Schedule a meeting with the design team next Tuesday at 2pm for 1 hour.\n\n\n\nINTERRUPTED: c5cc54c21b9258043abf580bec6fe356\nThis time we‚Äôve interrupted execution. Let‚Äôs inspect the interrupt events:\nfor interrupt_ in interrupts:\n    for request in interrupt_.value[\"action_requests\"]:\n        print(f\"INTERRUPTED: {interrupt_.id}\")\n        print(f\"{request['description']}\\n\")\n\nINTERRUPTED: c5cc54c21b9258043abf580bec6fe356\nCalendar event pending approval\n\nTool: create_calendar_event\nArgs: {'title': 'Design Team Meeting', 'start_time': '2026-02-24T14:00:00', 'end_time': '2026-02-24T15:00:00', 'attendees': ['design-team@example.com'], 'location': ''}\nWe can specify decisions for each interrupt by referring to its ID using a Command. Refer to the human-in-the-loop guide for additional details. For demonstration purposes, here we will:\nas follows:\nfrom langgraph.types import Command\n\nresume = {}\nfor interrupt_ in interrupts:\n    if interrupt_.id == \"c5cc54c21b9258043abf580bec6fe356\":\n        # Edit email\n        edited_action = interrupt_.value[\"action_requests\"][0].copy()\n        edited_action[\"args\"][\"subject\"] = \"Mockups reminder\"\n        resume[interrupt_.id] = {\n            \"decisions\": [{\"type\": \"edit\", \"edited_action\": edited_action}]\n        }\n    else:\n        resume[interrupt_.id] = {\"decisions\": [{\"type\": \"approve\"}]}\n\ninterrupts = []\nfor step in supervisor_agent.stream(\n    Command(resume=resume),\n    config,\n):\n    for update in step.values():\n        if isinstance(update, dict):\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n        else:\n            interrupt_ = update[0]\n            interrupts.append(interrupt_)\n            print(f\"\\nINTERRUPTED: {interrupt_.id}\")\n\n\n================================= Tool Message =================================\n\nName: schedule_event\n\n\n\nAll set. Here‚Äôs what I scheduled:\n\n\n\n- Event: Design Team Meeting\n\n- Date: Tuesday, February 24, 2026\n\n- Time: 2:00 PM ‚Äì 3:00 PM\n\n- Attendees: design-team@example.com\n\n- Location: not specified\n\n\n\nWould you like me to add a location, a video conferencing link, or an agenda?\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  manage_email (call_gLCOpI4HNDDyLPtRWPnUzlBX)\n\n Call ID: call_gLCOpI4HNDDyLPtRWPnUzlBX\n\n  Args:\n\n    request: Send a reminder email to the design team to review the new mockups before the meeting on Tuesday at 2 PM.\n\n================================= Tool Message =================================\n\nName: manage_email\n\n\n\nI‚Äôm ready to send this, but I need two details:\n\n1) The recipient email addresses (or confirm using your Design Team distribution list, e.g., design-team@yourdomain.com).\n\n2) The exact date for ‚ÄúTuesday‚Äù (the date, if you‚Äôd like me to include it in the body).\n\n\n\nDraft email ready to send (you can approve or adjust):\n\n\n\nSubject: Reminder: Review new mockups before Tuesday at 2:00 PM meeting\n\n\n\nBody:\n\nHello Design Team,\n\n\n\nThis is a quick reminder to review the new mockups before the meeting on Tuesday at 2:00 PM. Please share any feedback or questions in advance so we can address them during the discussion.\n\n\n\nThe latest mockups are available here: [location/link to mockups]\n\n\n\nThank you,\n\n[Your Name]\n\n[Your Role/Team]\n\n[Contact information]\n\n\n\nPlease provide:\n\n- The recipient email(s) to send to (or confirm the distribution list).\n\n- The exact date for Tuesday (so I can include the date in the email if you‚Äôd like).\n\n- Any link or location to include for the mockups.\n\n- Your name or signature details for the closing.\n\n\n\nOnce you confirm, I‚Äôll send the email and report back with the sent details.\n\n================================== Ai Message ==================================\n\n\n\nI‚Äôve got the meeting scheduled for Tuesday, February 24, 2026, 2:00‚Äì3:00 PM with the design team (design-team@example.com). Location isn‚Äôt set yet.\n\n\n\nFor the reminder email, I can send it as soon as I have a couple of details. Here‚Äôs a ready-to-send draft with the date included. You can approve or adjust, and I‚Äôll send it right away.\n\n\n\nProposed email draft\n\n- Subject: Reminder: Review new mockups before Tue Feb 24, 2026 at 2:00 PM\n\n- Body:\n\n  Hello Design Team,\n\n  \n\n  This is a quick reminder to review the new mockups before the meeting at 2:00 PM on Tuesday, February 24, 2026. Please share any feedback or questions in advance so we can address them during the discussion.\n\n  \n\n  The latest mockups are available here: [location/link to mockups]\n\n  \n\n  Thank you,\n\n  [Your Name]\n\n  [Your Role/Team]\n\n  [Contact information]\n\n\n\nWhat I need from you to send this:\n\n- Recipient(s): Confirm using the design-team distribution list (design-team@example.com) or provide specific addresses.\n\n- Mockups link/location: Provide the URL or file path to include.\n\n- Your signature: Name, role, and contact info to put at the bottom (e.g., ‚ÄúAlex Smith, Product Designer, alex@example.com‚Äù).\n\n- Timezone: Is 2:00 PM in your local timezone? Should I specify a timezone in the body?\n\n\n\nWould you like me to proceed with the default recipients (design-team@example.com) and use the date shown above, once you provide the mockups link and signature? If you have different recipients or a preferred wording, tell me and I‚Äôll tailor it.\nThe run proceeds with our input."
  },
  {
    "objectID": "L01/subagents_hil.html#advanced-control-information-flow",
    "href": "L01/subagents_hil.html#advanced-control-information-flow",
    "title": "6. Add human-in-the-loop review",
    "section": "7. Advanced: Control information flow",
    "text": "7. Advanced: Control information flow\nBy default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.\n\n\n\nAdvanced: Control information flow\n\n\n\nPass additional conversational context to sub-agents\n\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef schedule_event(\n    request: str,\n    runtime: ToolRuntime # Runtime context automatically injected into tools.\n) -&gt; str:\n    \"\"\"Schedule calendar events using natural language.\"\"\"\n    # Customize context received by sub-agent\n    original_user_message = next(\n        message for message in runtime.state[\"messages\"]\n        if message.type == \"human\"\n    )\n    prompt = (\n        \"You are assisting with the following user inquiry:\\n\\n\"\n        f\"{original_user_message.text}\\n\\n\"\n        \"You are tasked with the following sub-request:\\n\\n\"\n        f\"{request}\"\n    )\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n    })\n    return result[\"messages\"][-1].text\n\nThis allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like ‚Äúschedule it for the same time tomorrow‚Äù (referencing a previous conversation).\n\n\n\n\n\n\nTip\n\n\n\nYou can see the full context received by the sub agent in the chat model call of the LangSmith trace.\n\n\n\n\nControl what supervisor receives\nYou can also customize what information flows back to the supervisor:\n\nimport json\n\n@tool\ndef schedule_event(request: str) -&gt; str:\n    \"\"\"Schedule calendar events using natural language.\"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n\n    # Option 1: Return just the confirmation message\n    return result[\"messages\"][-1].text\n\n    # Option 2: Return structured data\n    # return json.dumps({\n    #     \"status\": \"success\",\n    #     \"event_id\": \"evt_123\",\n    #     \"summary\": result[\"messages\"][-1].text\n    # })\n\nImportant: Make sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but don‚Äôt include the results in their final response."
  },
  {
    "objectID": "L01/use-cases.html",
    "href": "L01/use-cases.html",
    "title": "Use Cases",
    "section": "",
    "text": "Reference of Use Cases:"
  },
  {
    "objectID": "L01/02_subagents.html",
    "href": "L01/02_subagents.html",
    "title": "Build a personal assistant with subagents",
    "section": "",
    "text": "The supervisor pattern is a multi-agent architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.\nIn this tutorial, you‚Äôll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:\n\nA calendar agent that handles scheduling, availability checking, and event management.\nAn email agent that manages communication, drafts messages, and sends notifications.\n\n\n\nMulti-agent architectures allow you to partition tools across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).\n\n\n\n\n\n\nArchitecture\n\n\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.\nThis separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.\n\n\n\nWe will need to select a chat model from LangChain‚Äôs suite of integrations:\n\n\nüëâ Read the OpenAI chat model integration docs\nuv add \"langchain[openai]\"\n\nimport os\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# We use OpenRouter for the agent ‚Äî set OPENROUTER_API_KEY in .env\n# Get your key at https://openrouter.ai/keys\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    raise RuntimeError(\n        \"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:\\n\"\n        \"OPENROUTER_API_KEY=your-openrouter-api-key\"\n    )\n\n\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")"
  },
  {
    "objectID": "L01/02_subagents.html#overview",
    "href": "L01/02_subagents.html#overview",
    "title": "Build a personal assistant with subagents",
    "section": "",
    "text": "The supervisor pattern is a multi-agent architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.\nIn this tutorial, you‚Äôll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:\n\nA calendar agent that handles scheduling, availability checking, and event management.\nAn email agent that manages communication, drafts messages, and sends notifications.\n\n\n\nMulti-agent architectures allow you to partition tools across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).\n\n\n\n\n\n\nArchitecture\n\n\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.\nThis separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.\n\n\n\nWe will need to select a chat model from LangChain‚Äôs suite of integrations:\n\n\nüëâ Read the OpenAI chat model integration docs\nuv add \"langchain[openai]\"\n\nimport os\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# We use OpenRouter for the agent ‚Äî set OPENROUTER_API_KEY in .env\n# Get your key at https://openrouter.ai/keys\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    raise RuntimeError(\n        \"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:\\n\"\n        \"OPENROUTER_API_KEY=your-openrouter-api-key\"\n    )\n\n\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")"
  },
  {
    "objectID": "L01/02_subagents.html#define-tools",
    "href": "L01/02_subagents.html#define-tools",
    "title": "Build a personal assistant with subagents",
    "section": "1. Define tools",
    "text": "1. Define tools\n\nStart by defining the tools that require structured inputs.\nIn real applications, these would call actual APIs (Google Calendar, SendGrid, etc.).\nFor this tutorial, you‚Äôll use stubs to demonstrate the pattern.\n\n\nfrom langchain.tools import tool\n\n\n@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,       # ISO format: \"2024-01-15T14:00:00\"\n    end_time: str,         # ISO format: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # email addresses\n    location: str = \"\"\n) -&gt; str:\n    \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\n    # Stub: In practice, this would call Google Calendar API, Outlook API, etc.\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\n\n\n@tool\ndef send_email(\n    to: list[str],  # email addresses\n    subject: str,\n    body: str,\n    cc: list[str] = []\n) -&gt; str:\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\n    # Stub: In practice, this would call SendGrid, Gmail API, etc.\n    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO format: \"2024-01-15\"\n    duration_minutes: int\n) -&gt; list[str]:\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\n    # Stub: In practice, this would query calendar APIs\n    return [\"09:00\", \"14:00\", \"16:00\"]"
  },
  {
    "objectID": "L01/02_subagents.html#create-specialized-sub-agents",
    "href": "L01/02_subagents.html#create-specialized-sub-agents",
    "title": "Build a personal assistant with subagents",
    "section": "2. Create specialized sub-agents",
    "text": "2. Create specialized sub-agents\nNext, we‚Äôll create specialized sub-agents that handle each domain.\n\nCreate a calendar agent\n\nThe calendar agent understands natural language scheduling requests and translates them into precise API calls.\nIt handles date parsing, availability checking, and event creation.\n\n\n\nfrom langchain.agents import create_agent\n\nCALENDAR_AGENT_PROMPT = (\n    \"You are a calendar scheduling assistant. \"\n    \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \"\n    \"into proper ISO datetime formats. \"\n    \"Use get_available_time_slots to check availability when needed. \"\n    \"Use create_calendar_event to schedule events. \"\n    \"Always confirm what was scheduled in your final response.\"\n)\n\ncalendar_agent = create_agent(\n    model,\n    tools=[\n        create_calendar_event,\n        get_available_time_slots\n    ],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n)\n\nTest the calendar agent to see how it handles natural language scheduling:\n\nfrom langchain.messages import (\n    SystemMessage,\n    HumanMessage,\n    AIMessage\n)\n\n\nquery = \"Schedule a team meeting next Tuesday at 2pm for 1 hour\"\ninput_ = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n\nfor step in calendar_agent.stream(input_):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  create_calendar_event (call_VPij7Wlqm5MTct1VYogczdmM)\n\n Call ID: call_VPij7Wlqm5MTct1VYogczdmM\n\n  Args:\n\n    title: Team meeting\n\n    start_time: 2026-02-24T14:00:00\n\n    end_time: 2026-02-24T15:00:00\n\n    attendees: []\n\n    location:\n\n================================= Tool Message =================================\n\nName: create_calendar_event\n\n\n\nEvent created: Team meeting from 2026-02-24T14:00:00 to 2026-02-24T15:00:00 with 0 attendees\n\n================================== Ai Message ==================================\n\n\n\nAll set. I‚Äôve scheduled the Team meeting for Tuesday, February 24, 2026, from 2:00 PM to 3:00 PM (local time). Attendees: none. Location: none.\n\n\n\nWould you like me to add attendees and/or a location (or conferencing details) and send invites?\n\n\n\n\nThe agent parses ‚Äúnext Tuesday at 2pm‚Äù into ISO format (‚Äú2024-01-16T14:00:00‚Äù), calculates the end time, calls create_calendar_event, and returns a natural language confirmation.\n\n\nCreate an email agent\n\nThe email agent handles message composition and sending.\nIt focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication.\n\n\nEMAIL_AGENT_PROMPT = (\n    \"You are an email assistant. \"\n    \"Compose professional emails based on natural language requests. \"\n    \"Extract recipient information and craft appropriate subject lines and body text. \"\n    \"Use send_email to send the message. \"\n    \"Always confirm what was sent in your final response.\"\n)\n\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n)\n\nTest the email agent with a natural language request:\n\nquery = \"Send the design team a reminder about reviewing the new mockups\"\ninput_ = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n\nfor step in email_agent.stream(input_):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\n\n================================== Ai Message ==================================\n\n\n\nHere‚Äôs a ready-to-send draft. I‚Äôve assumed a Design Team distribution list. If you want a different recipient, let me know.\n\n\n\nRecipients: design-team@yourcompany.com\n\nSubject: Reminder: Please review the new mockups\n\nBody:\n\nHi Design Team,\n\n\n\nThis is a friendly reminder to review the new mockups for the [Project/Feature]. Your feedback is important to ensure we stay aligned with requirements and timelines.\n\n\n\nPlease share your comments by [Due Date] or let me know if you need more time. You can access the mockups here: [Link to mockups].\n\n\n\nIf you‚Äôve already provided feedback, thank you and please disregard this message.\n\n\n\nBest regards,\n\n[Your Name]\n\n[Your Title]\n\n[Department]\n\n\n\nWould you like me to send this now as-is, or would you like me to replace the placeholders with the actual project name, due date, link, and your signature? If you confirm, I‚Äôll send it.\n\n\n\n\n\nThe agent infers the recipient from the informal request, crafts a professional subject line and body, calls send_email, and returns a confirmation.\nEach sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task."
  },
  {
    "objectID": "L01/02_subagents.html#wrap-sub-agents-as-tools",
    "href": "L01/02_subagents.html#wrap-sub-agents-as-tools",
    "title": "Build a personal assistant with subagents",
    "section": "3. Wrap sub-agents as tools",
    "text": "3. Wrap sub-agents as tools\n\nNow wrap each sub-agent as a tool that the supervisor can invoke.\nThis is the key architectural step that creates the layered system.\nThe supervisor will see high-level tools like \"schedule_event\", not low-level tools like \"create_calendar_event\".\n\n\n@tool\ndef schedule_event(request: str) -&gt; str:\n    \"\"\"Schedule calendar events using natural language.\n\n    Use this when the user wants to create, modify, or check calendar appointments.\n    Handles date/time parsing, availability checking, and event creation.\n\n    Input: Natural language scheduling request (e.g., 'meeting with design team\n    next Tuesday at 2pm')\n    \"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\n\n@tool\ndef manage_email(request: str) -&gt; str:\n    \"\"\"Send emails using natural language.\n\n    Use this when the user wants to send notifications, reminders, or any email\n    communication. Handles recipient extraction, subject generation, and email\n    composition.\n\n    Input: Natural language email request (e.g., 'send them a reminder about\n    the meeting')\n    \"\"\"\n    result = email_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\n\nThe tool descriptions (docstring) help the supervisor decide when to use each tool, so make them clear and specific.\nWe return only the sub-agent‚Äôs final response, as the supervisor doesn‚Äôt need to see intermediate reasoning or tool calls."
  },
  {
    "objectID": "L01/02_subagents.html#create-the-supervisor-agent",
    "href": "L01/02_subagents.html#create-the-supervisor-agent",
    "title": "Build a personal assistant with subagents",
    "section": "4. Create the supervisor agent",
    "text": "4. Create the supervisor agent\n\nNow create the supervisor that orchestrates the sub-agents.\nThe supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level.\n\n\nSUPERVISOR_PROMPT = (\n    \"You are a helpful personal assistant. \"\n    \"You can schedule calendar events and send emails. \"\n    \"Break down user requests into appropriate tool calls and coordinate the results. \"\n    \"When a request involves multiple actions, use multiple tools in sequence.\"\n)\n\nsupervisor_agent = create_agent(\n    model,\n    tools=[\n        schedule_event,\n        manage_email\n    ],\n    system_prompt=SUPERVISOR_PROMPT,\n)"
  },
  {
    "objectID": "L01/02_subagents.html#use-the-supervisor",
    "href": "L01/02_subagents.html#use-the-supervisor",
    "title": "Build a personal assistant with subagents",
    "section": "5. Use the supervisor",
    "text": "5. Use the supervisor\nNow test your complete system with complex requests that require coordination across multiple domains:\n\nExample 1: Simple single-domain request\n\nquery = \"Schedule a team standup for tomorrow at 9am\"\ninput_ = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n\nfor step in supervisor_agent.stream(input_):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  schedule_event (call_gtoo8MGcOP1cOyGS8hm1rkRM)\n\n Call ID: call_gtoo8MGcOP1cOyGS8hm1rkRM\n\n  Args:\n\n    request: Schedule a team standup tomorrow at 9am\n\n================================= Tool Message =================================\n\nName: schedule_event\n\n\n\nI can schedule that. A couple of details to confirm:\n\n\n\n- Date: Tomorrow is 2026-02-19. Is that correct?\n\n- Duration: What duration should I use? (default 15 minutes)\n\n- Attendees: Who should attend the standup? (e.g., list of names, or simply \"Team\")\n\n- Location/ conferencing: Should this be virtual (and which platform) or in-person? If virtual, do you want me to create a meeting link?\n\n- Timezone: Should I use your calendar‚Äôs default timezone, or specify a different one?\n\n\n\nIf you‚Äôd like, I can proceed with defaults (9:00‚Äì9:15 local time, attendees: Team, virtual meeting link to be determined) once you confirm.\n\n================================== Ai Message ==================================\n\n\n\nI can schedule that. I can proceed with defaults unless you want changes.\n\n\n\nProposed defaults:\n\n- Date/time: Tomorrow, 2026-02-19, 9:00‚Äì9:15 local time\n\n- Attendees: Team\n\n- Location: Virtual meeting link (link will be created)\n\n- Timezone: Your calendar‚Äôs default\n\n\n\nPlease confirm or tell me the adjustments (e.g., different duration, specific attendees, in-person vs virtual, or a preferred conferencing platform). If you‚Äôre okay with the defaults, I‚Äôll schedule it right away.\n\n\n\n\nThe supervisor identifies this as a calendar task, calls schedule_event, and the calendar agent handles date parsing and event creation.\n\n\n\n\n\n\nTip\n\n\n\nFor full transparency into the information flow, including prompts and responses for each chat model call, check out the LangSmith trace for the above run.\n\n\n\n\nExample 2: Complex multi-domain request\n\nquery = (\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n    \"and send them an email reminder about reviewing the new mockups.\"\n)\ninput_ = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n\nfor step in supervisor_agent.stream(input_):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\n\nThe supervisor recognizes this requires both calendar and email actions, calls schedule_event for the meeting, then calls manage_email for the reminder.\nEach sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.\n\n\n\n\n\n\n\nTip\n\n\n\nRefer to the LangSmith trace to see the detailed information flow for the above run, including individual chat model prompts and responses."
  },
  {
    "objectID": "L01/02_subagents.html#key-takeaways",
    "href": "L01/02_subagents.html#key-takeaways",
    "title": "Build a personal assistant with subagents",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nThe supervisor pattern creates layers of abstraction where each layer has a clear responsibility.\nWhen designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts.\nWrite clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.\n\nUse the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents don‚Äôt need to converse directly with users.\nFor simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use handoffs instead. For peer-to-peer collaboration between agents, consider other multi-agent patterns."
  },
  {
    "objectID": "L01/02_subagents.html#activity",
    "href": "L01/02_subagents.html#activity",
    "title": "Build a personal assistant with subagents",
    "section": "Activity",
    "text": "Activity\nOver to you: recreate the supervisor pattern on a different problem domain (other than calendar & emails). You may stub API calls or, better yet, use actual ones!"
  },
  {
    "objectID": "L01/xx_embeddings.html",
    "href": "L01/xx_embeddings.html",
    "title": "Embeddings",
    "section": "",
    "text": "Let‚Äôs start with the fundamental question: what are embeddings?\nEmbeddings are commonly used for:\nSee: Embeddings | OpenRouter and Guides &gt; Embeddings | OpenAI"
  },
  {
    "objectID": "L01/xx_embeddings.html#setup",
    "href": "L01/xx_embeddings.html#setup",
    "title": "Embeddings",
    "section": "Setup",
    "text": "Setup\nFirst, we‚Äôll install the necessary libraries.\npip install -U -q \"openai\" pandas seaborn matplotlib\n\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom openai import OpenAI\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 3\n      1 import os\n      2 import pandas as pd\n----&gt; 3 import seaborn as sns\n      4 import matplotlib.pyplot as plt\n      5 from openai import OpenAI\n\nModuleNotFoundError: No module named 'seaborn'\n\n\n\n\nSet up your API key\nEnsure your OPENROUTER_API_KEY is set in your environment variables (e.g.¬†in a .env file).\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\n\nif not OPENROUTER_API_KEY:\n    raise ValueError(\n        \"Please set the OPENROUTER_API_KEY environment variable. \"\n        \"Get your key at https://openrouter.ai/keys\"\n    )\n\n# Use OpenAI client with OpenRouter's base URL (OpenAI-compatible API)\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=OPENROUTER_API_KEY,\n)"
  },
  {
    "objectID": "L01/xx_embeddings.html#calculate-similarity-scores",
    "href": "L01/xx_embeddings.html#calculate-similarity-scores",
    "title": "Embeddings",
    "section": "Calculate similarity scores",
    "text": "Calculate similarity scores\nThis example embeds some variations on the pangram, The quick brown fox jumps over the lazy dog, including spelling mistakes and shortenings of the phrase. Another pangram and a somewhat unrelated phrase have been included for comparison.\nOpenRouter supports multiple embedding models; we use openai/text-embedding-3-small (same as OpenAI‚Äôs) for calculating similarity scores between texts.\n\ntexts = [\n    'The quick brown fox jumps over the lazy dog.',\n    'The quick rbown fox jumps over the lazy dog.',\n    'teh fast fox jumps over the slow woofer.',\n    'a quick brown fox jmps over lazy dog.',\n    'brown fox jumping over dog',\n    'fox &gt; dog',\n    # Alternative pangram for comparison:\n    'The five boxing wizards jump quickly.',\n    # Unrelated text, also for comparison:\n    'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus et hendrerit massa. Sed pulvinar, nisi a lobortis sagittis, neque risus gravida dolor, in porta dui odio vel purus.',\n]\n\n# OpenRouter model ID for OpenAI's text-embedding-3-small\nresponse = client.embeddings.create(\n    model=\"openai/text-embedding-3-small\",\n    input=texts,\n)\n\nDefine a short helper function that will make it easier to display longer embedding texts in our visualisation.\n\ndef truncate(t: str, limit: int = 50) -&gt; str:\n    \"\"\"Truncate labels to fit on the chart.\"\"\"\n    if len(t) &gt; limit:\n        return t[:limit-3] + '...'\n    else:\n        return t\n\ntruncated_texts = [truncate(t) for t in texts]\n\nA similarity score of two embedding vectors can be obtained by calculating their inner product. If \\(\\mathbf{u}\\) is the first embedding vector, and \\(\\\\mathbf{v}\\) the second, this is \\(\\mathbf{u}^T \\\\mathbf{v}\\). As the API provides embedding vectors that are normalised to unit length, this is also the cosine similarity.\nThis score can be computed across all embeddings through the matrix self-multiplication: df @ df.T.\nNote that the range from 0.0 (completely dissimilar) to 1.0 (completely similar) is depicted in the heatmap from light (0.0) to dark (1.0).\n\n# Set up the embeddings in a dataframe.\ndf = pd.DataFrame([e.embedding for e in response.data], index=truncated_texts)\n\n# Perform the similarity calculation\nsim = df @ df.T\n\n# Draw!\nplt.figure(figsize=(10, 8))\nsns.heatmap(sim, vmin=0, vmax=1, cmap=\"Greens\", annot=True)\nplt.title(\"Semantic Similarity Heatmap\")\nplt.show()\n\nYou can see the scores for a particular term directly by looking it up in the dataframe.\n\nprint(sim['The quick brown fox jumps over the lazy dog.'].sort_values(ascending=False))"
  },
  {
    "objectID": "L01/xx_embeddings.html#further-reading",
    "href": "L01/xx_embeddings.html#further-reading",
    "title": "Embeddings",
    "section": "Further reading",
    "text": "Further reading\n\nOpenRouter Embeddings API and available embedding models\nOpenAI embeddings documentation and similarity search use cases"
  },
  {
    "objectID": "L01/05_RAG_agent.html",
    "href": "L01/05_RAG_agent.html",
    "title": "Build a RAG agent with LangChain",
    "section": "",
    "text": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\n\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n\n\nWe will cover the following concepts:\n\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\n\n\n\n\n\n\nNote\n\n\n\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.\nIf your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\n\n\n\n\n\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nimport bs4\nfrom langchain.agents import AgentState, create_agent\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.messages import MessageLikeRepresentation\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load and chunk contents of the blog\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nall_splits = text_splitter.split_documents(docs)\n\n# Index chunks\n_ = vector_store.add_documents(documents=all_splits)\n\n# Construct a tool for retrieving context\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n\ntools = [retrieve_context]\n# If desired, specify custom instructions\nprompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\nCheck out the LangSmith trace."
  },
  {
    "objectID": "L01/05_RAG_agent.html#overview",
    "href": "L01/05_RAG_agent.html#overview",
    "title": "Build a RAG agent with LangChain",
    "section": "",
    "text": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\n\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n\n\nWe will cover the following concepts:\n\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\n\n\n\n\n\n\nNote\n\n\n\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.\nIf your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\n\n\n\n\n\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nimport bs4\nfrom langchain.agents import AgentState, create_agent\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.messages import MessageLikeRepresentation\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load and chunk contents of the blog\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nall_splits = text_splitter.split_documents(docs)\n\n# Index chunks\n_ = vector_store.add_documents(documents=all_splits)\n\n# Construct a tool for retrieving context\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n\ntools = [retrieve_context]\n# If desired, specify custom instructions\nprompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\nCheck out the LangSmith trace."
  },
  {
    "objectID": "L01/05_RAG_agent.html#setup",
    "href": "L01/05_RAG_agent.html#setup",
    "title": "Build a RAG agent with LangChain",
    "section": "Setup",
    "text": "Setup\n\nInstallation\nThis tutorial requires these langchain dependencies:\nuv add langchain langchain-text-splitters langchain-community bs4\nFor more details, see our Installation guide.\n\n\nComponents\nWe will need to select three components from LangChain‚Äôs suite of integrations.\n\n1. Select a chat model\nüëâ Read the OpenAI chat model integration docs\nuv add \"langchain[openai]\"\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# We use OpenRouter for the agent ‚Äî set OPENROUTER_API_KEY in .env\n# Get your key at https://openrouter.ai/keys\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    raise RuntimeError(\n        \"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:\\n\"\n        \"OPENROUTER_API_KEY=your-openrouter-api-key\"\n    )\n\n\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\n2. Select an embeddings model\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n\n\n3. Select a vector store\nuv add \"langchain-core\"\n\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nvector_store = InMemoryVectorStore(embeddings)"
  },
  {
    "objectID": "L01/05_RAG_agent.html#indexing",
    "href": "L01/05_RAG_agent.html#indexing",
    "title": "Build a RAG agent with LangChain",
    "section": "1. Indexing",
    "text": "1. Indexing\n\n\n\n\n\n\nNote\n\n\n\nThis section is an abbreviated version of the content in the semantic search tutorial.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you‚Äôre comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\n\n\nIndexing commonly works as follows:\n\nLoad: First we need to load our data. This is done with Document Loaders.\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n\n\n\n\nIndex Diagram\n\n\n\nLoading documents\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\nIn this case we‚Äôll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -&gt; text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\n\nimport bs4\nfrom langchain_community.document_loaders import WebBaseLoader\n\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs={\"parse_only\": bs4_strainer},\n)\ndocs = loader.load()\n\nassert len(docs) == 1\nprint(f\"Total characters: {len(docs[0].page_content)}\")\n\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\n\n\nTotal characters: 43047\n\n\n\nprint(docs[0].page_content[:500])\n\n\n\n      LLM Powered Autonomous Agents\n    \nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n\n\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\n\n\nGo deeper\nDocumentLoader: Object that loads data from a source as list of Documents.\n\nIntegrations: 160+ integrations to choose from.\nBaseLoader: API reference for the base interface.\n\n\n\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # chunk size (characters)\n    chunk_overlap=200,  # chunk overlap (characters)\n    add_start_index=True,  # track index in original document\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\n\nSplit blog post into 63 sub-documents.\n\n\nGo deeper\nTextSplitter: Object that splits a list of Document objects into smaller chunks for storage and retrieval.\n\nIntegrations\nInterface: API reference for the base interface.\n\n\n\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\n\ndocument_ids = vector_store.add_documents(documents=all_splits)\n\nprint(document_ids[:3])\n\n['7140256e-d3e0-4174-8abe-6941b0698ff3', 'b8079c9c-1552-4add-b4d0-c3bc454e258f', '5df27ff9-ff55-4f6c-8642-931608b5a21c']\n\n\nGo deeper\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\n\nIntegrations: 30+ integrations to choose from.\nInterface: API reference for the base interface.\n\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\n\nIntegrations: 40+ integrations to choose from.\nInterface: API reference for the base interface.\n\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question."
  },
  {
    "objectID": "L01/05_RAG_agent.html#retrieval-and-generation",
    "href": "L01/05_RAG_agent.html#retrieval-and-generation",
    "title": "Build a RAG agent with LangChain",
    "section": "2. Retrieval and generation",
    "text": "2. Retrieval and generation\nRAG applications commonly work as follows:\n\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\n\n\n\n\nRetrieval Diagram\n\n\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\n\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n\nRAG agents\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\n\nfrom langchain.tools import tool\n\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n\n\n\n\n\n\n\nTip\n\n\n\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n\n\n\n\n\n\n\n\nTip\n\n\n\nRetrieval tools are not limited to a single string query argument, as in the above example. You can force the LLM to specify additional search parameters by adding arguments‚Äî for example, a category:\nfrom typing import Literal\n\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\n\n\nGiven our tool, we can construct the agent:\n\nfrom langchain.agents import create_agent\n\ntools = [retrieve_context]\n# If desired, specify custom instructions\nprompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)\n\nLet‚Äôs test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\n\nquery = (\n    \"What is the standard method for Task Decomposition?\\n\\n\"\n    \"Once you get the answer, look up common extensions of that method.\"\n)\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    event[\"messages\"][-1].pretty_print()\n\n\n================================ Human Message =================================\n\n\n\nWhat is the standard method for Task Decomposition?\n\n\n\nOnce you get the answer, look up common extensions of that method.\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  retrieve_context (call_5ekkYX8vMgmb4Rzv4p2jFdAl)\n\n Call ID: call_5ekkYX8vMgmb4Rzv4p2jFdAl\n\n  Args:\n\n    query: standard method for Task Decomposition\n\n================================= Tool Message =================================\n\nName: retrieve_context\n\n\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n\nContent: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into ‚ÄúProblem PDDL‚Äù, then (2) requests a classical planner to generate a PDDL plan based on an existing ‚ÄúDomain PDDL‚Äù, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n\nSelf-Reflection#\n\n\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n\nContent: Component One: Planning#\n\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n\nTask Decomposition#\n\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\n\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  retrieve_context (call_p3OgKqTVTa0P51eT1Rlzy48U)\n\n Call ID: call_p3OgKqTVTa0P51eT1Rlzy48U\n\n  Args:\n\n    query: extensions of standard task decomposition methods\n\n================================= Tool Message =================================\n\nName: retrieve_context\n\n\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n\nContent: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into ‚ÄúProblem PDDL‚Äù, then (2) requests a classical planner to generate a PDDL plan based on an existing ‚ÄúDomain PDDL‚Äù, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n\nSelf-Reflection#\n\n\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n\nContent: Component One: Planning#\n\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n\nTask Decomposition#\n\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\n\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n\n================================== Ai Message ==================================\n\n\n\nThe standard method for Task Decomposition is the Chain of Thought (CoT) prompting technique. In CoT, the model is instructed to ‚Äúthink step by step‚Äù to break down complex tasks into smaller and simpler steps, aiding both task execution and interpretability of the reasoning process.\n\n\n\nCommon extensions of Chain of Thought include:\n\n\n\n1. Tree of Thoughts (ToT): This method extends CoT by exploring multiple reasoning possibilities at each step. The problem is decomposed into multiple ‚Äúthought steps,‚Äù and several alternative thoughts are generated for each, forming a tree structure. Search strategies like breadth-first search (BFS) or depth-first search (DFS) are used, with states evaluated using classifiers or majority voting.\n\n\n\n2. LLM + Planner (LLM+P): In this extension, long-horizon planning is handled by an external classical planner. The LLM first generates a problem description in Planning Domain Definition Language (PDDL), which a planner uses to generate a solution plan, then translates the plan back into natural language. This approach leverages domain-specific planning outside of the LLM.\n\n\n\nThese extensions allow for greater flexibility, systematic reasoning, and usage of external tools for sophisticated planning beyond the basic step-by-step CoT method.\n\n\n\n\nNote that the agent:\n\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\n\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n\n\n\n\n\n\nTip\n\n\n\nYou can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph‚Äôs Agentic RAG tutorial for more advanced formulations."
  },
  {
    "objectID": "L01/05_RAG_agent.html#overview-1",
    "href": "L01/05_RAG_agent.html#overview-1",
    "title": "Build a RAG agent with LangChain",
    "section": "Overview",
    "text": "Overview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\n\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n\nConcepts\nWe will cover the following concepts:\n\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\n\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.\nIf your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\n\n\n\nPreview\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\n ```python theme={null} import bs4 from langchain.agents import AgentState, create_agent from langchain_community.document_loaders import WebBaseLoader from langchain.messages import MessageLikeRepresentation from langchain_text_splitters import RecursiveCharacterTextSplitter\n# Load and chunk contents of the blog loader = WebBaseLoader( web_paths=(‚Äúhttps://lilianweng.github.io/posts/2023-06-23-agent/‚Äù,), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, ‚Äúpost-header‚Äù) ) ), ) docs = loader.load()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) all_splits = text_splitter.split_documents(docs)\n# Index chunks _ = vector_store.add_documents(documents=all_splits)\n# Construct a tool for retrieving context @tool(response_format=‚Äúcontent_and_artifact‚Äù) def retrieve_context(query: str): ‚Äú‚Äú‚ÄúRetrieve information to help answer a query.‚Äù‚Äú‚Äù retrieved_docs = vector_store.similarity_search(query, k=2) serialized = ‚Äú‚Äù.join( (f‚ÄùSource: {doc.metadata}: {doc.page_content}‚Äú) for doc in retrieved_docs ) return serialized, retrieved_docs\ntools = [retrieve_context] # If desired, specify custom instructions prompt = ( ‚ÄúYou have access to a tool that retrieves context from a blog post.‚Äù ‚ÄúUse the tool to help answer user queries.‚Äù ) agent = create_agent(model, tools, system_prompt=prompt) ```\npython  theme={null}   query = \"What is task decomposition?\"   for step in agent.stream(       {\"messages\": [{\"role\": \"user\", \"content\": query}]},       stream_mode=\"values\",   ):       step[\"messages\"][-1].pretty_print()\n================================ Human Message =================================\n\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\n  Args:\n    query: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\n\nTask decomposition refers to...\nCheck out the LangSmith trace."
  },
  {
    "objectID": "L01/05_RAG_agent.html#setup-1",
    "href": "L01/05_RAG_agent.html#setup-1",
    "title": "Build a RAG agent with LangChain",
    "section": "Setup",
    "text": "Setup\n\nInstallation\nThis tutorial requires these langchain dependencies:\n bash pip theme={null}   pip install langchain langchain-text-splitters langchain-community bs4\nbash uv theme={null}   uv add langchain langchain-text-splitters langchain-community bs4 \nFor more details, see our Installation guide.\n\n\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nshell  theme={null} export LANGSMITH_TRACING=\"true\" export LANGSMITH_API_KEY=\"...\"\nOr, set them in Python:\n```python theme={null} import getpass import os\nos.environ[‚ÄúLANGSMITH_TRACING‚Äù] = ‚Äútrue‚Äù os.environ[‚ÄúLANGSMITH_API_KEY‚Äù] = getpass.getpass()\n\n### Components\n\nWe will need to select three components from LangChain's suite of integrations.\n\nSelect a chat model:\n\n&lt;Tabs&gt;\n  &lt;Tab title=\"OpenAI\"&gt;\n    üëâ Read the [OpenAI chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"gpt-4.1\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import ChatOpenAI\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = ChatOpenAI(model=\"gpt-4.1\")\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Anthropic\"&gt;\n    üëâ Read the [Anthropic chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/anthropic/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[anthropic]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_anthropic import ChatAnthropic\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Azure\"&gt;\n    üëâ Read the [Azure chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/azure_chat_openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = init_chat_model(\n          \"azure_openai:gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import AzureChatOpenAI\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = AzureChatOpenAI(\n          model=\"gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n      )\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Google Gemini\"&gt;\n    üëâ Read the [Google GenAI chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[google-genai]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_google_genai import ChatGoogleGenerativeAI\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"AWS Bedrock\"&gt;\n    üëâ Read the [AWS Bedrock chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/bedrock/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[aws]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      from langchain.chat_models import init_chat_model\n\n      # Follow the steps here to configure your credentials:\n      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      model = init_chat_model(\n          \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n          model_provider=\"bedrock_converse\",\n      )\n      ```\n\n      ```python Model Class theme={null}\n      from langchain_aws import ChatBedrock\n\n      model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"HuggingFace\"&gt;\n    üëâ Read the [HuggingFace chat model integration docs](https://docs.langchain.com/oss/python/integrations/chat/huggingface/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[huggingface]\"\n    ```\n\n    &lt;CodeGroup&gt;\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n      model = init_chat_model(\n          \"microsoft/Phi-3-mini-4k-instruct\",\n          model_provider=\"huggingface\",\n          temperature=0.7,\n          max_tokens=1024,\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\n      os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n      llm = HuggingFaceEndpoint(\n          repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n          temperature=0.7,\n          max_length=1024,\n      )\n      model = ChatHuggingFace(llm=llm)\n      ```\n    &lt;/CodeGroup&gt;\n  &lt;/Tab&gt;\n&lt;/Tabs&gt;\n\nSelect an embeddings model:\n\n&lt;Tabs&gt;\n  &lt;Tab title=\"OpenAI\"&gt;\n    ```shell  theme={null}\n    pip install -U \"langchain-openai\"\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"OPENAI_API_KEY\"):\n        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n\n    from langchain_openai import OpenAIEmbeddings\n\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Azure\"&gt;\n    ```shell  theme={null}\n    pip install -U \"langchain-openai\"\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n        os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\n\n    from langchain_openai import AzureOpenAIEmbeddings\n\n    embeddings = AzureOpenAIEmbeddings(\n        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n        azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n        openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Google Gemini\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-google-genai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"GOOGLE_API_KEY\"):\n        os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n\n    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Google Vertex\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-google-vertexai\n    ```\n\n    ```python  theme={null}\n    from langchain_google_vertexai import VertexAIEmbeddings\n\n    embeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"AWS\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-aws\n    ```\n\n    ```python  theme={null}\n    from langchain_aws import BedrockEmbeddings\n\n    embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"HuggingFace\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-huggingface\n    ```\n\n    ```python  theme={null}\n    from langchain_huggingface import HuggingFaceEmbeddings\n\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Ollama\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-ollama\n    ```\n\n    ```python  theme={null}\n    from langchain_ollama import OllamaEmbeddings\n\n    embeddings = OllamaEmbeddings(model=\"llama3\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Cohere\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-cohere\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"COHERE_API_KEY\"):\n        os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\n\n    from langchain_cohere import CohereEmbeddings\n\n    embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"MistralAI\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-mistralai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"MISTRALAI_API_KEY\"):\n        os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\n\n    from langchain_mistralai import MistralAIEmbeddings\n\n    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Nomic\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-nomic\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"NOMIC_API_KEY\"):\n        os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\n\n    from langchain_nomic import NomicEmbeddings\n\n    embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"NVIDIA\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-nvidia-ai-endpoints\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"NVIDIA_API_KEY\"):\n        os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\n\n    from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n\n    embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Voyage AI\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-voyageai\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"VOYAGE_API_KEY\"):\n        os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\n\n    from langchain-voyageai import VoyageAIEmbeddings\n\n    embeddings = VoyageAIEmbeddings(model=\"voyage-3\")\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"IBM watsonx\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-ibm\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"WATSONX_APIKEY\"):\n        os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\n\n    from langchain_ibm import WatsonxEmbeddings\n\n    embeddings = WatsonxEmbeddings(\n        model_id=\"ibm/slate-125m-english-rtrvr\",\n        url=\"https://us-south.ml.cloud.ibm.com\",\n        project_id=\"&lt;WATSONX PROJECT_ID&gt;\",\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Fake\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-core\n    ```\n\n    ```python  theme={null}\n    from langchain_core.embeddings import DeterministicFakeEmbedding\n\n    embeddings = DeterministicFakeEmbedding(size=4096)\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Isaacus\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-isaacus\n    ```\n\n    ```python  theme={null}\n    import getpass\n    import os\n\n    if not os.environ.get(\"ISAACUS_API_KEY\"):\n    os.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\n\n    from langchain_isaacus import IsaacusEmbeddings\n\n    embeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\n    ```\n  &lt;/Tab&gt;\n&lt;/Tabs&gt;\n\nSelect a vector store:\n\n&lt;Tabs&gt;\n  &lt;Tab title=\"In-memory\"&gt;\n    ```shell  theme={null}\n    pip install -U \"langchain-core\"\n    ```\n\n    ```python  theme={null}\n    from langchain_core.vectorstores import InMemoryVectorStore\n\n    vector_store = InMemoryVectorStore(embeddings)\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Amazon OpenSearch\"&gt;\n    ```shell  theme={null}\n    pip install -qU  boto3\n    ```\n\n    ```python  theme={null}\n    from opensearchpy import RequestsHttpConnection\n\n    service = \"es\"  # must set the service as 'es'\n    region = \"us-east-2\"\n    credentials = boto3.Session(\n        aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\n    ).get_credentials()\n    awsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\n\n    vector_store = OpenSearchVectorSearch.from_documents(\n        docs,\n        embeddings,\n        opensearch_url=\"host url\",\n        http_auth=awsauth,\n        timeout=300,\n        use_ssl=True,\n        verify_certs=True,\n        connection_class=RequestsHttpConnection,\n        index_name=\"test-index\",\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"AstraDB\"&gt;\n    ```shell  theme={null}\n    pip install -U \"langchain-astradb\"\n    ```\n\n    ```python  theme={null}\n    from langchain_astradb import AstraDBVectorStore\n\n    vector_store = AstraDBVectorStore(\n        embedding=embeddings,\n        api_endpoint=ASTRA_DB_API_ENDPOINT,\n        collection_name=\"astra_vector_langchain\",\n        token=ASTRA_DB_APPLICATION_TOKEN,\n        namespace=ASTRA_DB_NAMESPACE,\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Chroma\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-chroma\n    ```\n\n    ```python  theme={null}\n    from langchain_chroma import Chroma\n\n    vector_store = Chroma(\n        collection_name=\"example_collection\",\n        embedding_function=embeddings,\n        persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"FAISS\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-community faiss-cpu\n    ```\n\n    ```python  theme={null}\n    import faiss\n    from langchain_community.docstore.in_memory import InMemoryDocstore\n    from langchain_community.vectorstores import FAISS\n\n    embedding_dim = len(embeddings.embed_query(\"hello world\"))\n    index = faiss.IndexFlatL2(embedding_dim)\n\n    vector_store = FAISS(\n        embedding_function=embeddings,\n        index=index,\n        docstore=InMemoryDocstore(),\n        index_to_docstore_id={},\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Milvus\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-milvus\n    ```\n\n    ```python  theme={null}\n    from langchain_milvus import Milvus\n\n    URI = \"./milvus_example.db\"\n\n    vector_store = Milvus(\n        embedding_function=embeddings,\n        connection_args={\"uri\": URI},\n        index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"MongoDB\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-mongodb\n    ```\n\n    ```python  theme={null}\n    from langchain_mongodb import MongoDBAtlasVectorSearch\n\n    vector_store = MongoDBAtlasVectorSearch(\n        embedding=embeddings,\n        collection=MONGODB_COLLECTION,\n        index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n        relevance_score_fn=\"cosine\",\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"PGVector\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-postgres\n    ```\n\n    ```python  theme={null}\n    from langchain_postgres import PGVector\n\n    vector_store = PGVector(\n        embeddings=embeddings,\n        collection_name=\"my_docs\",\n        connection=\"postgresql+psycopg://...\",\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"PGVectorStore\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-postgres\n    ```\n\n    ```python  theme={null}\n    from langchain_postgres import PGEngine, PGVectorStore\n\n    pg_engine = PGEngine.from_connection_string(\n        url=\"postgresql+psycopg://...\"\n    )\n\n    vector_store = PGVectorStore.create_sync(\n        engine=pg_engine,\n        table_name='test_table',\n        embedding_service=embeddings\n    )\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Pinecone\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-pinecone\n    ```\n\n    ```python  theme={null}\n    from langchain_pinecone import PineconeVectorStore\n    from pinecone import Pinecone\n\n    pc = Pinecone(api_key=...)\n    index = pc.Index(index_name)\n\n    vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n    ```\n  &lt;/Tab&gt;\n\n  &lt;Tab title=\"Qdrant\"&gt;\n    ```shell  theme={null}\n    pip install -qU langchain-qdrant\n    ```\n\n    ```python  theme={null}\n    from qdrant_client.models import Distance, VectorParams\n    from langchain_qdrant import QdrantVectorStore\n    from qdrant_client import QdrantClient\n\n    client = QdrantClient(\":memory:\")\n\n    vector_size = len(embeddings.embed_query(\"sample text\"))\n\n    if not client.collection_exists(\"test\"):\n        client.create_collection(\n            collection_name=\"test\",\n            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n        )\n    vector_store = QdrantVectorStore(\n        client=client,\n        collection_name=\"test\",\n        embedding=embeddings,\n    )\n    ```\n  &lt;/Tab&gt;\n&lt;/Tabs&gt;\n\n## 1. Indexing\n\n&lt;Note&gt;\n  **This section is an abbreviated version of the content in the [semantic search tutorial](https://docs.langchain.com/oss/python/langchain/knowledge-base).**\n\n  If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you're comfortable with [document loaders](https://docs.langchain.com/oss/python/langchain/retrieval#document_loaders), [embeddings](https://docs.langchain.com/oss/python/langchain/retrieval#embedding_models), and [vector stores](https://docs.langchain.com/oss/python/langchain/retrieval#vectorstores), feel free to skip to the next section on [retrieval and generation](https://docs.langchain.com/oss/python/langchain/rag#2-retrieval-and-generation).\n&lt;/Note&gt;\n\nIndexing commonly works as follows:\n\n1. **Load**: First we need to load our data. This is done with [Document Loaders](https://docs.langchain.com/oss/python/langchain/retrieval#document_loaders).\n2. **Split**: [Text splitters](https://docs.langchain.com/oss/python/langchain/retrieval#text_splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](https://docs.langchain.com/oss/python/langchain/retrieval#vectorstores) and [Embeddings](https://docs.langchain.com/oss/python/langchain/retrieval#embedding_models) model.\n\n&lt;img src=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=21403ce0d0c772da84dcc5b75cff4451\" alt=\"index_diagram\" data-og-width=\"2583\" width=\"2583\" data-og-height=\"1299\" height=\"1299\" data-path=\"images/rag_indexing.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=bf4eb8255b82a809dbbd2bc2a96d2ed7 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4ebc538b2c4765b609f416025e4dbbda 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=1838328a870c7353c42bf1cc2290a779 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=675f55e100bab5e2904d27db01775ccc 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4b9e544a7a3ec168651558bce854eb60 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=f5aeaaaea103128f374c03b05a317263 2500w\" /&gt;\n\n### Loading documents\n\nWe need to first load the blog post contents. We can use [DocumentLoaders](https://docs.langchain.com/oss/python/langchain/retrieval#document_loaders) for this, which are objects that load in data from a source and return a list of [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects.\n\nIn this case we'll use the [`WebBaseLoader`](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base), which uses `urllib` to load HTML from web URLs and `BeautifulSoup` to parse it to text. We can customize the HTML -&gt; text parsing by passing in parameters into the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we'll remove all others.\n\n```python  theme={null}\nimport bs4\nfrom langchain_community.document_loaders import WebBaseLoader\n\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs={\"parse_only\": bs4_strainer},\n)\ndocs = loader.load()\n\nassert len(docs) == 1\nprint(f\"Total characters: {len(docs[0].page_content)}\")\ntext  theme={null} Total characters: 43131\npython  theme={null} print(docs[0].page_content[:500])\n```text theme={null} LLM Powered Autonomous Agents\nDate: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In\n\n**Go deeper**\n\n`DocumentLoader`: Object that loads data from a source as list of `Documents`.\n\n* [Integrations](https://docs.langchain.com/oss/python/integrations/document_loaders/): 160+ integrations to choose from.\n* [`BaseLoader`](https://reference.langchain.com/python/langchain_core/document_loaders/#langchain_core.document_loaders.BaseLoader): API reference for the base interface.\n\n### Splitting documents\n\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n\nTo handle this we'll split the [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n\nAs in the [semantic search tutorial](https://docs.langchain.com/oss/python/langchain/knowledge-base), we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n\n```python  theme={null}\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # chunk size (characters)\n    chunk_overlap=200,  # chunk overlap (characters)\n    add_start_index=True,  # track index in original document\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\ntext  theme={null} Split blog post into 66 sub-documents.\nGo deeper\nTextSplitter: Object that splits a list of Document objects into smaller chunks for storage and retrieval.\n\nIntegrations\nInterface: API reference for the base interface.\n\n\n\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\n```python theme={null} document_ids = vector_store.add_documents(documents=all_splits)\nprint(document_ids[:3])\n\n```python  theme={null}\n['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\nGo deeper\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\n\nIntegrations: 30+ integrations to choose from.\nInterface: API reference for the base interface.\n\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\n\nIntegrations: 40+ integrations to choose from.\nInterface: API reference for the base interface.\n\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question."
  },
  {
    "objectID": "L01/05_RAG_agent.html#retrieval-and-generation-1",
    "href": "L01/05_RAG_agent.html#retrieval-and-generation-1",
    "title": "Build a RAG agent with LangChain",
    "section": "2. Retrieval and generation",
    "text": "2. Retrieval and generation\nRAG applications commonly work as follows:\n\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\n\n\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\n\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n\nRAG agents\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\n```python theme={null} from langchain.tools import tool\n@tool(response_format=‚Äúcontent_and_artifact‚Äù) def retrieve_context(query: str): ‚Äú‚Äú‚ÄúRetrieve information to help answer a query.‚Äù‚Äú‚Äù retrieved_docs = vector_store.similarity_search(query, k=2) serialized = ‚Äú‚Äù.join( (f‚ÄùSource: {doc.metadata}: {doc.page_content}‚Äú) for doc in retrieved_docs ) return serialized, retrieved_docs\n\n&lt;Tip&gt;\n  Here we use the [tool decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) to configure the tool to attach raw documents as [artifacts](https://docs.langchain.com/oss/python/langchain/messages#param-artifact) to each [ToolMessage](https://docs.langchain.com/oss/python/langchain/messages#tool-message). This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n&lt;/Tip&gt;\n\n&lt;Tip&gt;\n  Retrieval tools are not limited to a single string `query` argument, as in the above example. You can\n  force the LLM to specify additional search parameters by adding arguments‚Äî for example, a category:\n\n  ```python  theme={null}\n  from typing import Literal\n\n  def retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\n\nGiven our tool, we can construct the agent:\n```python theme={null} from langchain.agents import create_agent\ntools = [retrieve_context] # If desired, specify custom instructions prompt = ( ‚ÄúYou have access to a tool that retrieves context from a blog post.‚Äù ‚ÄúUse the tool to help answer user queries.‚Äù ) agent = create_agent(model, tools, system_prompt=prompt)\n\nLet's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\n\n```python  theme={null}\nquery = (\n    \"What is the standard method for Task Decomposition?\\n\\n\"\n    \"Once you get the answer, look up common extensions of that method.\"\n)\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    event[\"messages\"][-1].pretty_print()\n================================ Human Message =================================\n\nWhat is the standard method for Task Decomposition?\n\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\n  Args:\n    query: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\n  Args:\n    query: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\n\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\n\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\n\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n\nYou can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph‚Äôs Agentic RAG tutorial for more advanced formulations.\n\n\n\nRAG chains\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\n\n\n\n\n\n\n‚úÖ Benefits\n‚ö†Ô∏è Drawbacks\n\n\n\n\nSearch only when needed ‚Äì The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls ‚Äì When a search is performed, it requires one call to generate the query and another to produce the final response.\n\n\nContextual search queries ‚Äì By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.\nReduced control ‚Äì The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\n\n\nMultiple searches allowed ‚Äì The LLM can execute several searches in support of a single user query.\n\n\n\n\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\n```python theme={null} from langchain.agents.middleware import dynamic_prompt, ModelRequest\n@dynamic_prompt def prompt_with_context(request: ModelRequest) -&gt; str: ‚Äú‚Äú‚ÄúInject context into state messages.‚Äù‚Äú‚Äù last_query = request.state[‚Äúmessages‚Äù][-1].text retrieved_docs = vector_store.similarity_search(last_query)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n\nsystem_message = (\n    \"You are a helpful assistant. Use the following context in your response:\"\n    f\"\\n\\n{docs_content}\"\n)\n\nreturn system_message\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\n\nLet's try this out:\n\n```python  theme={null}\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n================================ Human Message =================================\n\nWhat is task decomposition?\n================================== Ai Message ==================================\n\nTask decomposition is...\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\n The above RAG chain incorporates retrieved context into a single system message for that run.\nAs in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\n\nAdding a key to the state to store the retrieved documents\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\n\n```python theme={null} from typing import Any from langchain_core.documents import Document from langchain.agents.middleware import AgentMiddleware, AgentState\nclass State(AgentState): context: list[Document]\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]): state_schema = State\n  def before_model(self, state: AgentState) -&gt; dict[str, Any] | None:\n      last_message = state[\"messages\"][-1]\n      retrieved_docs = vector_store.similarity_search(last_message.text)\n\n      docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n\n      augmented_message_content = (\n          f\"{last_message.text}\\n\\n\"\n          \"Use the following context to answer the query:\\n\"\n          f\"{docs_content}\"\n      )\n      return {\n          \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\n          \"context\": retrieved_docs,\n      }\nagent = create_agent( model, tools=[], middleware=[RetrieveDocumentsMiddleware()], ) ```"
  },
  {
    "objectID": "L01/05_RAG_agent.html#next-steps",
    "href": "L01/05_RAG_agent.html#next-steps",
    "title": "Build a RAG agent with LangChain",
    "section": "Next steps",
    "text": "Next steps\nNow that we‚Äôve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\n\nStream tokens and other information for responsive user experiences\nAdd conversational memory to support multi-turn interactions\nAdd long-term memory to support memory across conversational threads\nAdd structured responses\nDeploy your application with LangSmith Deployment"
  },
  {
    "objectID": "L01/notes-case-studies.html",
    "href": "L01/notes-case-studies.html",
    "title": "Building Agentic AI Systems",
    "section": "",
    "text": "AppFolio -&gt; Usability\nCaptide -&gt; Assistant (Analyst); Dynamic UI -&gt; Cited RAG (Templated report)\nBlackRock -&gt; (Domain: Assets Management) Ala‚Äôdin Copilot for 100+ apps on the platform (Usability; simplification of complexity).\nMonday.com -&gt; Customer Service (and file an issue and provide context).\nCISCO TAC -&gt; Customer service for each product line.\nCISCO Outshift -&gt; Repetitive tasks + human-in-the-loop (assigned through Jira or GitLab). User: DevOps team. Human is the trigger.\nCISCO Customer Experience -&gt; Customer service (troubleshooting and license renewal).\nCH Robinson (Logistics) -&gt; parse emails requesting shipment and attatchments.\nDocent (Travel) -&gt; makes travel plans based on structured and unstructured data.\nGitLab -&gt; planner agent, developer agent, debugger agent, review agent (and security agent). Human-in-the-loop. PR merge.\nExa (Search Engine) -&gt; answer questions not just search docs. Splitting yields better results.\nElastic (security) -&gt; read and give out alerts. Text-to-sql -&gt; get alerts and summarization. Structured multi-step reasoning?\nHarmonica.ai (research) -&gt; find and get information to you. Startups (investment). User: investors.\nDefinely -&gt; editor assisted with AI, extracting data from docs and pdfs, and shows them and highlights them side-by-side with the editor.\nInfor -&gt; decision making in ERP system.\nInConv -&gt; Text-to-SQL. User: business developer.\nJ.P Morgan -&gt; Contracts analysis. Assistant (coach AI) for conslutants in banking investment (finance). Fraud detection (ML). Sentiment Analysis. Personalized campaigns (marketing assistance). Research for investment opportunities to predict stock.\nLinkedIn -&gt; automate 67% of the work is AI-based and will be 100% (by 2030). Personlization.\nKlarna -&gt; Customer service, automated decision making. Assistant for shopping. Awareness of risks for merchants. Classify and summarize user feedback (UX, and other). Response-time.\nRakuten -&gt; assistant for each role (RAG) on the internal docs and data.\nReplit -&gt; agents develop software (similar GitLab).\nMinimol -&gt; sales (spending efficiency).\nProsper -&gt; structuring unstructured data. Customer service."
  },
  {
    "objectID": "L01/case-studies.html",
    "href": "L01/case-studies.html",
    "title": "LangChain Case Studies",
    "section": "",
    "text": "To strip away the marketing fluff, ‚ÄúAgentic AI‚Äù is essentially an architectural pattern where an LLM is used as a dynamic control-flow engine rather than just a text generator. In traditional software, execution paths are deterministic, dictated by rigid if/else/switch statements or static state machines. In agentic software, the LLM acts as an orchestrator inside a loop (often modeled as a directed graph, like in LangGraph). It evaluates the current state, decides which pre-defined functions (tools/APIs) to invoke, parses the output, and decides the next step until a termination condition is met.\nLooking strictly at the real-world case studies from LangGraph‚Äôs repository (used by companies like Uber, GitLab, Klarna, and Cisco), here is exactly what engineers are building with this pattern:\n\n1. Autonomous Code Generation & Developer Productivity\nWho is doing it: Uber, GitLab, Replit, LinkedIn.\nWhat it does: Instead of just auto-completing a line of code, the software acts as a background worker. It reads a Jira ticket or GitHub issue, uses tools to grep or search the repository for relevant files, generates a proposed fix, and runs unit tests. If the compiler or tests throw an error, the agent feeds the stack trace back into its own context window, attempts a fix, and iterates until the tests pass before opening a Pull Request.\n\n\n2. Complex API Orchestration (‚ÄúDomain-Specific Copilots‚Äù)\nWho is doing it: J.P. Morgan, BlackRock, Klarna, AppFolio.\nWhat it does: These companies have thousands of internal APIs. Instead of building hundreds of custom UI forms for business users to interact with those systems, engineers expose the APIs to the agent as callable tools. A user types, ‚ÄúPull the Q3 risk report for Client X, check if they meet compliance rule Y, and email the summary to the legal team.‚Äù The agent dynamically chains together the CRM lookup API, the compliance engine API, and the SMTP server payload, passing the JSON outputs of one into the inputs of the next.\n\n\n3. Deep Research & Multi-Hop RAG\nWho is doing it: Morningstar, Athena Intelligence.\nWhat it does: Basic Retrieval-Augmented Generation (RAG) is just vector search + summarization. Agentic research tools perform multi-hop retrieval. The agent writes a SQL query, realizes the returned database table is missing a specific metric, writes a web search query to find that missing metric, scrapes the resulting URL, and merges the data sources into a final, highly structured financial or intelligence report.\n\n\n4. Stateful Customer Support & Triage\nWho is doing it: Cisco TAC, Prosper, Minimal.\nWhat it does: Moving past stateless ‚Äúchatbots‚Äù that just regurgitate FAQ documents. These agents have read/write access to internal systems. When a user submits a support ticket, the agent checks the user‚Äôs subscription tier, queries server telemetry logs for recent errors associated with their account, executes safe diagnostic commands, and either resolves the issue autonomously or drafts a highly detailed hand-off summary for a human Tier-2 engineer.\n\n\n5. Messy Data Extraction & Browser Automation\nWho is doing it: AirTop, Captide, WebToon.\nWhat it does: Converting the chaotic, unstructured web into strictly typed JSON or SQL. These agents navigate messy internal portals or scrape complex sites, visually parsing the DOM or dealing with PDFs. Because websites change constantly, brittle traditional scrapers break. The agentic approach uses the LLM to dynamically find the correct data fields on the page, handle pagination or login states, and map the extracted text to a strict schema (like a Pydantic model).\n\n\nThe TL;DR for Software Engineers:\nAgentic AI is not magic; it is non-deterministic orchestration.\nYour job as an engineer building these systems isn‚Äôt writing the AI itself. Your job shifts to defining strict API contracts (tools) for the LLM to call, managing context-window state, handling retry logic when the LLM hallucinates a bad JSON payload, and defining graph boundaries (edges and conditional nodes) to ensure the LLM stays on the rails and doesn‚Äôt get stuck in an infinite loop."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent.html",
    "href": "L04_Advanced/Voice_Agent.html",
    "title": "Build a voice agent with LangChain",
    "section": "",
    "text": "Chat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.\nVoice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.\n\n\nVoice agents are agents that can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.\nThey‚Äôre suited for a variety of use cases, including:\n\nCustomer support\nPersonal assistants\nHands-free interfaces\nCoaching and training\n\n\n\n\nAt a high level, every voice agent needs to handle three tasks:\n\nListen - capture audio and transcribe it\nThink - interpret intent, reason, plan\nSpeak - generate audio and stream it back to the user\n\nThe difference lies in how these steps are sequenced and coupled. In practice, production agents follow one of two main architectures:\n\n\nThe Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).\n\n\n\n\n\n\n\n\n\n\n\nPros:\n\nFull control over each component (swap STT/TTS providers as needed)\nAccess to latest capabilities from modern text-modality models\nTransparent behavior with clear boundaries between components\n\nCons:\n\nRequires orchestrating multiple services\nAdditional complexity in managing the pipeline\nConversion from speech to text loses information (e.g., tone, emotion)\n\n\n\n\nSpeech-to-speech uses a multimodal model that processes audio input and generates audio output natively.\n\n\n\n\n\n\n\n\n\n\n\nPros:\n\nSimpler architecture with fewer moving parts\nTypically lower latency for simple interactions\nDirect audio processing captures tone and other nuances of speech\n\nCons:\n\nLimited model options, greater risk of provider lock-in\nFeatures may lag behind text-modality models\nLess transparency in how audio is processed\nReduced controllability and customization options\n\nThis guide demonstrates the sandwich architecture to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.\n\n\n\n\nWe‚Äôll walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using AssemblyAI for STT and Cartesia for TTS (although adapters can be built for most providers).\nAn end-to-end reference application is available in the voice-sandwich-demo repository. We will walk through that application here.\nThe demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe demo implements a streaming pipeline where each stage processes data asynchronously:\nClient (Browser)\n\nCaptures microphone audio and encodes it as PCM\nEstablishes WebSocket connection to the backend server\nStreams audio chunks to the server in real-time\nReceives and plays back synthesized speech audio\n\nServer (Python)\n\nAccepts WebSocket connections from clients\nOrchestrates the three-step pipeline:\n\nSpeech-to-text (STT): Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events\nAgent: Processes transcripts with LangChain agent, streams response tokens\nText-to-speech (TTS): Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks\n\nReturns synthesized audio to the client for playback\n\nThe pipeline uses async generators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\nEach stage processes events independently and concurrently:\n\naudio transcription begins as soon as audio arrives,\nthe agent starts reasoning as soon as a transcript is available,\nand speech synthesis begins as soon as agent text is generated.\n\nThis architecture can achieve sub-700ms latency to support natural conversation."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent.html#overview",
    "href": "L04_Advanced/Voice_Agent.html#overview",
    "title": "Build a voice agent with LangChain",
    "section": "",
    "text": "Chat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.\nVoice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.\n\n\nVoice agents are agents that can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.\nThey‚Äôre suited for a variety of use cases, including:\n\nCustomer support\nPersonal assistants\nHands-free interfaces\nCoaching and training\n\n\n\n\nAt a high level, every voice agent needs to handle three tasks:\n\nListen - capture audio and transcribe it\nThink - interpret intent, reason, plan\nSpeak - generate audio and stream it back to the user\n\nThe difference lies in how these steps are sequenced and coupled. In practice, production agents follow one of two main architectures:\n\n\nThe Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).\n\n\n\n\n\n\n\n\n\n\n\nPros:\n\nFull control over each component (swap STT/TTS providers as needed)\nAccess to latest capabilities from modern text-modality models\nTransparent behavior with clear boundaries between components\n\nCons:\n\nRequires orchestrating multiple services\nAdditional complexity in managing the pipeline\nConversion from speech to text loses information (e.g., tone, emotion)\n\n\n\n\nSpeech-to-speech uses a multimodal model that processes audio input and generates audio output natively.\n\n\n\n\n\n\n\n\n\n\n\nPros:\n\nSimpler architecture with fewer moving parts\nTypically lower latency for simple interactions\nDirect audio processing captures tone and other nuances of speech\n\nCons:\n\nLimited model options, greater risk of provider lock-in\nFeatures may lag behind text-modality models\nLess transparency in how audio is processed\nReduced controllability and customization options\n\nThis guide demonstrates the sandwich architecture to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.\n\n\n\n\nWe‚Äôll walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using AssemblyAI for STT and Cartesia for TTS (although adapters can be built for most providers).\nAn end-to-end reference application is available in the voice-sandwich-demo repository. We will walk through that application here.\nThe demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe demo implements a streaming pipeline where each stage processes data asynchronously:\nClient (Browser)\n\nCaptures microphone audio and encodes it as PCM\nEstablishes WebSocket connection to the backend server\nStreams audio chunks to the server in real-time\nReceives and plays back synthesized speech audio\n\nServer (Python)\n\nAccepts WebSocket connections from clients\nOrchestrates the three-step pipeline:\n\nSpeech-to-text (STT): Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events\nAgent: Processes transcripts with LangChain agent, streams response tokens\nText-to-speech (TTS): Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks\n\nReturns synthesized audio to the client for playback\n\nThe pipeline uses async generators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\nEach stage processes events independently and concurrently:\n\naudio transcription begins as soon as audio arrives,\nthe agent starts reasoning as soon as a transcript is available,\nand speech synthesis begins as soon as agent text is generated.\n\nThis architecture can achieve sub-700ms latency to support natural conversation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Index",
    "section": "",
    "text": "Python Basics. To prepare or refresh: ÿßŸÑŸÖŸÇÿØŸÖÿ© ÿßŸÑÿ®ÿßŸäÿ´ŸàŸÜŸäÿ© ŸÑŸÑÿ®ÿ±ŸÖÿ¨ÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© | ÿ≠ÿ≥ÿßŸÜ ÿßŸÑŸÇŸàÿ≤"
  },
  {
    "objectID": "index.html#pre-requisites",
    "href": "index.html#pre-requisites",
    "title": "Index",
    "section": "",
    "text": "Python Basics. To prepare or refresh: ÿßŸÑŸÖŸÇÿØŸÖÿ© ÿßŸÑÿ®ÿßŸäÿ´ŸàŸÜŸäÿ© ŸÑŸÑÿ®ÿ±ŸÖÿ¨ÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© | ÿ≠ÿ≥ÿßŸÜ ÿßŸÑŸÇŸàÿ≤"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Index",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nLesson\nDescription\nMaterials\n\n\n\n\n\nCase Studies\n[summary]\n\n\n\nOverview LangChain, LangGraph, LangSmith.\n[lesson]\n\n\n\nUse Cases\n[lesson]\n\n\n1\nAgentsModels, Messages, Prompts, Structured Output, Tools, and Internet Search.\n[notebook]\n\n\n2\nSub Agents Build a personal assistant with agents as tools.\n[notebook]\n\n\n3\nSemantic SearchLoad, Embed, Store, Retrieve text based on meaning.\n[notebook]\n\n\n4\nRetrieval Augmented Generation (RAG)Answer questions about specific source of information.\n[notebook]\n\n\n5\nRAG in Agentic Systems Ingestion Pipeline, Retrieval, Orchestration, and Generation.\n[lesson]\n\n\n6\nLangGraph Workflow Patterns\n[notebook]\n\n\n7\nMemory Short-term, Long-term memories, and Immutable context.\n[notebook]\n\n\n8\nLangSmith\n[notebook]\n\n\n\n\nGet Help or Ask the docs anything about LangChain, powered by real-time docs."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#overview",
    "href": "L04_Advanced/Voice_Agent_slides.html#overview",
    "title": "Build a voice agent with LangChain",
    "section": "Overview",
    "text": "Overview\nChat interfaces have dominated how we interact with AI. Multimodal AI and text-to-speech (TTS) now make it possible to build agents that feel like conversational partners.\nVoice agents: interact by spoken words instead of keyboard and mouse ‚Äî more natural and engaging."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#what-are-voice-agents",
    "href": "L04_Advanced/Voice_Agent_slides.html#what-are-voice-agents",
    "title": "Build a voice agent with LangChain",
    "section": "What are voice agents?",
    "text": "What are voice agents?\nAgents that engage in natural spoken conversations: speech recognition + NLP + generative AI + TTS.\nUse cases:\n\nCustomer support\nPersonal assistants\nHands-free interfaces\nCoaching and training"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#how-do-voice-agents-work",
    "href": "L04_Advanced/Voice_Agent_slides.html#how-do-voice-agents-work",
    "title": "Build a voice agent with LangChain",
    "section": "How do voice agents work?",
    "text": "How do voice agents work?\nEvery voice agent handles three tasks:\n\nListen ‚Äî capture audio and transcribe it\nThink ‚Äî interpret intent, reason, plan\nSpeak ‚Äî generate audio and stream it back\n\nDifference: how these steps are sequenced and coupled. Two main architectures:"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#stt-agent-tts-sandwich",
    "href": "L04_Advanced/Voice_Agent_slides.html#stt-agent-tts-sandwich",
    "title": "Build a voice agent with LangChain",
    "section": "1. STT ‚Üí Agent ‚Üí TTS (‚ÄúSandwich‚Äù)",
    "text": "1. STT ‚Üí Agent ‚Üí TTS (‚ÄúSandwich‚Äù)\nThree components: STT ‚Üí LangChain Agent ‚Üí TTS.\n\n\n\n\n\n\n\n\n\n\n\nPros: Full control, swap providers, latest text models, transparent.\nCons: More orchestration, pipeline complexity, speech‚Üítext loses tone/emotion."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#speech-to-speech-s2s",
    "href": "L04_Advanced/Voice_Agent_slides.html#speech-to-speech-s2s",
    "title": "Build a voice agent with LangChain",
    "section": "2. Speech-to-Speech (S2S)",
    "text": "2. Speech-to-Speech (S2S)\nOne multimodal model: audio in ‚Üí audio out.\n\n\n\n\n\n\n\n\n\n\n\nPros: Simpler, lower latency, preserves tone.\nCons: Fewer models, lock-in risk, less transparency and control."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#why-the-sandwich-in-this-guide",
    "href": "L04_Advanced/Voice_Agent_slides.html#why-the-sandwich-in-this-guide",
    "title": "Build a voice agent with LangChain",
    "section": "Why the sandwich in this guide?",
    "text": "Why the sandwich in this guide?\nSandwich architecture balances performance, controllability, and access to modern models.\n\nSub-700ms latency with some STT/TTS providers\nModular components you can swap and control"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#demo-application-overview",
    "href": "L04_Advanced/Voice_Agent_slides.html#demo-application-overview",
    "title": "Build a voice agent with LangChain",
    "section": "Demo application overview",
    "text": "Demo application overview\n\nVoice agent with sandwich architecture: orders for a sandwich shop\nAssemblyAI for STT, Cartesia for TTS (adapters possible for most providers)\nReference app: voice-sandwich-demo\nWebSockets for real-time browser ‚ÜîÔ∏é server; adaptable to Twilio, Vonage, WebRTC"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#architecture",
    "href": "L04_Advanced/Voice_Agent_slides.html#architecture",
    "title": "Build a voice agent with LangChain",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#pipeline-client-server",
    "href": "L04_Advanced/Voice_Agent_slides.html#pipeline-client-server",
    "title": "Build a voice agent with LangChain",
    "section": "Pipeline: client & server",
    "text": "Pipeline: client & server\nClient (browser): Mic ‚Üí PCM ‚Üí WebSocket ‚Üí server; receives and plays synthesized audio.\nServer (Python): WebSocket in ‚Üí STT (e.g.¬†AssemblyAI) ‚Üí Agent (LangChain) ‚Üí TTS (e.g.¬†Cartesia) ‚Üí WebSocket out.\nStreaming at each stage with async generators: downstream can start before upstream finishes ‚Üí lower latency."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#pipeline-concurrency",
    "href": "L04_Advanced/Voice_Agent_slides.html#pipeline-concurrency",
    "title": "Build a voice agent with LangChain",
    "section": "Pipeline: concurrency",
    "text": "Pipeline: concurrency\nStages run independently and concurrently:\n\nTranscription starts as soon as audio arrives\nAgent reasons as soon as a transcript is available\nSpeech synthesis starts as soon as agent text is generated\n\n‚Üí Sub-700ms latency for natural conversation."
  },
  {
    "objectID": "L04_Advanced/Voice_Agent_slides.html#summary",
    "href": "L04_Advanced/Voice_Agent_slides.html#summary",
    "title": "Build a voice agent with LangChain",
    "section": "Summary",
    "text": "Summary\n\nVoice agents: Listen ‚Üí Think ‚Üí Speak; natural spoken interaction with AI\nSandwich architecture: STT ‚Üí LangChain Agent ‚Üí TTS\nDemo: voice-sandwich-demo ‚Äî WebSockets, AssemblyAI STT, Cartesia TTS, streaming pipeline"
  },
  {
    "objectID": "L01/01_agents.html",
    "href": "L01/01_agents.html",
    "title": "Agents in LangChain",
    "section": "",
    "text": "This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes."
  },
  {
    "objectID": "L01/01_agents.html#questions",
    "href": "L01/01_agents.html#questions",
    "title": "Agents in LangChain",
    "section": "Questions",
    "text": "Questions\n\nWhat is an Agent in LangChain and how to make one?\nWhat‚Äôs the relationship between an LLM and an Agent?\nWhat can agents do?\nHow do I compare and select the best model for my agent?\nCan I run an agent locally without a provider?"
  },
  {
    "objectID": "L01/01_agents.html#setup-virtual-environment",
    "href": "L01/01_agents.html#setup-virtual-environment",
    "title": "Agents in LangChain",
    "section": "Setup Virtual Environment",
    "text": "Setup Virtual Environment\nuv init\nuv venv -p 3.12\n\nActivate the virtual environment\n\nWindowsMacOS / Linux\n\n\n.venv\\Scripts\\activate.bat\n\n\nsource .venv/bin/activate\n\n\n\n\n\nInstall dependencies\nuv add ipykernel\nuv add langchain langchain_openai langchain_community \nNote: Agents require a model that supports tool calling."
  },
  {
    "objectID": "L01/01_agents.html#openrouter",
    "href": "L01/01_agents.html#openrouter",
    "title": "Agents in LangChain",
    "section": "OpenRouter",
    "text": "OpenRouter\nOpenRouter is the Unified Interface For LLMs.\nKey benefits include:\n\nOne API for all models\n\nno subscription to each provider needed\nswitch easily between models and providers by changing a str value\n\nSome models are free\nCircumvent regional restrictions\n\n\n\n\nOpen Router\n\n\nIf you don‚Äôt already have an OpenRouter API key, you can create one for free at: OpenRouter."
  },
  {
    "objectID": "L01/01_agents.html#what-is-an-api-key",
    "href": "L01/01_agents.html#what-is-an-api-key",
    "title": "Agents in LangChain",
    "section": "What is an API Key?",
    "text": "What is an API Key?\nThink of an API Key as a hotel key card.\n\nThe Hotel (Server): Has resources (rooms) but keeps them locked.\nThe Guest (Client): Wants access.\nThe Key Card (API Key): Identifies you and proves you are allowed to enter specific rooms.\n\n\n\nWhat & Why\nAn API key is a unique string of characters used to identify the calling program.\n\nIdentification: Keys ‚Äúauthenticate the calling project,‚Äù allowing the server to recognize who is asking for data.\nControl: This lets the server track usage for billing and enforce limits (quotas) so one user doesn‚Äôt crash the system.\n\n\n\n\nSecurity Risks\nIf you lose your key, it is like dropping your credit card.\n\nTheft: Attackers can use your key to make requests on your behalf.\nConsequences: You suffer financial loss (paying for their usage) or service denial (they use up your available quota).\n\n\nRule: Never post keys on public sites like GitHub.\n\n\n\nHow to Set Your API Key?\nWrite your API key into an .env file as an environment variable, as follows:\nOPENROUTER_API_KEY=...\n\nNote: make sure to add it to .gitignore to avoid committing it to the repository.\nNote: this is different than the .venv file used for the virtual environment.\n\nIf we use the OpenAI API, we‚Äôll have to add:\nOPENAI_API_BASE=\"https://openrouter.ai/api/v1\"\n.. such that the model uses OpenRouter instead of the default OpenAI API.\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# We use OpenRouter for the agent ‚Äî set OPENROUTER_API_KEY in .env\n# Get your key at https://openrouter.ai/keys\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    raise RuntimeError(\n        \"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:\\n\"\n        \"OPENROUTER_API_KEY=your-openrouter-api-key\"\n    )"
  },
  {
    "objectID": "L01/01_agents.html#basic-usage",
    "href": "L01/01_agents.html#basic-usage",
    "title": "Agents in LangChain",
    "section": "Basic usage",
    "text": "Basic usage\nModels can be utilized in two ways:\n\nWith agents - Models can be dynamically specified when creating an agent.\nStandalone - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.\n\nHere is a useful how-to for all the things that you can do with chat models, but we‚Äôll show a few highlights below.\nThere are a few standard parameters that we can set with chat models. Two of the most common are:\n\nmodel: the name of the model\nmax_tokens: limits the total number of tokens in the response, effectively controlling how long the output can be.\ntemperature: the sampling temperature\n\nLow temperature (close to 0) is more deterministic and focused outputs. This is good for tasks requiring accuracy or factual responses.\nHigh temperature (close to 1) is good for creative tasks or generating varied responses.\n\n\nLangChain supports many models via third-party integrations. By default, the course will use ChatOpenAI because it is both popular and performant.\n\nfrom langchain_openai import ChatOpenAI\n\n# https://openrouter.ai/openai/gpt-5-nano\nmodel_gpt5_nano = ChatOpenAI(\n    model=\"openai/gpt-5-nano\",\n    temperature=0,\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n)\n\n# https://openrouter.ai/nvidia/nemotron-3-nano-30b-a3b:free\nmodel_nemotron3_nano = ChatOpenAI(\n    model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n    temperature=0,\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n)\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nRunning a model locally\nLangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.\nOllama is one of the easiest ways to run chat and embedding models locally."
  },
  {
    "objectID": "L01/01_agents.html#key-methods",
    "href": "L01/01_agents.html#key-methods",
    "title": "Agents in LangChain",
    "section": "Key Methods",
    "text": "Key Methods\n\n.invoke(): results are only sent from the server when generation stops\n.stream(): results are sent from the server as they are being generated\n.batch(): send multiple inputs at once\n\n\n1. Invoke\nThe most straightforward way to call a model is to use invoke() with a single message or a list of messages:\n\nmessage = model_nemotron3_nano.invoke(\"what is Ramadan?\")\n\n.. this returns an AIMessage object:\n\nmessage\n\nAIMessage(content='**Ramadan** is the ninth month of the Islamic lunar calendar and is observed by Muslims worldwide as a month of fasting, prayer, reflection, and community.  \\n\\n### Key Features\\n\\n| Aspect | Description |\\n|--------|-------------|\\n| **Fasting (Sawm)** | From dawn (Fajr) to sunset (Maghrib) Muslims abstain from food, drink, smoking, and marital relations. The fast is intended to cultivate self‚Äëdiscipline, empathy for the less fortunate, and spiritual growth. |\\n| **Prayer** | Additional nightly prayers called **Tarawih** are performed at the mosque, often completing a recitation of the entire Qur‚Äôan over the course of the month. |\\n| **Qur‚Äôan Recitation** | Many Muslims aim to read or listen to the entire Qur‚Äôan during Ramadan, taking advantage of the increased spiritual focus. |\\n| **Charity (Zakat & Sadaqah)** | The month emphasizes giving to those in need; many people increase charitable donations and volunteer work. |\\n| **Community & Iftar** | The evening meal that breaks the fast, **iftar**, is often shared with family, friends, and sometimes the wider community. It typically starts with dates and water, following the example of the Prophet Muhammad. |\\n| **Spiritual Reflection** | Fasting is seen as a way to purify the heart, increase gratitude, and strengthen one‚Äôs relationship with God (Allah). |\\n\\n### Timing\\n\\n- **Lunar Calendar:** Ramadan follows the Islamic lunar calendar, so it moves about 10‚Äì12 days earlier each Gregorian year.  \\n- **Duration:** It lasts 29 or 30 days, depending on the sighting of the new moon.  \\n\\n### End of Ramadan\\n\\n- The month concludes with **Eid al‚ÄëFitr**, a festive celebration marking the first day of Shawwal (the next lunar month). Eid al‚ÄëFitr includes communal prayers, feasting, giving of gifts, and charitable donations (often called **Zakat al‚ÄëFitr**).  \\n\\n### Who Observes It?\\n\\n- **Obligatory** for all adult Muslims who are physically able, though exemptions exist for those who are ill, traveling, pregnant, nursing, or otherwise unable to fast.  \\n- Many children begin practicing partial fasts to prepare for adulthood.  \\n\\n### Cultural Variations\\n\\n- While the core practices are universal, local customs enrich the observance: special foods for suhoor (pre‚Äëdawn meal), communal iftars, charitable projects, and unique cultural performances.  \\n\\nIn summary, Ramadan is a deeply spiritual period that combines physical discipline through fasting with heightened devotion, community bonding, and charitable giving, culminating in the joyous celebration of Eid al‚ÄëFitr.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 646, 'prompt_tokens': 21, 'total_tokens': 667, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 67, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771843067-CE39pAmJGHoeBrlPaCGT', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c8a13-9ed6-7273-9973-db9b1588cee6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 21, 'output_tokens': 646, 'total_tokens': 667, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 67}})\n\n\n.. which has a content property, which includes the generated response text:\n\nprint(message.content)\n\n**Ramadan** is the ninth month of the Islamic lunar calendar and is observed by Muslims worldwide as a month of fasting, prayer, reflection, and community.  \n\n### Key Features\n\n| Aspect | Description |\n|--------|-------------|\n| **Fasting (Sawm)** | From dawn (Fajr) to sunset (Maghrib) Muslims abstain from food, drink, smoking, and marital relations. The fast is intended to cultivate self‚Äëdiscipline, empathy for the less fortunate, and spiritual growth. |\n| **Prayer** | Additional nightly prayers called **Tarawih** are performed at the mosque, often completing a recitation of the entire Qur‚Äôan over the course of the month. |\n| **Qur‚Äôan Recitation** | Many Muslims aim to read or listen to the entire Qur‚Äôan during Ramadan, taking advantage of the increased spiritual focus. |\n| **Charity (Zakat & Sadaqah)** | The month emphasizes giving to those in need; many people increase charitable donations and volunteer work. |\n| **Community & Iftar** | The evening meal that breaks the fast, **iftar**, is often shared with family, friends, and sometimes the wider community. It typically starts with dates and water, following the example of the Prophet Muhammad. |\n| **Spiritual Reflection** | Fasting is seen as a way to purify the heart, increase gratitude, and strengthen one‚Äôs relationship with God (Allah). |\n\n### Timing\n\n- **Lunar Calendar:** Ramadan follows the Islamic lunar calendar, so it moves about 10‚Äì12 days earlier each Gregorian year.  \n- **Duration:** It lasts 29 or 30 days, depending on the sighting of the new moon.  \n\n### End of Ramadan\n\n- The month concludes with **Eid al‚ÄëFitr**, a festive celebration marking the first day of Shawwal (the next lunar month). Eid al‚ÄëFitr includes communal prayers, feasting, giving of gifts, and charitable donations (often called **Zakat al‚ÄëFitr**).  \n\n### Who Observes It?\n\n- **Obligatory** for all adult Muslims who are physically able, though exemptions exist for those who are ill, traveling, pregnant, nursing, or otherwise unable to fast.  \n- Many children begin practicing partial fasts to prepare for adulthood.  \n\n### Cultural Variations\n\n- While the core practices are universal, local customs enrich the observance: special foods for suhoor (pre‚Äëdawn meal), communal iftars, charitable projects, and unique cultural performances.  \n\nIn summary, Ramadan is a deeply spiritual period that combines physical discipline through fasting with heightened devotion, community bonding, and charitable giving, culminating in the joyous celebration of Eid al‚ÄëFitr."
  },
  {
    "objectID": "L01/01_agents.html#messages",
    "href": "L01/01_agents.html#messages",
    "title": "Agents in LangChain",
    "section": "Messages",
    "text": "Messages\nMessages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\nMessages are objects that contain three things:\n\nContent: Actual model response: text, images, audio, documents, etc.\nMetadata: Optional fields such as response information, message IDs, and token usage\nRole: Identifies the message type. One of:\n\nSystemMessage: Tells the model how to behave and provide context for interactions\nHumanMessage: Represents user input and interactions with the model\nAIMessage: Responses generated by the model, including text content, tool calls, and metadata\nToolMessage: Represents the outputs of tool calls\n\n\n\nfrom langchain.messages import (\n    SystemMessage,\n    HumanMessage,\n    AIMessage\n)\n\n\nsystem_prompt = SystemMessage(\"You always answer with 10 concise words, no more.\")\nuser_prompt = HumanMessage(\"How do I make an Agent in Python?\")\n\nmessages = [\n    system_prompt,\n    user_prompt,\n]\nresponse = model_nemotron3_nano.invoke(messages)\n\n\ntype(message)\n\nlangchain_core.messages.ai.AIMessage\n\n\n\nprint(response.content)\n\nDefine class inherit from base implement act sense communicate loop\n\n\n\nMetadata\nAn AIMessage can hold token counts and other usage metadata in its usage_metadata field:\n\nresponse.usage_metadata\n\n{'input_tokens': 38,\n 'output_tokens': 299,\n 'total_tokens': 337,\n 'input_token_details': {'audio': 0, 'cache_read': 0},\n 'output_token_details': {'audio': 0, 'reasoning': 276}}\n\n\n\n\nConversation\nA list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.\n\nconversation = [\n    SystemMessage(\"You are a helpful assistant that translates English to Arabic.\"),\n    HumanMessage(\"Translate: I love programming.\"),\n    AIMessage(\"ÿ£ÿ≠ÿ® ÿßŸÑÿ®ÿ±ŸÖÿ¨ÿ©.\"),\n    HumanMessage(\"I love building applications.\")\n]\n\nmessage = model_nemotron3_nano.invoke(conversation)\n\n\nprint(message.content)\n\nÿ£ÿ≠ÿ® ÿ®ŸÜÿßÿ° ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™.\n\n\n\n\n2. Streaming and chunks\nMost models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.\nCalling stream() returns an iterator that yields output AIMessageChunk objects as they are produced. You can use a loop to process each chunk in real-time.\n\nchunks = []\nfor chunk in model_nemotron3_nano.stream(\"what is Ramadan? keep it short, keep it simple.\"):\n    chunks.append(chunk)\n    print(chunk.text, end=\"\", flush=True)\n\nRamadan is the ninth month of the Islamic lunar calendar. During it, Muslims fast from sunrise to sunset‚Äîno food, drink, or other physical needs‚Äîwhile also increasing prayer, charity, and reflection. The month ends with the celebration of‚ÄØEid‚ÄØal‚ÄëFitr.content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019c8ac4-24a9-7243-b435-06c1fabbe92f' tool_calls=[] invalid_tool_calls=[] tool_call_chunks=[]\n\n\n\nfor ch in chunks[50:60]:\n    print(ch.content)\n\n of\n the\n Islamic\n lunar\n calendar\n.\n During\n it\n,\n Muslims\n\n\n\n\n3. Batch\nBatching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:\n\nresponses = model_nemotron3_nano.batch([\n    \"What is the capital of Saudi Arabia?\",\n    \"What is 2 + 8\",\n    \"Is the sky blue or is it our perception? give a short and concise answer\"\n])\n\nfor response in responses:\n    print(response)\n\ncontent='The capital of Saudi Arabia is **Riyadh**.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 24, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 26, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587230-rws1sKd4yd2Ir45jBE92', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-dab8-7670-aebf-be1a19d73902-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 24, 'output_tokens': 38, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 26}}\ncontent='2\\u202f+\\u202f8\\u202f=\\u202f10.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 23, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 22, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587230-FEcwZ8VMp5p1Qgvgnrqr', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-daba-76d3-856b-d4204b17db9a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 23, 'output_tokens': 40, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 22}}\ncontent='The sky appears blue because molecules in the atmosphere scatter short‚Äëwavelength (blue) sunlight‚Äîan objective physical effect that our visual system interprets as the color blue.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 32, 'total_tokens': 206, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 159, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587231-VBn1J8V9CcCv10J2gqBT', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-dabc-7871-bdd8-f89b2bed8661-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 32, 'output_tokens': 174, 'total_tokens': 206, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 159}}\n\n\n\nfor i, response in enumerate(responses):\n    print(response.content)\n    print(\"=\"*100)\n\nThe capital of Saudi Arabia is **Riyadh**.\n====================================================================================================\n2‚ÄØ+‚ÄØ8‚ÄØ=‚ÄØ10.\n====================================================================================================\nThe sky appears blue because molecules in the atmosphere scatter short‚Äëwavelength (blue) sunlight‚Äîan objective physical effect that our visual system interprets as the color blue.\n===================================================================================================="
  },
  {
    "objectID": "L01/01_agents.html#structured-output",
    "href": "L01/01_agents.html#structured-output",
    "title": "Agents in LangChain",
    "section": "Structured output",
    "text": "Structured output\nModels can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.\nPydantic models provide the richest feature set with field validation, descriptions, and nested structures.\n\nfrom pydantic import BaseModel, Field\n\nclass Movie(BaseModel):\n    \"\"\"A movie with details.\"\"\"\n    title: str = Field(..., description=\"The title of the movie\")\n    year: int = Field(..., description=\"The year the movie was released\")\n    director: str = Field(..., description=\"The director of the movie\")\n    rating: float = Field(..., description=\"The movie's rating out of 10\")\n\n\nmodel_with_structure = model_nemotron3_nano.with_structured_output(Movie)\nresponse = model_with_structure.invoke(\n    \"Provide details about the movie Inception\"\n)\n\n\nprint(\"Title:\", response.title)\nprint(\"Year:\", response.year)\nprint(\"Director:\", response.director)\nprint(\"Rating:\", response.rating)\n\nTitle: Inception\nYear: 2010\nDirector: Christopher Nolan\nRating: 8.8\n\n\nAnother example: Refining Search Query\n\n# Schema for structured output\nfrom pydantic import BaseModel, Field\n\n\nclass SearchQuery(BaseModel):\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n    justification: str = Field(\n        None, description=\"Why this query is relevant to the user's request.\"\n    )\n\n# Augment the LLM with schema for structured output\nstructured_llm = model_nemotron3_nano.with_structured_output(SearchQuery)\n\n\n# Invoke the augmented LLM\noutput = structured_llm.invoke(\n    \"How does Calcium CT score relate to high cholesterol?\"\n)\n\n\nprint(\"search_query:\", output.search_query)\nprint(\"justification:\", output.justification)\n\nsearch_query: Calcium CT score relationship with high cholesterol atherosclerosis risk\njustification: The user wants to understand how a coronary artery calcium (CAC) score derived from CT imaging connects to elevated blood cholesterol levels. This requires information on the pathophysiology of atherosclerotic plaque formation, the role of lipid disorders, and how CAC scoring is used clinically to assess cardiovascular risk in the context of cholesterol.\n\n\n\nBenefits of Structured Output\nModels are trained specifically to structure their outputs, because benefits where paramount:\n\nCost: the model doesn‚Äôt generate extra text.\nAccuracy: you get only what you want; no more, no less.\nProgramming: structre can be parsed and used in programs. Example: tool calling."
  },
  {
    "objectID": "L01/01_agents.html#llms-and-augmentations",
    "href": "L01/01_agents.html#llms-and-augmentations",
    "title": "Agents in LangChain",
    "section": "LLMs and augmentations",
    "text": "LLMs and augmentations\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs."
  },
  {
    "objectID": "L01/01_agents.html#tool-calling",
    "href": "L01/01_agents.html#tool-calling",
    "title": "Agents in LangChain",
    "section": "Tool calling",
    "text": "Tool calling\nModels can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\n\nA schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\nA function or coroutine to execute.\n\nNote: A coroutine is a method that can suspend execution and resume at a later time\n\nfrom langchain.tools import tool\n\n# Define a tool\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiplies two numbers.\"\"\"\n    return a * b\n\n\n# Bind tools to the model\nllm_with_tools = model_nemotron3_nano.bind_tools([multiply])\nsystem_prompt = \"You are a helpful assistant that can use tools to perform calculations.\"\n\nStep 1: Model generates tool calls\n\nquestion = \"What is 2 times 3?\"\nmessages = [\n    SystemMessage(system_prompt),\n    HumanMessage(question),\n]\n\n\nai_msg = llm_with_tools.invoke(messages)\n\n\ntype(ai_msg)\n\nlangchain_core.messages.ai.AIMessage\n\n\n\n# Get the tool call\nai_msg.tool_calls\n\n[{'name': 'multiply',\n  'args': {'a': 2, 'b': 3},\n  'id': 'call_f60c3d347ce746c29de453f5',\n  'type': 'tool_call'}]\n\n\nStep 2: Execute tools and collect results\n\nfor tool_call in ai_msg.tool_calls:\n    # Execute the tool with the generated arguments\n    tool_msg = multiply.invoke(tool_call)\n    messages.append(tool_msg)\n\n\ntype(tool_msg)\n\nlangchain_core.messages.tool.ToolMessage\n\n\n\nmessages\n\n[SystemMessage(content='You are a helpful assistant that can use tools to perform calculations.', additional_kwargs={}, response_metadata={}),\n HumanMessage(content='What is 2 times 3?', additional_kwargs={}, response_metadata={}),\n ToolMessage(content='6', name='multiply', tool_call_id='call_f60c3d347ce746c29de453f5')]\n\n\nStep 3: Pass results back to model for final response\n\nfinal_response = model_nemotron3_nano.invoke(messages)\n\n\nprint(final_response.text)\n\nThe result of 2 times 3 is **6**. \n\nThis is a basic multiplication fact:  \n$ 2 \\times 3 = 6 $.  \n\nLet me know if you'd like further clarification! üòä\n\n\nWe‚Äôll say how this multi-step code instructcion is wrapped inside a create_agent function in langchain.\n\nInternet Search Tool\nTavily is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it‚Äôs easy to sign up and offers a generous free tier.\nuv add tavily-python\n\nSet up Tavily API for web search (Free)\n\nTavily Search API is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results.\nYou can sign up for an API key here. It‚Äôs easy to sign up and offers a very generous free tier. Some lessons (in Module 4) will use Tavily.\nSet TAVILY_API_KEY in your environment.\n\n\nfrom tavily import TavilyClient\n\ntavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n\n\nfrom typing import Literal\n\n\ndef internet_search(\n    query: str,\n    max_results: int = 5,\n    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n    include_raw_content: bool = False,\n):\n    \"\"\"Run a web search\"\"\"\n    return tavily_client.search(\n        query,\n        max_results=max_results,\n        include_raw_content=include_raw_content,\n        topic=topic,\n    )\n\n\nresult = internet_search(\"What is LangGraph?\", max_results=3)\nresult\n\n{'query': 'What is LangGraph?',\n 'follow_up_questions': None,\n 'answer': None,\n 'images': [],\n 'results': [{'url': 'https://www.ibm.com/think/topics/langgraph',\n   'title': 'What is LangGraph? - IBM',\n   'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent‚Äôs state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.',\n   'score': 0.95539,\n   'raw_content': None},\n  {'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n   'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',\n   'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions. LangGraph Studio is a visual development environment for LangChain‚Äôs LangGraph framework, simplifying the development of complex agentic applications built with LangChain components.',\n   'score': 0.942385,\n   'raw_content': None},\n  {'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/',\n   'title': 'What is LangGraph? - GeeksforGeeks',\n   'content': 'LangGraph is an open-source framework built by LangChain that streamlines the creation and management of AI agent workflows. At its core, LangGraph combines large language models (LLMs) with graph-based architectures allowing developers to map, organize and optimize how AI agents interact and make decisions. By treating workflows as interconnected nodes and edges, LangGraph offers a scalable, transparent and developer-friendly way to design advanced AI systems ranging from simple chatbots to multi-agent system. The diagram below shows how LangGraph structures its agent-based workflow using distinct tools and stages. By designing workflows, users combine multiple nodes into powerful, dynamic AI processes. * ****langgraph:**** Framework for building graph-based AI workflows. ### Step 6: Build LangGraph Workflow. * Build the workflow graph using LangGraph, adding nodes for classification and response, connecting them with edges and compiling the app. * Send each input through the workflow graph and returns the bot‚Äôs response, either a greeting or an AI-powered answer. + Machine Learning Interview Questions and Answers15+ min read.',\n   'score': 0.9393874,\n   'raw_content': None}],\n 'response_time': 0.93,\n 'request_id': '42d8adb7-c9b7-49b5-afcb-fc4749da5892'}"
  },
  {
    "objectID": "L01/01_agents.html#create-an-agent",
    "href": "L01/01_agents.html#create-an-agent",
    "title": "Agents in LangChain",
    "section": "Create an Agent",
    "text": "Create an Agent\nAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\nAn LLM Agent runs tools in a loop to achieve a goal. An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n\n\nAgent Loop\n\n\ncreate_agent provides a production-ready agent implementation.\n\nfrom langchain.agents import create_agent\n\n# System prompt to steer the agent to be an expert researcher\nAGENT_PROMPT = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n\nYou have access to an internet search tool as your primary means of gathering information.\n\nKeep it short and concise.\n\n## `internet_search`\n\nUse this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n\"\"\"\n\nagent = create_agent(\n    model=model_nemotron3_nano,\n    tools=[internet_search],\n    system_prompt=AGENT_PROMPT\n)\n\n\nInvoke the agent\n\nresult = agent.invoke({\n    \"messages\": [\n        HumanMessage(\"Explain agentic AI in a tweet\")\n    ]\n})\n\n\nresult\n\n{'messages': [HumanMessage(content='Explain agentic AI in a tweet', additional_kwargs={}, response_metadata={}, id='eb3b86d8-3111-4646-bd11-7f2b3d24e2cb'),\n  AIMessage(content='Agentic AI refers to AI systems that can autonomously set goals, plan, and act to achieve them without constant human input‚Äîthink of AI agents that decide, adapt, and execute tasks like a digital personal assistant on steroids.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 916, 'prompt_tokens': 456, 'total_tokens': 1372, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 651, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771856080-ALnud8RDRnzK2VkU71XS', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c8ada-2c1c-7260-b40d-69d3d37a726b-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 456, 'output_tokens': 916, 'total_tokens': 1372, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 651}})]}\n\n\n\n# Print the agent's response\nprint(result[\"messages\"][-1].content)\n\nAgentic AI refers to AI systems that can autonomously set goals, plan, and act to achieve them without constant human input‚Äîthink of AI agents that decide, adapt, and execute tasks like a digital personal assistant on steroids.\n\n\nLet‚Äôs ask about something that needs search:\n\nresult = agent.invoke({\n    \"messages\": [\n        HumanMessage(\"What is the difference between LangChain, LangGraph and LangSmith?\")\n    ]\n})\n\n\nresult\n\n{'messages': [HumanMessage(content='What is the difference between LangChain, LangGraph and LangSmith?', additional_kwargs={}, response_metadata={}, id='7f8c9005-eb8e-4072-86e0-64dd26bec6a6'),\n  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 123, 'prompt_tokens': 462, 'total_tokens': 585, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 66, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771856086-8OPWhAITZng1xT0GmOTD', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c8ada-45c5-7e13-8c63-b2bdba91d977-0', tool_calls=[{'name': 'internet_search', 'args': {'max_results': 5, 'topic': 'general', 'query': 'LangChain LangGraph LangSmith differences', 'include_raw_content': True}, 'id': 'call_8a3e988aa40842cd95601cea', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 462, 'output_tokens': 123, 'total_tokens': 585, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 66}}),\n  ToolMessage(content='{\"query\": \"LangChain LangGraph LangSmith differences\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://dev.to/rajkundalia/langchain-vs-langgraph-vs-langsmith-understanding-the-ecosystem-3m5o\", \"title\": \"LangChain vs LangGraph vs LangSmith - DEV Community\", \"content\": \"* **LangChain** provides the foundational building blocks for creating LLM applications through modular components and a unified interface for working with different AI providers. * **LangGraph** extends this foundation with **stateful, graph-based orchestration** for complex multi-agent workflows requiring loops, branching, and persistent state. * **LangSmith** completes the picture by offering **observability, tracing, and evaluation** tools for debugging and monitoring LLM applications in production. * **LangGraph** when you need sophisticated state management and agent coordination. What began as simple prompt‚Äìresponse interactions has grown into **multi-step workflows** involving retrieval systems, tool usage, autonomous agents, and long-running processes. LangChain is the **core framework** for building LLM-powered applications. If your application doesn‚Äôt need complex branching or shared long-lived state, **LangChain is the right tool**. ## LangGraph: Stateful Agent Orchestration. | LangChain | Composition | Linear workflows, RAG, simple agents |. | LangGraph | Orchestration | Branching, loops, shared state, multi-agent |. LangFlow provides a **visual, drag-and-drop** interface for building LangChain workflows.\", \"score\": 0.918838, \"raw_content\": \" [Skip to content](#main-content)\\\\n\\\\n[Log in](https://dev.to/enter?signup_subforem=1)    [Create account](https://dev.to/enter?signup_subforem=1&state=new-user)\\\\n\\\\n## DEV Community\\\\n\\\\n[Raj Kundalia](/rajkundalia)\\\\n\\\\nPosted on\\\\n\\\\n# LangChain vs LangGraph vs LangSmith: Understanding the Ecosystem\\\\n\\\\n[#langchain](/t/langchain) [#langsmith](/t/langsmith) [#langgraph](/t/langgraph)\\\\n\\\\n&gt; Building LLM apps isn‚Äôt just about prompts anymore.  \\\\n&gt;  It‚Äôs about **composition**, **orchestration**, and **observability**.\\\\n\\\\n---\\\\n\\\\n## TL;DR\\\\n\\\\n* **LangChain** provides the foundational building blocks for creating LLM applications through modular components and a unified interface for working with different AI providers.\\\\n* **LangGraph** extends this foundation with **stateful, graph-based orchestration** for complex multi-agent workflows requiring loops, branching, and persistent state.\\\\n* **LangSmith** completes the picture by offering **observability, tracing, and evaluation** tools for debugging and monitoring LLM applications in production.\\\\n\\\\n**Use:**\\\\n\\\\n* **LangChain** for straightforward chains and RAG systems\\\\n* **LangGraph** when you need sophisticated state management and agent coordination\\\\n* **LangSmith** throughout development and production for visibility into behavior\\\\n\\\\n### Hands-on GitHub Repositories\\\\n\\\\n* **LangChain RAG Project** ‚Üí &lt;https://github.com/rajkundalia/langchain-rag-project&gt;\\\\n* **LangGraph Analyzer** ‚Üí &lt;https://github.com/rajkundalia/langgraph-analyzer&gt;\\\\n* **LangSmith Learning** ‚Üí &lt;https://github.com/rajkundalia/langsmith-learning&gt;\\\\n\\\\n---\\\\n\\\\n## Introduction\\\\n\\\\nThe landscape of LLM application development has evolved rapidly since 2022.\\\\n\\\\nWhat began as simple prompt‚Äìresponse interactions has grown into **multi-step workflows** involving retrieval systems, tool usage, autonomous agents, and long-running processes. This evolution introduced **new problems at each stage** of the development lifecycle.\\\\n\\\\n* **The composition problem** ‚Üí How do you connect prompts, models, tools, and data?\\\\n* **The orchestration problem** ‚Üí How do you manage branching, retries, loops, and shared state?\\\\n* **The observability problem** ‚Üí How do you debug, evaluate, and monitor these systems?\\\\n\\\\nThe LangChain ecosystem emerged to address each layer:\\\\n\\\\n| Problem | Tool | Year |\\\\n| --- | --- | --- |\\\\n| Composition | LangChain | 2022 |\\\\n| Orchestration | LangGraph | 2024 |\\\\n| Observability | LangSmith | 2023‚Äì2024 |\\\\n\\\\nEach tool targets a **specific layer** in the LLM application stack.\\\\n\\\\n---\\\\n\\\\n## LangChain: The Foundation\\\\n\\\\nLangChain is the **core framework** for building LLM-powered applications.\\\\n\\\\nIts primary goal is abstraction: different LLM providers expose different APIs, capabilities, and quirks. LangChain hides these differences behind a **unified interface**.\\\\n\\\\n### Core Building Blocks\\\\n\\\\nLangChain is composed of modular, swappable components:\\\\n\\\\n* **Prompts** ‚Äì Templates and structured inputs for models\\\\n* **Models** ‚Äì OpenAI, Anthropic, Google, or local LLMs\\\\n* **Memory** ‚Äì Conversation history and contextual state\\\\n* **Tools** ‚Äì Function calls to external systems\\\\n* **Retrievers** ‚Äì Vector databases and RAG pipelines\\\\n\\\\n---\\\\n\\\\n### LCEL: LangChain Expression Language\\\\n\\\\nWhat ties everything together is **LCEL**.\\\\n\\\\nLCEL introduces a **declarative, pipe-based syntax** for composing chains:\\\\n\\\\n```\\\\nprompt | model | output_parser \\\\n```\\\\n\\\\nInstead of writing imperative glue code, you describe **data flow**.\\\\n\\\\n### Why LCEL Matters\\\\n\\\\nLCEL enables:\\\\n\\\\n* Automatic async, streaming, and batch execution\\\\n* Built-in LangSmith tracing\\\\n* Parallel execution of independent steps\\\\n* A unified `Runnable` interface (`invoke`, `batch`, `stream`)\\\\n\\\\nThis makes chains **faster**, **cleaner**, and easier to reason about.\\\\n\\\\n---\\\\n\\\\n### Multi-Provider Support\\\\n\\\\nLangChain supports dozens of LLM providers and integrations.\\\\n\\\\nYou can switch providers by changing **one line of configuration**, enabling:\\\\n\\\\n* Vendor independence\\\\n* A/B testing across models\\\\n* Cost and latency optimization\\\\n\\\\n---\\\\n\\\\n### When LangChain Is Enough\\\\n\\\\nUse LangChain when your workflow is primarily:\\\\n\\\\n```\\\\nInput ‚Üí Process ‚Üí Output \\\\n```\\\\n\\\\nTypical use cases include:\\\\n\\\\n* Chatbots with memory\\\\n* RAG-based Q&A systems\\\\n* Natural language ‚Üí SQL generation\\\\n* Linear tool pipelines\\\\n\\\\nIf your application doesn‚Äôt need complex branching or shared long-lived state, **LangChain is the right tool**.\\\\n\\\\n## \\\\n\\\\n## LangGraph: Stateful Agent Orchestration\\\\n\\\\nLangGraph solves the **orchestration problem**.\\\\n\\\\nAs soon as your application needs to:\\\\n\\\\n* make decisions,\\\\n* loop,\\\\n* retry,\\\\n* or coordinate multiple agents, linear chains start to break down.\\\\n\\\\n---\\\\n\\\\n### Graph-Based Architecture\\\\n\\\\nLangGraph models your application as a **directed graph**:\\\\n\\\\n* **Nodes** ‚Üí processing steps or agents\\\\n* **Edges** ‚Üí execution flow between nodes\\\\n\\\\nThis enables patterns that are hard or impossible with chains:\\\\n\\\\n* Loops and retries\\\\n* Conditional branching\\\\n* Parallel execution\\\\n* Shared, persistent state\\\\n\\\\n---\\\\n\\\\n### State as a First-Class Concept\\\\n\\\\nEvery LangGraph workflow operates on a **shared state object**.\\\\n\\\\n* Nodes receive the current state\\\\n* They compute updates\\\\n* Updates are merged back into state\\\\n\\\\nThis allows multiple agents to collaborate naturally.\\\\n\\\\n**Example:**\\\\n\\\\n* Research agent gathers sources\\\\n* Fact-checking agent validates claims\\\\n* Synthesis agent produces the final answer\\\\n\\\\nAll without complex message passing.\\\\n\\\\n---\\\\n\\\\n### Conditional Routing\\\\n\\\\nLangGraph supports **conditional edges**.\\\\n\\\\nA function decides which node runs next based on runtime state:\\\\n\\\\n* Route customer queries to specialist agents\\\\n* Loop back when required information is missing\\\\n* Retry until success conditions are met\\\\n\\\\n---\\\\n\\\\n### Persistence & Checkpointing\\\\n\\\\nLangGraph includes built-in **checkpointing**:\\\\n\\\\n* Persist state across restarts\\\\n* Resume long-running workflows\\\\n* Support human-in-the-loop pauses\\\\n* Enable time-travel debugging\\\\n\\\\nThis is critical for production-grade agent systems.\\\\n\\\\n---\\\\n\\\\n### Visualization Support\\\\n\\\\nLangGraph workflows are inspectable and exportable:\\\\n\\\\n* Mermaid diagrams for documentation\\\\n* PNG images for presentations\\\\n* ASCII graphs for terminal debugging\\\\n\\\\nThis makes complex agent systems **understandable and communicable**.\\\\n\\\\n---\\\\n\\\\n### When You Need LangGraph\\\\n\\\\nChoose LangGraph when you need:\\\\n\\\\n* Explicit shared state\\\\n* Runtime decision-making\\\\n* Retry and failure recovery\\\\n* Multi-agent coordination\\\\n* Long-running workflows\\\\n\\\\nA classic example is an **autonomous research agent** that iteratively searches, reads, verifies, and synthesizes information.\\\\n\\\\n## \\\\n\\\\n## LangSmith: The Observability Layer\\\\n\\\\nLangSmith answers the question:\\\\n\\\\n&gt; ‚ÄúWhat is my LLM application actually doing?‚Äù\\\\n\\\\nIt doesn‚Äôt build workflows ‚Äî it **illuminates them**.\\\\n\\\\n---\\\\n\\\\n### Tracing Everything\\\\n\\\\nLangSmith captures full execution traces:\\\\n\\\\n* Prompts and responses\\\\n* Token usage and latency\\\\n* Component call stacks\\\\n* Errors and retries\\\\n\\\\nYou can drill down from:\\\\n\\\\n* a full workflow run ‚Üí to a single LLM call.\\\\n\\\\nThis makes debugging *dramatically* easier.\\\\n\\\\n---\\\\n\\\\n### Evaluation & Regression Testing\\\\n\\\\nLangSmith allows you to:\\\\n\\\\n* Create evaluation datasets\\\\n* Run structured tests\\\\n* Track quality metrics\\\\n* Compare prompts and models\\\\n\\\\nThis enables **regression testing** for LLM apps ‚Äî a must-have for production systems.\\\\n\\\\n---\\\\n\\\\n### Production Monitoring\\\\n\\\\nIn production, LangSmith tracks:\\\\n\\\\n* Response times\\\\n* Error rates\\\\n* Token and cost trends\\\\n* Usage by workflow or user\\\\n\\\\nAlerts help you catch issues early and optimize costs.\\\\n\\\\n---\\\\n\\\\n### Framework-Agnostic\\\\n\\\\nWhile LangSmith integrates seamlessly with LangChain and LangGraph, it‚Äôs **not limited to them**.\\\\n\\\\nYou can instrument *any* LLM application with LangSmith.\\\\n\\\\n## \\\\n\\\\n## Quick Comparison\\\\n\\\\n| Tool | Solves | Use When |\\\\n| --- | --- | --- |\\\\n| LangChain | Composition | Linear workflows, RAG, simple agents |\\\\n| LangGraph | Orchestration | Branching, loops, shared state, multi-agent |\\\\n| LangSmith | Observability | Debugging, evaluation, production monitoring |\\\\n\\\\n## \\\\n\\\\n## The Broader Ecosystem\\\\n\\\\n### LangFlow\\\\n\\\\nLangFlow provides a **visual, drag-and-drop** interface for building LangChain workflows.\\\\n\\\\n* Great for prototyping\\\\n* Helpful for non-technical collaboration\\\\n* Often exported to code for production\\\\n\\\\n---\\\\n\\\\n### Model Context Protocol (MCP)\\\\n\\\\nMCP (by Anthropic) standardizes **tool and resource access** for LLMs.\\\\n\\\\n* Works at the tool/retriever layer\\\\n* Complements LangChain and LangGraph\\\\n* Reduces custom integration effort\\\\n* Framework-agnostic\\\\n\\\\nMCP does **not** replace orchestration tools ‚Äî it enhances connectivity.\\\\n\\\\n---\\\\n\\\\n## Conclusion\\\\n\\\\nThe LangChain ecosystem is **layered, not competitive**.\\\\n\\\\n* **LangChain** builds the core logic\\\\n* **LangGraph** manages complex workflows\\\\n* **LangSmith** makes everything observable\\\\n\\\\nMost serious LLM applications will use **more than one** of these tools.\\\\n\\\\nStart simple, add complexity only when needed, and **never ship without observability**.\\\\n\\\\n---\\\\n\\\\n## Further Reading & Resources\\\\n\\\\n* &lt;https://www.datacamp.com/tutorial/langchain-vs-langgraph-vs-langsmith-vs-langflow&gt;\\\\n* &lt;https://www.datacamp.com/tutorial/langgraph-tutorial&gt;\\\\n* &lt;https://www.datacamp.com/tutorial/langgraph-agents&gt;\\\\n* &lt;https://www.techvoot.com/blog/langchain-vs-langgraph-vs-langflow-vs-langsmith-2025&gt;\\\\n\\\\n**Video**  \\\\n &lt;https://www.youtube.com/watch?v=vJOGC8QJZJQ&gt;\\\\n\\\\n**Academy Finxter Series (Excellent Deep Dive)**\\\\n\\\\n* &lt;https://academy.finxter.com/langchain-langsmith-and-langgraph/&gt;\\\\n* &lt;https://academy.finxter.com/langsmith-and-writing-tools/&gt;\\\\n* &lt;https://academy.finxter.com/langgraph/&gt;\\\\n* &lt;https://academy.finxter.com/multi-agent-teams-preparation/&gt;\\\\n* &lt;https://academy.finxter.com/setting-up-our-multi-agent-team/&gt;\\\\n* &lt;https://academy.finxter.com/web-research-and-asynchronous-tools/&gt;\\\\n\\\\n## Top comments (0)\\\\n\\\\nSubscribe\\\\n\\\\nFor further actions, you may consider blocking this person and/or [reporting abuse](/report-abuse)\\\\n\\\\nWe\\'re a place where coders share, stay up-to-date and grow their careers.\\\\n\\\\n[Log in](https://dev.to/enter?signup_subforem=1)   [Create account](https://dev.to/enter?signup_subforem=1&state=new-user)\\\\n\\\\n \"}, {\"url\": \"https://medium.com/@anshuman4luv/langchain-vs-langgraph-vs-langflow-vs-langsmith-a-detailed-comparison-74bc0d7ddaa9\", \"title\": \"LangChain vs LangGraph vs LangFlow vs LangSmith - Medium\", \"content\": \"# LangChain vs LangGraph vs LangFlow vs LangSmith: A Detailed Comparison. In the rapidly evolving world of AI, building applications powered by advanced language models like GPT-4 or Llama 3 has become more accessible, thanks to frameworks like **LangChain**, **LangGraph**, **LangFlow**, and **LangSmith**. LangChain is an open-source framework designed to streamline the development of applications leveraging language models. * **Chains**: Link multiple tasks such as API calls, LLM queries, and data processing. LangGraph builds on LangChain‚Äôs capabilities, focusing on orchestrating complex, multi-agent interactions. * **Cyclical Workflows**: Support iterative processes where tasks feed back into earlier stages. Imagine building a task management assistant agent. LangGraph represents this as a graph where each action (e.g., add task, complete task, summarize) is a node. LangGraph‚Äôs flexibility allows dynamic routing and revisiting nodes, making it ideal for stateful, interactive systems. LangFlow: Visual Design for LLM Applications. LangSmith is a monitoring and testing tool for LLM applications in production. 2. **Choose LangGraph** for applications involving multiple agents with interdependent tasks.\", \"score\": 0.915091, \"raw_content\": \"[Sitemap](/sitemap/sitemap.xml)\\\\n\\\\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\\\n\\\\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40anshuman4luv%2Flangchain-vs-langgraph-vs-langflow-vs-langsmith-a-detailed-comparison-74bc0d7ddaa9&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\\\n\\\\n[Search](/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\\\n\\\\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40anshuman4luv%2Flangchain-vs-langgraph-vs-langflow-vs-langsmith-a-detailed-comparison-74bc0d7ddaa9&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\\\n\\\\n# LangChain vs LangGraph vs LangFlow vs LangSmith: A Detailed Comparison\\\\n\\\\n[Anshuman](/@anshuman4luv?source=post_page---byline--74bc0d7ddaa9---------------------------------------)\\\\n\\\\n3 min read\\\\n\\\\n¬∑\\\\n\\\\nJan 17, 2025\\\\n\\\\n--\\\\n\\\\nIn the rapidly evolving world of AI, building applications powered by advanced language models like GPT-4 or Llama 3 has become more accessible, thanks to frameworks like **LangChain**, **LangGraph**, **LangFlow**, and **LangSmith**. While these tools share some similarities, they cater to different aspects of AI application development. This guide provides a clear comparison to help you choose the right tool for your project.\\\\n\\\\n## 1. LangChain: The Backbone of LLM Workflows\\\\n\\\\nLangChain is an open-source framework designed to streamline the development of applications leveraging language models. It offers modular components to connect various tasks, enabling a seamless development experience.\\\\n\\\\n### Key Features:\\\\n\\\\n* **LLM Integration**: Compatible with both open-source and closed-source models.\\\\n* **Prompt Management**: Supports dynamic prompt creation and templating.\\\\n* **Chains**: Link multiple tasks such as API calls, LLM queries, and data processing.\\\\n* **Memory**: Store contextual information for personalized, long-term interactions.\\\\n* **Agents**: Implement decision-making logic for dynamic task execution.\\\\n* **Data Integration**: Connect external data sources via document loaders and vector databases.\\\\n\\\\n### Example Use Case:\\\\n\\\\nA customer service chatbot that:\\\\n\\\\n1. Uses llm (large language model) to generate responses.\\\\n2. Dynamically retrieves product information from a database.\\\\n3. Stores user interactions for continuity in future conversations.\\\\n\\\\n## 2. LangGraph: Coordinating Multi-Agent Workflows\\\\n\\\\nLangGraph builds on LangChain‚Äôs capabilities, focusing on orchestrating complex, multi-agent interactions. It‚Äôs particularly useful for applications requiring multiple components to collaborate intelligently.\\\\n\\\\n### Key Concepts:\\\\n\\\\n* **Nodes and Edges**: Define tasks (nodes) and their relationships (edges) in a graph structure.\\\\n* **State Management**: Share and update a global state across agents.\\\\n* **Cyclical Workflows**: Support iterative processes where tasks feed back into earlier stages.\\\\n\\\\n### Detailed Example:\\\\n\\\\nImagine building a task management assistant agent. The workflow involves processing user inputs to:\\\\n\\\\n1. Add tasks.\\\\n2. Complete tasks.\\\\n3. Summarize tasks.\\\\n\\\\nLangGraph represents this as a graph where each action (e.g., add task, complete task, summarize) is a node. The user input is processed in a central ‚ÄúProcess Input‚Äù node, which routes the request to the appropriate action node. A **state component** maintains the task list across interactions:\\\\n\\\\n* The ‚ÄúAdd Task‚Äù node adds tasks to the state.\\\\n* The ‚ÄúComplete Task‚Äù node updates the state to mark tasks as done.\\\\n* The ‚ÄúSummarize‚Äù node generates a report using an LLM based on the current state.\\\\n\\\\nLangGraph‚Äôs flexibility allows dynamic routing and revisiting nodes, making it ideal for stateful, interactive systems.\\\\n\\\\n### Ideal Use Case:\\\\n\\\\nA research assistant system where:\\\\n\\\\n1. One agent retrieves data.\\\\n2. Another analyzes it.\\\\n3. A third summarizes the findings.\\\\n\\\\nLangGraph‚Äôs graph-based architecture ensures smooth coordination and data flow.\\\\n\\\\n## 3. LangFlow: Visual Design for LLM Applications\\\\n\\\\nLangFlow simplifies the prototyping process by providing a visual, drag-and-drop interface to design and test workflows. It‚Äôs ideal for non-developers or teams aiming to rapidly iterate on ideas.\\\\n\\\\n### Key Features:\\\\n\\\\n* Visual workflow builder.\\\\n* Pre-configured components for common tasks.\\\\n* Integration with LangChain‚Äôs core functionalities.\\\\n\\\\n### Ideal Use Case:\\\\n\\\\nA prototype for a document summarization tool where:\\\\n\\\\n1. Users upload a document.\\\\n2. An LLM extracts key points.\\\\n3. Results are displayed in a user-friendly format.\\\\n\\\\nLangFlow allows teams to test workflows without extensive coding.\\\\n\\\\n## 4. LangSmith: Ensuring Reliability in Production\\\\n\\\\nLangSmith is a monitoring and testing tool for LLM applications in production. It helps developers ensure their systems are efficient, reliable, and cost-effective.\\\\n\\\\n### Key Features:\\\\n\\\\n* **Performance Metrics**: Track token usage, API latency, and error rates.\\\\n* **Error Logging**: Identify and debug issues in real-time.\\\\n* **Cost Optimization**: Monitor resource usage to minimize expenses.\\\\n\\\\n### Ideal Use Case:\\\\n\\\\nA customer support system handling high volumes of queries. LangSmith provides insights into:\\\\n\\\\n1. Token consumption trends.\\\\n2. Latency spikes during peak hours.\\\\n3. Error patterns affecting user experience.\\\\n\\\\n## Comparison Table\\\\n\\\\n## How to Choose the Right Tool\\\\n\\\\n1. **Start with LangChain** if your focus is on creating a robust LLM-powered application.\\\\n2. **Choose LangGraph** for applications involving multiple agents with interdependent tasks.\\\\n3. **Use LangFlow** for quick prototyping or collaborating with non-technical stakeholders.\\\\n4. **Implement LangSmith** when scaling applications to production to ensure reliability and cost-efficiency.\\\\n\\\\nBy leveraging these tools effectively, you can streamline your AI development process and focus on delivering impactful solutions.\\\\n\\\\n[## Written by Anshuman](/@anshuman4luv?source=post_page---post_author_info--74bc0d7ddaa9---------------------------------------)\\\\n\\\\n[42 followers](/@anshuman4luv/followers?source=post_page---post_author_info--74bc0d7ddaa9---------------------------------------)\\\\n\\\\n¬∑[1 following](/@anshuman4luv/following?source=post_page---post_author_info--74bc0d7ddaa9---------------------------------------)\\\\n\\\\n## Responses (2)\\\\n\\\\n[Text to speech](https://speechify.com/medium?source=post_page-----74bc0d7ddaa9---------------------------------------)\"}, {\"url\": \"https://aws.plainenglish.io/langchain-vs-langgraph-vs-langsmith-vs-langflow-understanding-through-a-realtime-project-2c3efd1606e7\", \"title\": \"LangChain vs LangGraph vs LangSmith vs LangFlow\", \"content\": \"How They Work Together in One Project ¬∑ LangChain is the code foundation. ¬∑ LangGraph is the workflow controller. ¬∑ LangSmith is the debug and\", \"score\": 0.90432173, \"raw_content\": null}, {\"url\": \"https://codebasics.io/blog/what-are-the-differences-between-langchain-langgraph-and-langsmith\", \"title\": \"LangChain vs LangGraph vs LangSmith: Which One Should You Use?\", \"content\": \"6. When Should You Use LangChain, LangGraph, or LangSmith? Whether you\\'re designing a chatbot, building a multi-step agentic workflow, or deploying a production-grade AI product‚Äîchoosing the correct tooling is more important. In this blog post, we deep dive into the three powerful tools LangChain vs LangGraph vs LangSmith which are frequently used together but serve distinctly different purposes. While LangChain and LangGraph help you build apps, LangSmith helps you understand and improve them in production. When Should You Use LangChain, LangGraph, or LangSmith? Choosing between LangChain, LangGraph, and LangSmith depends on your project\\'s workflow complexity, debugging needs, and production scale. Watch this YouTube video that clearly explains how LangChain, LangGraph, and LangSmith fit into modern LLM app development. Yes. LangSmith seamlessly integrates with LangChain and LangGraph, providing monitoring and observability for both simple and complex workflows. While LangChain and LangGraph help you build apps, LangSmith is the best tool for observability and prompt evaluation in production.\", \"score\": 0.8958969, \"raw_content\": \"Transform Your Career in 2026  12-Month Gen AI & DS Bootcamp. Live Mentorship, Real Projects, Real Skills.  [Explore Program!](https://codebasics.io/genai-bootcamp-3.0)\\\\n\\\\n[Explore Program!](https://codebasics.io/genai-bootcamp-3.0)\\\\n\\\\n# What Are the Differences Between LangChain, LangGraph, and LangSmith?\\\\n\\\\nAI & Data Science\\\\n\\\\nAug 04, 2025 | By Codebasics Team\\\\n\\\\n### Table of Contents\\\\n\\\\n1. Introduction\\\\n\\\\n2. What is LangChain\\\\n\\\\n2.1 Key Features of the LangChain Framework\\\\n\\\\n3. What is LangGraph\\\\n\\\\n3.1 When LangGraph Stands Out\\\\n\\\\n4. What is LangSmith\\\\n\\\\n4.1 Why LangSmith is Essential\\\\n\\\\n5. LangChain vs LangGraph vs LangSmith ‚Äì A Detailed Comparison\\\\n\\\\n6. When Should You Use LangChain, LangGraph, or LangSmith?\\\\n\\\\n7. Final Thoughts\\\\n\\\\n8. FAQs\\\\n\\\\n## 1. Introduction\\\\n\\\\nAs the development of the [Large Language Model](https://en.wikipedia.org/wiki/Large_language_model) (LLM) matures, developers and product teams are facing more complex challenges in building robust, scalable AI-powered applications. Whether you\\'re designing a chatbot, building a multi-step agentic workflow, or deploying a production-grade AI product‚Äîchoosing the correct tooling is more important.\\\\n\\\\nIn this blog post, we deep dive into the three powerful tools LangChain vs LangGraph vs LangSmith which are frequently used together but serve distinctly different purposes. If you‚Äôre unsure or wondering how to select the right tool for your project or how they work together then this guide is for you.\\\\n\\\\n## 2. What is LangChain?\\\\n\\\\n[LangChain](../../resources/langchain-crash-course) is presented as a Python-based framework that simplifies the creation of LLM-powered applications, particularly for straightforward, linear workflows where the LLM performs predefined tasks‚Äîsuch as question-answering, summarization, or document retrieval.\\\\n\\\\n### 2.1 Key Features of the LangChain Framework:\\\\n\\\\n* **Modularity**: Chains, tools, prompts, and memory are modularized for flexibility.\\\\n* **Ease of Use**: Ideal for developers building LLM apps with minimal agentic complexity.\\\\n* **Tool Integration**: Easily connects with APIs, vector stores, databases, and more.\\\\n* **Best For**: Chatbots, document Q&A, SQL query generation, RAG pipelines.\\\\n\\\\n## 3. What is LangGraph?\\\\n\\\\nLangGraph is introduced as a more advanced, stateful, graph-based framework built on top of LangChain. It is designed to orchestrate complex, multi-step, stateful agentic workflows that involve autonomous decision-making, retries, and iterative processes, often represented as a graph.\\\\n\\\\n### 3.1 When LangGraph Stands Out:\\\\n\\\\n* **State Machines**: Constructs workflows as DAGs (Directed Acyclic Graphs).\\\\n* **Asynchronous Agent Support**: Enables multiple agents to collaborate or loop.\\\\n* **Advanced Control Flow**: Perfect for situations where you need feedback loops, branching logic, and retries.\\\\n* **Best For**: AI agents, autonomous systems, iterative planning, complex tools orchestration.\\\\n\\\\n## 4. What is LangSmith?\\\\n\\\\nLangSmith is a monitoring and debugging platform purpose-built for LLM applications. While LangChain and LangGraph help you build apps, LangSmith helps you understand and improve them in production.\\\\n\\\\n### 4.1 Why LangSmith is Essential:\\\\n\\\\n* **Tracing and Debugging**: Visualize how prompts, models, and chains interact.\\\\n* **Evaluation**: Use human or automated grading to test model performance.\\\\n* **Prompt Experiments**: Run A/B tests to optimize system prompts.\\\\n* **Best For**: Teams deploying apps at scale who need transparency and version control.\\\\n\\\\n## 5. LangChain vs LangGraph vs LangSmith ‚Äì A Detailed Comparison\\\\n\\\\nHere is the detailed comparison of LangChain, LangGraph, and LangSmith:\\\\n\\\\n| **Feature** | **LangChain** | **LangGraph** | **LangSmith** |\\\\n| --- | --- | --- | --- |\\\\n| **Purpose** | Build linear LLM applications | Handle complex, stateful workflows | Debug, monitor, and evaluate LLM apps |\\\\n| **Ideal For** | Simple chatbots, Q&A tools | Autonomous agents, iterative workflows | Production-grade observability |\\\\n| **Control Flow** | Sequential | Graph-based, branching & looping | Not applicable (used for monitoring) |\\\\n| **Workflow Complexity** | Basic | Advanced | Not Applicable |\\\\n| **Agent Support** | Basic agent capabilities | Full agent support with state mgmt | Full support for agent monitoring |\\\\n| **Integration** | Python, JS, APIs, vector DBs | Built on LangChain, supports same | LangChain & LangGraph compatible |\\\\n| **Deployment Focus** | Prototypes, MVPs | Production agents | Production debugging & evaluation |\\\\n| **Visualization** | No | Partial (via LangSmith) | Full tracing and logs |\\\\n| **Evaluation Tools** | None | None | Prompt tests, metrics, user grading |\\\\n\\\\n## 6. When Should You Use LangChain, LangGraph, or LangSmith?\\\\n\\\\nChoosing between LangChain, LangGraph, and LangSmith depends on your project\\'s workflow complexity, debugging needs, and production scale.\\\\n\\\\n**Use LangChain if:**\\\\n\\\\n* You\\'re creating a simple chatbot, summarizing a document, or RAG system.\\\\n* You prefer simplicity and fast prototyping.\\\\n* Your use case doesn‚Äôt require advanced memory, loops, or retries.\\\\n\\\\n**Use LangGraph if:**\\\\n\\\\n* You need to create multi-agent, multi-step, or self-correcting workflows.\\\\n* Your LLM app requires state management and branching logic.\\\\n* You‚Äôre building production-grade agents or autonomous systems.\\\\n\\\\n**Use LangSmith if:**\\\\n\\\\n* You\\'re ready to move to production and need visibility into what your app is doing.\\\\n* You want to evaluate different prompts, models, or tools.\\\\n* You\\'re managing LLM behavior across teams or projects.\\\\n\\\\n[Watch this YouTube video](https://www.youtube.com/watch?v=vJOGC8QJZJQ) that clearly explains how LangChain, LangGraph, and LangSmith fit into modern LLM app development.\\\\n\\\\n## 7. Final Thoughts\\\\n\\\\nLangChain, LangGraph, and LangSmith aren‚Äôt competitors‚Äîthey\\'re complementary. Think of them as a [full-stack toolkit](https://www.geeksforgeeks.org/blogs/full-stack-development-tools/) for modern LLM app development:\\\\n\\\\n* LangChain helps you build.\\\\n* LangGraph helps you orchestrate.\\\\n* LangSmith helps you optimize.\\\\n\\\\nUnderstanding where each tool fits allows you to architect better, debug smarter, and scale faster.\\\\n\\\\n*Note: The features and comparisons mentioned are based on the capabilities of LangChain, LangGraph, and LangSmith as of July 2025. These tools are evolving quickly, so we recommend checking their official documentation for the most up-to-date information.*\\\\n\\\\n## FAQs\\\\n\\\\n**1. Can you use LangSmith with LangChain?**  \\\\nYes. LangSmith seamlessly integrates with LangChain and LangGraph, providing monitoring and observability for both simple and complex workflows.\\\\n\\\\n**2. Is LangGraph better than LangChain?**  \\\\nNot necessarily. LangGraph is more powerful for advanced workflows, but LangChain is easier and faster for simple applications. Choose based on your use case.\\\\n\\\\n**3. Which is best for production debugging: LangSmith vs others?**  \\\\nLangSmith is purpose-built for debugging and monitoring. While LangChain and LangGraph help you build apps, LangSmith is the best tool for observability and prompt evaluation in production.\\\\n\\\\n**4. Can LangChain be used for fine-tuning?**  \\\\nNo, LangChain is not for fine-tuning models. It\\'s designed to build LLM applications, but fine-tuning is done using frameworks like Hugging Face or OpenAI\\'s API.\\\\n\\\\n**5. Should I learn LangGraph instead of LangChain?**  \\\\nLearn LangChain if you need simple, linear workflows like chatbots or Q&A systems. Learn LangGraph if you need more complex workflows with multi-step logic, decision-making, or autonomous agents. Start with LangChain, and move to LangGraph if your projects become more complex.\\\\n\\\\n#### Share With Friends\\\\n\\\\n[8 Must-Have Skills to Get a Data Analyst Job in 2024](/blog/8-must-have-skills-to-get-a-data-analyst-job) [How to Learn SQL and Python for Data Engineering: A Complete Step-by-Step Guide](/blog/how-to-learn-sql-and-python-for-data-engineering-a-complete-step-by-step-guide)\\\\n\\\\nIn Demand\\\\n\\\\nUS$291\\\\n\\\\n[Gen AI & Data Science Bootcamp 3.0: With Practical Job Placement Support & Virtual Internship](https://codebasics.io/bootcamps/gen-ai-data-science-bootcamp-with-virtual-internship)  [Become a high-paying AI-enabled Data Scientist by learning the secrets of the industry taught by data scientist hiring managers with 8+ years of international experience in data industry.](https://codebasics.io/bootcamps/gen-ai-data-science-bootcamp-with-virtual-internship)\\\\n\\\\n#### Categories\\\\n\\\\n* [Data Analysis](https://codebasics.io/blogs/data-analysis)\\\\n* [Talent Management](https://codebasics.io/blogs/talent-management)\\\\n* [AI & Data Science](https://codebasics.io/blogs/ai-data-science66cec6148d22d)\\\\n* [Data Science](https://codebasics.io/blogs/data-science)\\\\n* [Deep Learning](https://codebasics.io/blogs/deep-learning)\\\\n* [Artificial Intelligence](https://codebasics.io/blogs/artificial-intelligence66629f6dc7fb7)\\\\n* [Python](https://codebasics.io/blogs/python)\\\\n* [Data Engineering](https://codebasics.io/blogs/data-engineering692ec50b6daa5)\\\\n* [Machine Learning](https://codebasics.io/blogs/machine-learning669f54cfb1cad)\\\\n* [Programming](https://codebasics.io/blogs/programming)\\\\n\\\\n#### Related Blogs\\\\n\\\\nFeb 11, 2026\\\\n\\\\n[How Gen AI Will Revolutionize Data Science in 2026](/blog/how-gen-ai-will-revolutionize-data-science-in-2026)\\\\n\\\\nMay 27, 2025\\\\n\\\\n[What is Agentic AI and How Does it Work?](/blog/what-is-agentic-ai-and-how-does-it-work)\\\\n\\\\nMay 20, 2025\\\\n\\\\n[How to Start an AI Career in 2025 ‚Äì Roles & Skills Explained](/blog/how-to-start-an-ai-career-in-2025-roles-skills-explained)\\\\n\\\\nMay 08, 2025\\\\n\\\\n[Model Context Protocol Explained: Streamlining AI Integration and Development](/blog/model-context-protocol-explained-streamlining-ai-integration-and-development)\\\\n\\\\nApr 23, 2025\\\\n\\\\n[AI Agents: The Ultimate Productivity Tool Revolutionizing Automation in 2025](/blog/ai-agents-the-ultimate-productivity-tool-revolutionizing-automation-in-2025)\\\\n\\\\nLearning knows no limits. Here‚Äôs to your journey of seamless learning. Pick your preferred course from the list of paid & free resources.\\\\n\\\\nQuick Links\\\\n\\\\n* [Courses](https://codebasics.io/courses)\\\\n* [Blogs](https://codebasics.io/blogs)\\\\n* [Data Challenges](/challenges/resume-project-challenge)\\\\n* [Hire Talent](https://codebasics.io/hiring-partners)\\\\n* [Contact Us](https://codebasics.io/contact-us)\\\\n\\\\nHelp & Support\\\\n\\\\n* [Refund Policy](https://codebasics.io/refund-policy)\\\\n* [Terms & Conditions](https://codebasics.io/terms-and-conditions)\\\\n* [Privacy Policy](https://codebasics.io/privacy-policy)\\\\n* [Certificate Verification](https://codebasics.io/certificate_validation)\\\\n* [Business Inquiries](/cdn-cgi/l/email-protection#37175542445e5952444477545853525556445e5444195e58)\\\\n\\\\nCourse Topics\\\\n\\\\n* [AI & Data Science](https://codebasics.io/categories/data-science)\\\\n* [Exploratory Data Analysis (EDA)](https://codebasics.io/categories/exploratory-data-analysis-eda)\\\\n* [Career Advice](https://codebasics.io/categories/career-advice)\\\\n* [Conversations](https://codebasics.io/categories/conversations)\\\\n* [Data Analysis](https://codebasics.io/categories/data-analysis)\\\\n\\\\n* [Deep Learning](https://codebasics.io/categories/deep-learning)\\\\n* [Julia](https://codebasics.io/categories/julia)\\\\n* [Jupyter Notebook](https://codebasics.io/categories/jupyter-notebook)\\\\n* [Machine Learning](https://codebasics.io/categories/machine-learning)\\\\n* [Matplotlib](https://codebasics.io/categories/matplotlib)\\\\n\\\\n¬© 2026 [Codebasics](https://codebasics.io). Owned and operated by LearnerX EdTech Private Limited.   \\\\n Technology partner:  [AtliQ Technologies](https://www.atliq.com/)\\\\n\\\\n##### Login\\\\n\\\\n[Log in with Google](https://codebasics.io/google)\\\\n\\\\n[Log in with Linkedin](https://codebasics.io/linkedin)\\\\n\\\\nDon\\'t have an account? [Register Now!](https://codebasics.io/register)\\\\n\\\\nBy signing up, you agree to  \\\\n our [Terms and Conditions](https://codebasics.io/terms-and-conditions) and [Privacy Policy](https://codebasics.io/privacy-policy).\\\\n\\\\n[Talk to us](https://forms.office.com/r/0d6zvJeYa1 \\\\\"Talk to us\\\\\")  [Chat with us](https://wa.me/918977530886?text=Hi%20Codebasics!\\\\n\\\\n \\\\\"Chat with us\\\\\")\\\\n\\\\n \"}, {\"url\": \"https://www.reddit.com/r/Rag/comments/1mxs81z/finally_figured_out_the_langchain_vs_langgraph_vs/\", \"title\": \"Finally figured out the LangChain vs LangGraph vs LangSmith ...\", \"content\": \"# Finally figured out the LangChain vs LangGraph vs LangSmith confusion - here\\'s what I learned. After weeks of being confused about when to use LangChain, LangGraph, or LangSmith (and honestly making some poor choices), I decided to dive deep and create a breakdown. The TLDR: They\\'re not competitors - they\\'re actually designed to work together, but each serves a very specific purpose that most tutorials don\\'t explain clearly. üîó Full breakdown: LangSmith vs LangChain vs LangGraph The REAL Difference for Developers. The game-changer for me was understanding that you can (and often should) use them together. LangChain for the basics, LangGraph for complex flows, LangSmith to see what\\'s actually happening under the hood. Anyone else been through this confusion? What\\'s your go-to setup for production LLM apps? Would love to hear how others are structuring their GenAI projects - especially if you\\'ve found better alternatives or have war stories about debugging LLM applications üòÖ.\", \"score\": 0.8923472, \"raw_content\": \"           \\\\n\\\\n  \\\\n            \\\\n   \\\\n\\\\n[Go to Rag](/r/Rag/)   \\\\n\\\\n[r/Rag](/r/Rag/)   ‚Ä¢\\\\n\\\\n# Finally figured out the LangChain vs LangGraph vs LangSmith confusion - here\\'s what I learned\\\\n\\\\nAfter weeks of being confused about when to use LangChain, LangGraph, or LangSmith (and honestly making some poor choices), I decided to dive deep and create a breakdown.\\\\n\\\\nThe TLDR: They\\'re not competitors - they\\'re actually designed to work together, but each serves a very specific purpose that most tutorials don\\'t explain clearly.\\\\n\\\\nüîó Full breakdown: [LangSmith vs LangChain vs LangGraph The REAL Difference for Developers](https://www.youtube.com/watch?v=DGxf0X1GdtQ)\\\\n\\\\nThe game-changer for me was understanding that you can (and often should) use them together. LangChain for the basics, LangGraph for complex flows, LangSmith to see what\\'s actually happening under the hood.\\\\n\\\\nAnyone else been through this confusion? What\\'s your go-to setup for production LLM apps?\\\\n\\\\nWould love to hear how others are structuring their GenAI projects - especially if you\\'ve found better alternatives or have war stories about debugging LLM applications üòÖ\\\\n\\\\n          \\\\n\\\\n[Reddit Rules](https://www.redditinc.com/policies/content-policy)    [Privacy Policy](https://www.reddit.com/policies/privacy-policy)    [User Agreement](https://www.redditinc.com/policies/user-agreement)    [Your Privacy Choices](https://support.reddithelp.com/hc/articles/43980704794004)    [Accessibility](https://support.reddithelp.com/hc/sections/38303584022676-Accessibility)   [Reddit, Inc. ¬© 2026. All rights reserved.](https://redditinc.com)\\\\n\\\\n                                    \"}], \"response_time\": 5.43, \"request_id\": \"54b07b6b-5d65-42aa-83ab-1694988b125d\"}', name='internet_search', id='0380801e-3706-41ee-adad-e4593fbb87a6', tool_call_id='call_8a3e988aa40842cd95601cea'),\n  AIMessage(content='**LangChain\\u202fvs\\u202fLangGraph\\u202fvs\\u202fLangSmith ‚Äì What They Do and When to Use Them**\\n\\n| Aspect | **LangChain** | **LangGraph** | **LangSmith** |\\n|--------|---------------|---------------|---------------|\\n| **Primary Goal** | Provide a **modular toolbox** for building LLM‚Äëpowered apps (prompts, models, memory, tools, RAG, etc.). | Add **stateful, graph‚Äëbased orchestration** for complex, multi‚Äëstep workflows that need branching, loops, retries, and shared state. | Offer **observability, tracing, evaluation, and monitoring** for any LLM application (especially those built with LangChain/LangGraph). |\\n| **Core Concept** | **Chains** ‚Äì linear pipelines of components (`prompt ‚Üí model ‚Üí parser`). | **Graphs** ‚Äì nodes (steps/agents) and edges (control flow) that can loop, branch, and retain a mutable state. | **Traces** ‚Äì full execution logs that let you drill from a workflow run down to a single LLM call, plus evaluation & metrics. |\\n| **Typical Use‚ÄëCases** | ‚Ä¢ Simple chatbots, Q&A, RAG pipelines&lt;br&gt;‚Ä¢ Linear tool usage (e.g., fetch‚Äëthen‚Äësummarize)&lt;br&gt;‚Ä¢ Prototyping & MVPs | ‚Ä¢ Multi‚Äëagent systems&lt;br&gt;‚Ä¢ Workflows with feedback loops, retries, or conditional routing&lt;br&gt;‚Ä¢ Long‚Äërunning processes that need persistent state | ‚Ä¢ Debugging production pipelines&lt;br&gt;‚Ä¢ A/B‚Äëtesting prompts/models&lt;br&gt;‚Ä¢ Monitoring latency, cost, error rates&lt;br&gt;‚Ä¢ Regression testing of LLM behavior |\\n| **When to Choose It** | Your app fits a **straight‚Äëthrough** flow and does not need complex state or branching. | You need **shared state, dynamic routing, or multi‚Äëagent coordination** (e.g., research assistants, autonomous planners). | You are moving to **production** and need visibility, debugging, and evaluation capabilities. |\\n| **Integration** | Works with any LLM provider via a unified interface. | Built on top of LangChain; can be used alongside it. | Agnostic ‚Äì can instrument LangChain, LangGraph, or any custom LLM code. |\\n| **Key Benefits** | ‚Ä¢ Rapid prototyping&lt;br&gt;‚Ä¢ Vendor‚Äëagnostic model switching&lt;br&gt;‚Ä¢ Built‚Äëin async/streaming via LCEL | ‚Ä¢ Explicit state management&lt;br&gt;‚Ä¢ Conditional edges & loops&lt;br&gt;‚Ä¢ Persistence & checkpointing for long‚Äërunning agents | ‚Ä¢ End‚Äëto‚Äëend tracing&lt;br&gt;‚Ä¢ Prompt & model evaluation tools&lt;br&gt;‚Ä¢ Production‚Äëgrade monitoring & alerting |\\n\\n### TL;DR Summary\\n- **LangChain** = *building blocks* for basic LLM apps.  \\n- **LangGraph** = *orchestrator* for sophisticated, stateful, multi‚Äëagent workflows.  \\n- **LangSmith** = *debugger & observability layer* that makes those apps production‚Äëready.\\n\\nIn practice, a typical production stack uses **all three together**: build with LangChain, orchestrate complex flows with LangGraph, and monitor/evaluate everything with LangSmith.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 722, 'prompt_tokens': 9878, 'total_tokens': 10600, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 67, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771856094-Dwpy8GjlZFprVUGX6Yh0', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c8ada-64de-7c50-b22f-0d0678c3c586-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 9878, 'output_tokens': 722, 'total_tokens': 10600, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 67}})]}\n\n\n\ntype(result[\"messages\"][-2])\n\nlangchain_core.messages.tool.ToolMessage\n\n\n\n# Print the agent's response\nprint(result[\"messages\"][-1].content)\n\n**LangChain‚ÄØvs‚ÄØLangGraph‚ÄØvs‚ÄØLangSmith ‚Äì What They Do and When to Use Them**\n\n| Aspect | **LangChain** | **LangGraph** | **LangSmith** |\n|--------|---------------|---------------|---------------|\n| **Primary Goal** | Provide a **modular toolbox** for building LLM‚Äëpowered apps (prompts, models, memory, tools, RAG, etc.). | Add **stateful, graph‚Äëbased orchestration** for complex, multi‚Äëstep workflows that need branching, loops, retries, and shared state. | Offer **observability, tracing, evaluation, and monitoring** for any LLM application (especially those built with LangChain/LangGraph). |\n| **Core Concept** | **Chains** ‚Äì linear pipelines of components (`prompt ‚Üí model ‚Üí parser`). | **Graphs** ‚Äì nodes (steps/agents) and edges (control flow) that can loop, branch, and retain a mutable state. | **Traces** ‚Äì full execution logs that let you drill from a workflow run down to a single LLM call, plus evaluation & metrics. |\n| **Typical Use‚ÄëCases** | ‚Ä¢ Simple chatbots, Q&A, RAG pipelines&lt;br&gt;‚Ä¢ Linear tool usage (e.g., fetch‚Äëthen‚Äësummarize)&lt;br&gt;‚Ä¢ Prototyping & MVPs | ‚Ä¢ Multi‚Äëagent systems&lt;br&gt;‚Ä¢ Workflows with feedback loops, retries, or conditional routing&lt;br&gt;‚Ä¢ Long‚Äërunning processes that need persistent state | ‚Ä¢ Debugging production pipelines&lt;br&gt;‚Ä¢ A/B‚Äëtesting prompts/models&lt;br&gt;‚Ä¢ Monitoring latency, cost, error rates&lt;br&gt;‚Ä¢ Regression testing of LLM behavior |\n| **When to Choose It** | Your app fits a **straight‚Äëthrough** flow and does not need complex state or branching. | You need **shared state, dynamic routing, or multi‚Äëagent coordination** (e.g., research assistants, autonomous planners). | You are moving to **production** and need visibility, debugging, and evaluation capabilities. |\n| **Integration** | Works with any LLM provider via a unified interface. | Built on top of LangChain; can be used alongside it. | Agnostic ‚Äì can instrument LangChain, LangGraph, or any custom LLM code. |\n| **Key Benefits** | ‚Ä¢ Rapid prototyping&lt;br&gt;‚Ä¢ Vendor‚Äëagnostic model switching&lt;br&gt;‚Ä¢ Built‚Äëin async/streaming via LCEL | ‚Ä¢ Explicit state management&lt;br&gt;‚Ä¢ Conditional edges & loops&lt;br&gt;‚Ä¢ Persistence & checkpointing for long‚Äërunning agents | ‚Ä¢ End‚Äëto‚Äëend tracing&lt;br&gt;‚Ä¢ Prompt & model evaluation tools&lt;br&gt;‚Ä¢ Production‚Äëgrade monitoring & alerting |\n\n### TL;DR Summary\n- **LangChain** = *building blocks* for basic LLM apps.  \n- **LangGraph** = *orchestrator* for sophisticated, stateful, multi‚Äëagent workflows.  \n- **LangSmith** = *debugger & observability layer* that makes those apps production‚Äëready.\n\nIn practice, a typical production stack uses **all three together**: build with LangChain, orchestrate complex flows with LangGraph, and monitor/evaluate everything with LangSmith."
  },
  {
    "objectID": "L01/01_agents.html#key-takeaways",
    "href": "L01/01_agents.html#key-takeaways",
    "title": "Agents in LangChain",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThree key methods for models: invoke, stream, and batch.\nLLMs can be configured to responsd in a structured format\nAgent = Model + Tools\nModels (LLMs) are the brain-power of agents\nTools connect the LLM to the outside world."
  },
  {
    "objectID": "L01/01_agents.html#activity",
    "href": "L01/01_agents.html#activity",
    "title": "Agents in LangChain",
    "section": "Activity",
    "text": "Activity\nOver to you: The previous model returned one search result. Can you make an Agent that uses the content from the top-3 ranking sites, to answer the question?"
  },
  {
    "objectID": "L01/10_langsmith.html",
    "href": "L01/10_langsmith.html",
    "title": "What is LangSmith?",
    "section": "",
    "text": "Know what your agents are really doing\nLangSmith Observability gives you complete visibility into agent behavior with tracing, real-time monitoring, alerting, and high-level insights into usage.\nWhat Is LangSmith? Explained in 5 Minutes.\n\n\nSign up and Set LangSmith API (Free)\n\nSign up for LangSmith here, find out more about LangSmith and how to use it within your workflow here.\nSet LANGSMITH_API_KEY, LANGSMITH_TRACING_V2=\"true\" LANGSMITH_PROJECT=\"langchain-academy\"in your environment\nIf you are on the EU instance also set LANGSMITH_ENDPOINT=‚Äúhttps://eu.api.smith.langchain.com‚Äù as well."
  },
  {
    "objectID": "L01/why_split.html",
    "href": "L01/why_split.html",
    "title": "Building Agentic AI Systems",
    "section": "",
    "text": "Why do we split? ..\n\nA. Navigating Context Window Limits\nEven the most advanced Large Language Models (LLMs) have a context window‚Äîa maximum number of tokens (words/characters) they can ‚Äúsee‚Äù at one time.\n\nThe Problem: Many documents (legal contracts, technical manuals, books) are significantly larger than an LLM‚Äôs context window.\nThe Solution: By splitting a 500-page manual into 500-word chunks, an agent can ‚Äúpull‚Äù only the relevant pieces into its memory to answer a specific query without hitting a technical ceiling.\n\n\n\nB. Improving Retrieval Accuracy (RAG)\nMost agentic systems use Retrieval-Augmented Generation (RAG). This process relies on converting text into ‚Äúembeddings‚Äù (mathematical vectors) to find similarities.\n\nSemantic Density: A 50-page document covers many different topics. If you turn that entire document into one vector, the ‚Äúmeaning‚Äù becomes diluted and fuzzy.\nPrecision: Chunking allows the agent to find the exact paragraph that answers a question. It‚Äôs easier for a system to match the query ‚ÄúWhat is the refund policy?‚Äù to a specific 200-word chunk about refunds than to a 10,000-word general Terms of Service document.\n\n\n\nC. Reducing ‚ÄúLost in the Middle‚Äù Phenomena\nResearch shows that LLMs are best at recalling information located at the very beginning or the very end of a prompt. Information buried in the middle of a massive block of text is often ignored or ‚Äúhallucinated‚Äù away.\nBy providing the agent with several small, concise chunks, you ensure the relevant information stays at the ‚Äútop of mind‚Äù for the model, leading to higher reasoning accuracy.\n\n\nD. Cost and Latency Optimization\nEvery token sent to an LLM costs money and takes time to process.\n\nEfficiency: If an agent needs to know a specific date in a 100MB file, sending the whole file is expensive and slow.\nSpeed: Sending three 500-token chunks is nearly instantaneous and costs a fraction of the price, allowing the agent to move through its task pipeline much faster."
  },
  {
    "objectID": "L01/08_workflow_patterns.html",
    "href": "L01/08_workflow_patterns.html",
    "title": "Workflows and Agents",
    "section": "",
    "text": "This guide reviews common workflow and agent patterns.\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment."
  },
  {
    "objectID": "L01/08_workflow_patterns.html#setup",
    "href": "L01/08_workflow_patterns.html#setup",
    "title": "Workflows and Agents",
    "section": "Setup",
    "text": "Setup\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\n\nInstall dependencies:\n\nuv add langchain_core langchain-anthropic langgraph\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# We use OpenRouter for the agent ‚Äî set OPENROUTER_API_KEY in .env\n# Get your key at https://openrouter.ai/keys\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    raise RuntimeError(\n        \"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:\\n\"\n        \"OPENROUTER_API_KEY=your-openrouter-api-key\"\n    )\n\n\nInitialize the LLM:\n\n\nfrom langchain_openai import ChatOpenAI\n\n# https://openrouter.ai/openai/gpt-5-nano\n# model_gpt5_nano = ChatOpenAI(\n#     model=\"openai/gpt-5-nano\",\n#     temperature=0,\n#     base_url=\"https://openrouter.ai/api/v1\",\n#     api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n# )\n\n# https://openrouter.ai/nvidia/nemotron-3-nano-30b-a3b:free\nllm = ChatOpenAI(\n    model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n    temperature=0,\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n)\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "L01/08_workflow_patterns.html#langgraph",
    "href": "L01/08_workflow_patterns.html#langgraph",
    "title": "Workflows and Agents",
    "section": "LangGraph",
    "text": "LangGraph\nLangGraph provides two different APIs to build agent workflows: the Graph API and the Functional API. Both APIs share the same underlying runtime and can be used together in the same application, but they are designed for different use cases and development preferences.\nBottom line: Use the Graph when your logic looks like a web, and the Functional when it looks like a list. For more details checkout the comparison here.\n\nUnderstanding the Functional API of LangGraph\nThe Functional API allows you to add LangGraph‚Äôs key features ‚Äî persistence, memory, human-in-the-loop, and streaming ‚Äî to your applications with minimal changes to your existing code.\nIt is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\nThe Functional API uses two key building blocks:\n\n@entrypoint ‚Äì Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\n@task ‚Äì Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\n\nThis provides a minimal abstraction for building workflows with state management and streaming.\nSee: The Functional API overview for more informatino.\n\n\nCore benefits of LangGraph\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\n\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows."
  },
  {
    "objectID": "L01/08_workflow_patterns.html#prompt-chaining",
    "href": "L01/08_workflow_patterns.html#prompt-chaining",
    "title": "Workflows and Agents",
    "section": "Prompt chaining",
    "text": "Prompt chaining\nPrompt chaining is when each LLM call processes the output of the previous call. It‚Äôs often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\n\nTranslating documents into different languages\nVerifying generated content for consistency\n\n\n\nfrom langgraph.func import task\n\n\n# Tasks\n@task\ndef generate_joke(topic: str):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\n    return msg.content\n\n\ndef check_punchline(joke: str):\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\n    # Simple check - does the joke contain \"?\" or \"!\"\n    if \"?\" in joke or \"!\" in joke:\n        return \"Fail\"\n\n    return \"Pass\"\n\n\n@task\ndef improve_joke(joke: str):\n    \"\"\"Second LLM call to improve the joke\"\"\"\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\n    return msg.content\n\n\n@task\ndef polish_joke(joke: str):\n    \"\"\"Third LLM call for final polish\"\"\"\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\n    return msg.content\n\n\nfrom langgraph.func import entrypoint\n\n@entrypoint()\ndef prompt_chaining_workflow(topic: str):\n    original_joke = generate_joke(topic).result()\n    if check_punchline(original_joke) == \"Pass\":\n        return original_joke\n\n    improved_joke = improve_joke(original_joke).result()\n    return polish_joke(improved_joke).result()\n\n\n# Invoke\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n\n{'generate_joke': 'Here\\'s a short, purr-fect joke for you:  \\n\\n&gt; *My cat knocked over my coffee.  \\n&gt; It was purr-fect.* üò∏  \\n\\n*(Bonus: It‚Äôs short, uses a cat pun, and the \"purr-fect\" twist lands in 5 words!)*'}\n\n\n{'improve_joke': '**My cat knocked over my coffee‚Äîtalk about a *purr‚Äëfect* disaster!**  \\nNow I‚Äôm *espresso‚Äëly* cat‚Äëastrophic. ‚òïüò∏  \\n\\n*(Wordplay added: ‚Äúpurr‚Äëfect‚Äù ‚Üí perfect, ‚Äúespresso‚Äëly‚Äù ‚Üí especially, ‚Äúcat‚Äëastrophic‚Äù ‚Üí catastrophic.)*'}\n\n\n{'polish_joke': '**My cat knocked over my coffee‚Äîtalk about a *purr‚Äëfect* disaster!**  \\nNow I‚Äôm *espresso‚Äëly* cat‚Äëastrophic. ‚òïüò∏  \\n\\n*But here‚Äôs the twist:* the little furball didn‚Äôt just spill the brew‚Äîhe **re‚Äëprogrammed the coffee maker to dispense catnip instead of caffeine**.  \\n\\nSo now every time I reach for a pick‚Äëme‚Äëup, I‚Äôm actually getting a **‚Äúpurr‚Äëcasso‚Äù** of espresso‚Äëinfused catnip, and the cat‚Äôs proudly serving it up with a side of whisker‚Äëtwitching swagger.  \\n\\n*Bottom line:* I‚Äôm not just *cat‚Äëastrophic* anymore‚ÄîI‚Äôm **caffeinated‚Äëand‚Äëcat‚Äëified**. üêæ‚ú®'}\n\n\n{'prompt_chaining_workflow': '**My cat knocked over my coffee‚Äîtalk about a *purr‚Äëfect* disaster!**  \\nNow I‚Äôm *espresso‚Äëly* cat‚Äëastrophic. ‚òïüò∏  \\n\\n*But here‚Äôs the twist:* the little furball didn‚Äôt just spill the brew‚Äîhe **re‚Äëprogrammed the coffee maker to dispense catnip instead of caffeine**.  \\n\\nSo now every time I reach for a pick‚Äëme‚Äëup, I‚Äôm actually getting a **‚Äúpurr‚Äëcasso‚Äù** of espresso‚Äëinfused catnip, and the cat‚Äôs proudly serving it up with a side of whisker‚Äëtwitching swagger.  \\n\\n*Bottom line:* I‚Äôm not just *cat‚Äëastrophic* anymore‚ÄîI‚Äôm **caffeinated‚Äëand‚Äëcat‚Äëified**. üêæ‚ú®'}"
  },
  {
    "objectID": "L01/08_workflow_patterns.html#parallelization",
    "href": "L01/08_workflow_patterns.html#parallelization",
    "title": "Workflows and Agents",
    "section": "Parallelization",
    "text": "Parallelization\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\n\nSplit up subtasks and run them in parallel, which increases speed\nRun tasks multiple times to check for different outputs, which increases confidence\n\nSome examples include:\n\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\n\n\n\n@task\ndef call_llm_1(topic: str):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n    msg = llm.invoke(f\"Write a joke about {topic}\")\n    return msg.content\n\n\n@task\ndef call_llm_2(topic: str):\n    \"\"\"Second LLM call to generate story\"\"\"\n    msg = llm.invoke(f\"Write a story about {topic}\")\n    return msg.content\n\n\n@task\ndef call_llm_3(topic):\n    \"\"\"Third LLM call to generate poem\"\"\"\n    msg = llm.invoke(f\"Write a poem about {topic}\")\n    return msg.content\n\n\n@task\ndef aggregator(topic, joke, story, poem):\n    \"\"\"Combine the joke and story into a single output\"\"\"\n\n    combined = f\"Here's a story, joke, and poem about {topic}!\\n\\n\"\n    combined += f\"STORY:\\n{story}\\n\\n\"\n    combined += f\"JOKE:\\n{joke}\\n\\n\"\n    combined += f\"POEM:\\n{poem}\"\n    return combined\n\n\n# Build workflow\n@entrypoint()\ndef parallel_workflow(topic: str):\n    joke_fut = call_llm_1(topic)\n    story_fut = call_llm_2(topic)\n    poem_fut = call_llm_3(topic)\n    return aggregator(\n        topic,\n        joke_fut.result(),\n        story_fut.result(),\n        poem_fut.result()\n    ).result()\n\n\n# Invoke\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n\n{'call_llm_3': '**Whiskers in the Moonlight**\\n\\nIn the hush of night‚Äôs soft sigh,  \\nA shadow slips on velvet paws‚Äî  \\nEyes like amber lanterns high,  \\nA silent hunter, caught in awe.\\n\\nShe curls around the world‚Äôs warm seam,  \\nA purr that rolls like rolling tide;  \\nEach ripple sings a secret dream,  \\nA lullaby where hearts can hide.\\n\\nShe stalks the sunbeams on the sill,  \\nA tiger in a tuxedoed coat;  \\nShe leaps, she lands, she never will‚Äî  \\nMiss a beat, she owns the float.\\n\\nHer tail, a question mark, unfurls,  \\nA comet tracing lazy arcs;  \\nShe paints the air with silent swirls,  \\nAnd leaves a trail of quiet sparks.\\n\\nWhen dawn awakes with amber glow,  \\nShe stretches, yawns, and claims the day;  \\nA regal queen of softest glow,  \\nShe rules the world in whiskered sway.\\n\\nSo here‚Äôs to cats‚Äîboth shy and bold‚Äî  \\nThe poets of the feline kind;  \\nIn every purr, a story told,  \\nA mystery we‚Äôll never fully find.'}\n\n\n{'call_llm_1': \"Here's a purr-fectly simple one for you:  \\n\\n&gt; *Why did the cat get kicked out of the party?*  \\n&gt; *Because it kept knocking over the punch bowl... and then *paw*-tying on the floor!* üò∏  \\n\\n*(Bonus groan: It was a *cat*-astrophe!)*\"}\n\n\n{'call_llm_2': '\\n**The Midnight Library**\\n\\nWhen the clock struck twelve in the sleepy town of Willowbrook, the old stone library on Main Street began to hum with a sound no one could quite place. It wasn‚Äôt the creak of the ancient wooden floorboards, nor the whisper of the wind through the cracked stained‚Äëglass windows. It was a soft, rhythmic purring that seemed to rise from the very shelves themselves.\\n\\nThe source of the purring was a sleek, silver‚Äëtabby cat named **Mira**. She had appeared one foggy evening a month earlier, slipping through the cracked door of the library as if she owned the place. The townsfolk had watched her with a mixture of curiosity and amusement as she padded between the rows of books, her tail flicking in time with the rustle of pages. She never knocked anything over, never scratched a single tome‚Äîshe simply settled herself on a high stool near the reference desk and began to read.\\n\\nMira‚Äôs eyes were a deep amber, and they seemed to glow whenever she turned a page. She would stare at the words as if they were tiny constellations, tracing their shapes with a paw that hovered just above the paper. The librarians, Mrs. Penelope Hargrove and her grandson, Theo, soon realized that Mira was not just any cat. She could understand the stories she read, and more astonishingly, she could *write* them.\\n\\nOne rainy night, as thunder rattled the panes, a stray kitten named **Pip** slipped into the library, shivering and soaked. Pip was a tiny, mottled gray furball with oversized ears that twitched at every sound. He tried to hide behind a stack of encyclopedias, but Mira‚Äôs gentle nudge guided him toward a warm spot on a plush armchair. She lowered her head and brushed her whiskers against his cheek, as if saying, ‚ÄúYou‚Äôre safe here.‚Äù\\n\\nThe next morning, Mrs. Hargrove found a handwritten note tucked between the pages of *The Secret Garden*. It read:\\n\\n&gt; *‚ÄúThe garden is not just a place of flowers, but a sanctuary for those who listen. Come, little one, and hear the stories the wind tells.‚Äù*\\n\\nShe looked up to see Mira perched on the arm of the chair, her tail curled around Pip, who was now curled up, eyes half‚Äëclosed, listening to the soft rustle of pages. The kitten‚Äôs ears perked up whenever a new sentence was spoken aloud, as if the words themselves were a lullaby.\\n\\nFrom that day on, the Midnight Library became a haven for more than just books. Animals of all kinds‚Äîsquirrels with bright eyes, a shy hedgehog named Quill, even an old barn owl that occasionally swooped in through the open window‚Äîfound their way to the quiet sanctuary. Each creature was greeted by Mira with a soft purr and a gentle nudge toward a spot where they could curl up and listen.\\n\\nMira‚Äôs true talent, however, was not just in reading or comforting. She could *weave* stories from the thoughts and feelings that swirled in the hearts of those who entered. When a child cried over a lost toy, Mira would curl up beside them and, with a flick of her tail, conjure a tale of a brave mouse who embarked on a daring rescue mission. When an elderly man sighed with nostalgia, she would settle on his lap and spin a yarn about a distant sea voyage that seemed to echo his own memories.\\n\\nOne evening, as the town prepared for its annual Harvest Festival, a sudden storm rolled in, threatening to cancel the celebrations. The townsfolk gathered in the library, worried that the rain would wash away their plans. Mira leapt onto the central reading table, her paws landing softly on a stack of old maps. She stared at the ceiling, then at the anxious faces around her, and began to purr‚Äîa deep, resonant sound that seemed to vibrate through the very walls.\\n\\nAs the purring grew louder, the lights flickered, and a soft glow began to emanate from the books themselves. The pages fluttered, and words rose off the paper like fireflies, forming a luminous tapestry across the ceiling. The story that unfolded was one of a brave cat who, during a storm, guided a lost flock of birds back to safety by leading them through a hidden tunnel beneath the town. The tale ended with a promise: *‚ÄúWhen the rain falls, the heart of the library shines brighter than any lantern.‚Äù*\\n\\nThe next morning, the storm had passed, and the sky cleared to a brilliant sunrise. The townspeople emerged to find the streets glistening, but more importantly, they found a renewed sense of hope. The Harvest Festival went ahead, brighter than ever, with lanterns hanging from the library‚Äôs windows, each one reflecting the story Mira had told.\\n\\nFrom that night on, Mira was no longer just a cat who liked to read. She became the **Guardian of Stories**, a silent protector who ensured that every heart that entered the library left with a tale to carry forward. And whenever a new creature‚Äîbe it a trembling kitten, a weary traveler, or a curious squirrel‚Äîstepped through the doors, they would find a warm spot, a gentle purr, and perhaps, if they listened closely, a story waiting to be written.\\n\\nAnd so, the Midnight Library continued to hum with the soft purring of a silver‚Äëtabby cat, its shelves alive with whispered adventures, and its heart forever open to the magic that only stories‚Äîand a few well‚Äëplaced purrs‚Äîcan bring.'}\n\n\n{'aggregator': \"Here's a story, joke, and poem about cats!\\n\\nSTORY:\\n\\n**The Midnight Library**\\n\\nWhen the clock struck twelve in the sleepy town of Willowbrook, the old stone library on Main Street began to hum with a sound no one could quite place. It wasn‚Äôt the creak of the ancient wooden floorboards, nor the whisper of the wind through the cracked stained‚Äëglass windows. It was a soft, rhythmic purring that seemed to rise from the very shelves themselves.\\n\\nThe source of the purring was a sleek, silver‚Äëtabby cat named **Mira**. She had appeared one foggy evening a month earlier, slipping through the cracked door of the library as if she owned the place. The townsfolk had watched her with a mixture of curiosity and amusement as she padded between the rows of books, her tail flicking in time with the rustle of pages. She never knocked anything over, never scratched a single tome‚Äîshe simply settled herself on a high stool near the reference desk and began to read.\\n\\nMira‚Äôs eyes were a deep amber, and they seemed to glow whenever she turned a page. She would stare at the words as if they were tiny constellations, tracing their shapes with a paw that hovered just above the paper. The librarians, Mrs. Penelope Hargrove and her grandson, Theo, soon realized that Mira was not just any cat. She could understand the stories she read, and more astonishingly, she could *write* them.\\n\\nOne rainy night, as thunder rattled the panes, a stray kitten named **Pip** slipped into the library, shivering and soaked. Pip was a tiny, mottled gray furball with oversized ears that twitched at every sound. He tried to hide behind a stack of encyclopedias, but Mira‚Äôs gentle nudge guided him toward a warm spot on a plush armchair. She lowered her head and brushed her whiskers against his cheek, as if saying, ‚ÄúYou‚Äôre safe here.‚Äù\\n\\nThe next morning, Mrs. Hargrove found a handwritten note tucked between the pages of *The Secret Garden*. It read:\\n\\n&gt; *‚ÄúThe garden is not just a place of flowers, but a sanctuary for those who listen. Come, little one, and hear the stories the wind tells.‚Äù*\\n\\nShe looked up to see Mira perched on the arm of the chair, her tail curled around Pip, who was now curled up, eyes half‚Äëclosed, listening to the soft rustle of pages. The kitten‚Äôs ears perked up whenever a new sentence was spoken aloud, as if the words themselves were a lullaby.\\n\\nFrom that day on, the Midnight Library became a haven for more than just books. Animals of all kinds‚Äîsquirrels with bright eyes, a shy hedgehog named Quill, even an old barn owl that occasionally swooped in through the open window‚Äîfound their way to the quiet sanctuary. Each creature was greeted by Mira with a soft purr and a gentle nudge toward a spot where they could curl up and listen.\\n\\nMira‚Äôs true talent, however, was not just in reading or comforting. She could *weave* stories from the thoughts and feelings that swirled in the hearts of those who entered. When a child cried over a lost toy, Mira would curl up beside them and, with a flick of her tail, conjure a tale of a brave mouse who embarked on a daring rescue mission. When an elderly man sighed with nostalgia, she would settle on his lap and spin a yarn about a distant sea voyage that seemed to echo his own memories.\\n\\nOne evening, as the town prepared for its annual Harvest Festival, a sudden storm rolled in, threatening to cancel the celebrations. The townsfolk gathered in the library, worried that the rain would wash away their plans. Mira leapt onto the central reading table, her paws landing softly on a stack of old maps. She stared at the ceiling, then at the anxious faces around her, and began to purr‚Äîa deep, resonant sound that seemed to vibrate through the very walls.\\n\\nAs the purring grew louder, the lights flickered, and a soft glow began to emanate from the books themselves. The pages fluttered, and words rose off the paper like fireflies, forming a luminous tapestry across the ceiling. The story that unfolded was one of a brave cat who, during a storm, guided a lost flock of birds back to safety by leading them through a hidden tunnel beneath the town. The tale ended with a promise: *‚ÄúWhen the rain falls, the heart of the library shines brighter than any lantern.‚Äù*\\n\\nThe next morning, the storm had passed, and the sky cleared to a brilliant sunrise. The townspeople emerged to find the streets glistening, but more importantly, they found a renewed sense of hope. The Harvest Festival went ahead, brighter than ever, with lanterns hanging from the library‚Äôs windows, each one reflecting the story Mira had told.\\n\\nFrom that night on, Mira was no longer just a cat who liked to read. She became the **Guardian of Stories**, a silent protector who ensured that every heart that entered the library left with a tale to carry forward. And whenever a new creature‚Äîbe it a trembling kitten, a weary traveler, or a curious squirrel‚Äîstepped through the doors, they would find a warm spot, a gentle purr, and perhaps, if they listened closely, a story waiting to be written.\\n\\nAnd so, the Midnight Library continued to hum with the soft purring of a silver‚Äëtabby cat, its shelves alive with whispered adventures, and its heart forever open to the magic that only stories‚Äîand a few well‚Äëplaced purrs‚Äîcan bring.\\n\\nJOKE:\\nHere's a purr-fectly simple one for you:  \\n\\n&gt; *Why did the cat get kicked out of the party?*  \\n&gt; *Because it kept knocking over the punch bowl... and then *paw*-tying on the floor!* üò∏  \\n\\n*(Bonus groan: It was a *cat*-astrophe!)*\\n\\nPOEM:\\n**Whiskers in the Moonlight**\\n\\nIn the hush of night‚Äôs soft sigh,  \\nA shadow slips on velvet paws‚Äî  \\nEyes like amber lanterns high,  \\nA silent hunter, caught in awe.\\n\\nShe curls around the world‚Äôs warm seam,  \\nA purr that rolls like rolling tide;  \\nEach ripple sings a secret dream,  \\nA lullaby where hearts can hide.\\n\\nShe stalks the sunbeams on the sill,  \\nA tiger in a tuxedoed coat;  \\nShe leaps, she lands, she never will‚Äî  \\nMiss a beat, she owns the float.\\n\\nHer tail, a question mark, unfurls,  \\nA comet tracing lazy arcs;  \\nShe paints the air with silent swirls,  \\nAnd leaves a trail of quiet sparks.\\n\\nWhen dawn awakes with amber glow,  \\nShe stretches, yawns, and claims the day;  \\nA regal queen of softest glow,  \\nShe rules the world in whiskered sway.\\n\\nSo here‚Äôs to cats‚Äîboth shy and bold‚Äî  \\nThe poets of the feline kind;  \\nIn every purr, a story told,  \\nA mystery we‚Äôll never fully find.\"}\n\n\n{'parallel_workflow': \"Here's a story, joke, and poem about cats!\\n\\nSTORY:\\n\\n**The Midnight Library**\\n\\nWhen the clock struck twelve in the sleepy town of Willowbrook, the old stone library on Main Street began to hum with a sound no one could quite place. It wasn‚Äôt the creak of the ancient wooden floorboards, nor the whisper of the wind through the cracked stained‚Äëglass windows. It was a soft, rhythmic purring that seemed to rise from the very shelves themselves.\\n\\nThe source of the purring was a sleek, silver‚Äëtabby cat named **Mira**. She had appeared one foggy evening a month earlier, slipping through the cracked door of the library as if she owned the place. The townsfolk had watched her with a mixture of curiosity and amusement as she padded between the rows of books, her tail flicking in time with the rustle of pages. She never knocked anything over, never scratched a single tome‚Äîshe simply settled herself on a high stool near the reference desk and began to read.\\n\\nMira‚Äôs eyes were a deep amber, and they seemed to glow whenever she turned a page. She would stare at the words as if they were tiny constellations, tracing their shapes with a paw that hovered just above the paper. The librarians, Mrs. Penelope Hargrove and her grandson, Theo, soon realized that Mira was not just any cat. She could understand the stories she read, and more astonishingly, she could *write* them.\\n\\nOne rainy night, as thunder rattled the panes, a stray kitten named **Pip** slipped into the library, shivering and soaked. Pip was a tiny, mottled gray furball with oversized ears that twitched at every sound. He tried to hide behind a stack of encyclopedias, but Mira‚Äôs gentle nudge guided him toward a warm spot on a plush armchair. She lowered her head and brushed her whiskers against his cheek, as if saying, ‚ÄúYou‚Äôre safe here.‚Äù\\n\\nThe next morning, Mrs. Hargrove found a handwritten note tucked between the pages of *The Secret Garden*. It read:\\n\\n&gt; *‚ÄúThe garden is not just a place of flowers, but a sanctuary for those who listen. Come, little one, and hear the stories the wind tells.‚Äù*\\n\\nShe looked up to see Mira perched on the arm of the chair, her tail curled around Pip, who was now curled up, eyes half‚Äëclosed, listening to the soft rustle of pages. The kitten‚Äôs ears perked up whenever a new sentence was spoken aloud, as if the words themselves were a lullaby.\\n\\nFrom that day on, the Midnight Library became a haven for more than just books. Animals of all kinds‚Äîsquirrels with bright eyes, a shy hedgehog named Quill, even an old barn owl that occasionally swooped in through the open window‚Äîfound their way to the quiet sanctuary. Each creature was greeted by Mira with a soft purr and a gentle nudge toward a spot where they could curl up and listen.\\n\\nMira‚Äôs true talent, however, was not just in reading or comforting. She could *weave* stories from the thoughts and feelings that swirled in the hearts of those who entered. When a child cried over a lost toy, Mira would curl up beside them and, with a flick of her tail, conjure a tale of a brave mouse who embarked on a daring rescue mission. When an elderly man sighed with nostalgia, she would settle on his lap and spin a yarn about a distant sea voyage that seemed to echo his own memories.\\n\\nOne evening, as the town prepared for its annual Harvest Festival, a sudden storm rolled in, threatening to cancel the celebrations. The townsfolk gathered in the library, worried that the rain would wash away their plans. Mira leapt onto the central reading table, her paws landing softly on a stack of old maps. She stared at the ceiling, then at the anxious faces around her, and began to purr‚Äîa deep, resonant sound that seemed to vibrate through the very walls.\\n\\nAs the purring grew louder, the lights flickered, and a soft glow began to emanate from the books themselves. The pages fluttered, and words rose off the paper like fireflies, forming a luminous tapestry across the ceiling. The story that unfolded was one of a brave cat who, during a storm, guided a lost flock of birds back to safety by leading them through a hidden tunnel beneath the town. The tale ended with a promise: *‚ÄúWhen the rain falls, the heart of the library shines brighter than any lantern.‚Äù*\\n\\nThe next morning, the storm had passed, and the sky cleared to a brilliant sunrise. The townspeople emerged to find the streets glistening, but more importantly, they found a renewed sense of hope. The Harvest Festival went ahead, brighter than ever, with lanterns hanging from the library‚Äôs windows, each one reflecting the story Mira had told.\\n\\nFrom that night on, Mira was no longer just a cat who liked to read. She became the **Guardian of Stories**, a silent protector who ensured that every heart that entered the library left with a tale to carry forward. And whenever a new creature‚Äîbe it a trembling kitten, a weary traveler, or a curious squirrel‚Äîstepped through the doors, they would find a warm spot, a gentle purr, and perhaps, if they listened closely, a story waiting to be written.\\n\\nAnd so, the Midnight Library continued to hum with the soft purring of a silver‚Äëtabby cat, its shelves alive with whispered adventures, and its heart forever open to the magic that only stories‚Äîand a few well‚Äëplaced purrs‚Äîcan bring.\\n\\nJOKE:\\nHere's a purr-fectly simple one for you:  \\n\\n&gt; *Why did the cat get kicked out of the party?*  \\n&gt; *Because it kept knocking over the punch bowl... and then *paw*-tying on the floor!* üò∏  \\n\\n*(Bonus groan: It was a *cat*-astrophe!)*\\n\\nPOEM:\\n**Whiskers in the Moonlight**\\n\\nIn the hush of night‚Äôs soft sigh,  \\nA shadow slips on velvet paws‚Äî  \\nEyes like amber lanterns high,  \\nA silent hunter, caught in awe.\\n\\nShe curls around the world‚Äôs warm seam,  \\nA purr that rolls like rolling tide;  \\nEach ripple sings a secret dream,  \\nA lullaby where hearts can hide.\\n\\nShe stalks the sunbeams on the sill,  \\nA tiger in a tuxedoed coat;  \\nShe leaps, she lands, she never will‚Äî  \\nMiss a beat, she owns the float.\\n\\nHer tail, a question mark, unfurls,  \\nA comet tracing lazy arcs;  \\nShe paints the air with silent swirls,  \\nAnd leaves a trail of quiet sparks.\\n\\nWhen dawn awakes with amber glow,  \\nShe stretches, yawns, and claims the day;  \\nA regal queen of softest glow,  \\nShe rules the world in whiskered sway.\\n\\nSo here‚Äôs to cats‚Äîboth shy and bold‚Äî  \\nThe poets of the feline kind;  \\nIn every purr, a story told,  \\nA mystery we‚Äôll never fully find.\"}"
  },
  {
    "objectID": "L01/08_workflow_patterns.html#routing",
    "href": "L01/08_workflow_patterns.html#routing",
    "title": "Workflows and Agents",
    "section": "Routing",
    "text": "Routing\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\n\n\nfrom typing_extensions import Literal\nfrom pydantic import BaseModel, Field\nfrom langchain.messages import HumanMessage, SystemMessage\n\n\n# Schema for structured output to use as routing logic\nclass Route(BaseModel):\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n        None, description=\"The next step in the routing process\"\n    )\n\n# Augment the LLM with schema for structured output\nrouter = llm.with_structured_output(Route)\n\ndef llm_call_router(input_: str):\n    \"\"\"Route the input to the appropriate node\"\"\"\n    # Run the augmented LLM with structured output to serve as routing logic\n    decision = router.invoke(\n        [\n            SystemMessage(\n                content=\"Route the input to story, joke, or poem based on the user's request.\"\n            ),\n            HumanMessage(content=input_),\n        ]\n    )\n    return decision.step\n\n\n@task\ndef llm_call_1(input_: str):\n    \"\"\"Write a story\"\"\"\n    result = llm.invoke(input_)\n    return result.content\n\n\n@task\ndef llm_call_2(input_: str):\n    \"\"\"Write a joke\"\"\"\n    result = llm.invoke(input_)\n    return result.content\n\n\n@task\ndef llm_call_3(input_: str):\n    \"\"\"Write a poem\"\"\"\n    result = llm.invoke(input_)\n    return result.content\n\n\n# Create workflow\n@entrypoint()\ndef router_workflow(input_: str):\n    next_step = llm_call_router(input_)\n    if next_step == \"story\":\n        llm_call = llm_call_1\n    elif next_step == \"joke\":\n        llm_call = llm_call_2\n    elif next_step == \"poem\":\n        llm_call = llm_call_3\n\n    return llm_call(input_).result()\n\n\n# Invoke\nfor step in router_workflow.stream(\"Tell me a joke about cats\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=Route(step='joke'), input_type=Route])\n  return self.__pydantic_serializer__.to_python(\n\n\n{'llm_call_2': \"Here's a classic cat joke that‚Äôs purr-fect for any cat lover:  \\n\\n&gt; **Why did the cat sit on the computer?**  \\n&gt; *Because it wanted to keep an eye on the mouse!* üòº  \\n\\n*(Bonus groan: Because it heard the mouse was *running* the system!)*  \\n\\nHope that gives you a little *purr* of laughter! üêæ\"}\n\n\n{'router_workflow': \"Here's a classic cat joke that‚Äôs purr-fect for any cat lover:  \\n\\n&gt; **Why did the cat sit on the computer?**  \\n&gt; *Because it wanted to keep an eye on the mouse!* üòº  \\n\\n*(Bonus groan: Because it heard the mouse was *running* the system!)*  \\n\\nHope that gives you a little *purr* of laughter! üêæ\"}"
  },
  {
    "objectID": "L01/08_workflow_patterns.html#orchestrator-worker",
    "href": "L01/08_workflow_patterns.html#orchestrator-worker",
    "title": "Workflows and Agents",
    "section": "Orchestrator-worker",
    "text": "Orchestrator-worker\nIn an orchestrator-worker configuration, the orchestrator:\n\nBreaks down tasks into subtasks\nDelegates subtasks to workers\nSynthesizes worker outputs into a final result\n\n\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\n\nfrom typing import List\n\n\n# Schema for structured output to use in planning\nclass Section(BaseModel):\n    name: str = Field(\n        description=\"Name for this section of the report.\",\n    )\n    description: str = Field(\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n    )\n\n\nclass Sections(BaseModel):\n    sections: List[Section] = Field(\n        description=\"Sections of the report.\",\n    )\n\n\n# Augment the LLM with schema for structured output\nplanner = llm.with_structured_output(Sections)\n\n\n@task\ndef orchestrator(topic: str):\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n    # Generate queries\n    report_sections = planner.invoke(\n        [\n            SystemMessage(content=\"Generate a plan for the report.\"),\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\n        ]\n    )\n\n    return report_sections.sections\n\n\n@task\ndef llm_call(section: Section):\n    \"\"\"Worker writes a section of the report\"\"\"\n\n    # Generate section\n    result = llm.invoke(\n        [\n            SystemMessage(content=\"Write a report section.\"),\n            HumanMessage(\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\n            ),\n        ]\n    )\n\n    # Write the updated section to completed sections\n    return result.content\n\n\n@task\ndef synthesizer(completed_sections: list[str]):\n    \"\"\"Synthesize full report from sections\"\"\"\n    final_report = \"\\n\\n---\\n\\n\".join(completed_sections)\n    return final_report\n\n\n@entrypoint()\ndef orchestrator_worker(topic: str):\n    sections = orchestrator(topic).result()\n    section_futures = [llm_call(section) for section in sections]\n    final_report = synthesizer(\n        [section_fut.result() for section_fut in section_futures]\n    ).result()\n    return final_report\n\n\n# Invoke\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=Sections(sections=[Sectio...ary of abbreviations')]), input_type=Sections])\n  return self.__pydantic_serializer__.to_python(\n\n\n\nfrom IPython.display import Markdown\n\n\nMarkdown(report)\n\nExecutive Summary\nPurpose\nThis report provides a comprehensive analysis of the current market landscape for renewable energy adoption in emerging economies, evaluates the performance of key policy initiatives, and assesses the financial viability of proposed investment strategies. Its primary objective is to equip policymakers, investors, and development agencies with actionable insights that can accelerate the transition to sustainable energy systems while fostering economic growth.\nKey Findings\n- Rapid Growth Potential: Emerging markets collectively possess an estimated 1.2‚ÄØTW of untapped renewable capacity, with solar and wind accounting for 68‚ÄØ% of the projected expansion.\n- Policy Impact: Countries that have implemented stable feed‚Äëin tariffs and streamlined permitting processes have seen a 35‚ÄØ% increase in renewable project completions within two years, compared with a 12‚ÄØ% rise in nations lacking such frameworks.\n- Economic Benefits: Transitioning to a 30‚ÄØ% renewable energy mix could generate up to 4.5‚ÄØmillion new jobs, reduce energy import bills by $18‚ÄØbillion annually, and lower CO‚ÇÇ emissions by 1.1‚ÄØGt‚ÄØCO‚ÇÇe per year.\n- Financial Viability: The levelized cost of electricity (LCOE) for utility‚Äëscale solar has fallen to $0.028‚ÄØ/kWh, making it competitive with fossil‚Äëfuel generation in 14 of the 20 studied economies.\n- Barriers to Scale: Limited grid infrastructure, fragmented financing mechanisms, and insufficient local technical expertise remain the most significant obstacles to scaling up renewable projects.\nRecommendations\n1. Establish Predictable Policy Frameworks: Governments should adopt long‚Äëterm renewable energy targets, stable feed‚Äëin tariffs, and transparent permitting processes to attract private capital.\n2. Mobilize Blended Finance: Leverage public‚Äësector guarantees and concessional loans to de‚Äërisk private investments, particularly in early‚Äëstage projects and emerging technologies such as storage and green hydrogen.\n3. Strengthen Grid Resilience: Prioritize investments in transmission upgrades and smart‚Äëgrid technologies to integrate variable renewable sources and ensure reliable supply.\n4. Build Local Capacity: Implement training programs and incentives for domestic firms to develop expertise in renewable installation, operation, and maintenance, thereby creating a self‚Äësustaining industry ecosystem.\n5. Promote Regional Cooperation: Facilitate cross‚Äëborder power trade and joint research initiatives to share best practices, reduce costs, and maximize resource utilization across neighboring economies.\nBy implementing these targeted actions, stakeholders can unlock the full economic and environmental potential of renewable energy in emerging markets, driving sustainable development and fostering inclusive prosperity.\n\n1. Introduction and Description: Context and Motivation for Studying LLM Scaling Laws; Objectives and Scope\n\n\n1.1. Background and Motivation\nThe performance of large language models (LLMs) exhibits a remarkably predictable dependence on three principal scaling factors: model size (parameter count), dataset size, and compute budget (often measured in FLOPs). Empirical studies‚Äîmost notably the ‚Äúscaling laws‚Äù first formalized by Kaplan et‚ÄØal. (2020) and subsequently refined by a growing body of work‚Äîhave demonstrated that, within certain regimes, the error of a model scales as a power‚Äëlaw function of these variables. This regularity has profound implications:\n\nPredictive Power: It enables researchers and practitioners to forecast the resources required to achieve a target level of performance, guiding efficient allocation of compute and data.\n\nDesign Guidance: Scaling laws inform architectural decisions (e.g., depth vs.¬†width, token‚Äëmix strategies) and help prioritize research directions such as sparsity, mixture‚Äëof‚Äëexperts, or curriculum learning.\n\nEconomic & Ethical Considerations: Understanding the cost‚Äëperformance trade‚Äëoffs is essential for responsible deployment, budgeting, and assessing the environmental footprint of ever‚Äëlarger models.\n\nDespite their utility, existing scaling‚Äëlaw analyses are often limited to specific model families, training regimes, or evaluation metrics. Moreover, the rapid emergence of new model architectures (e.g., transformer‚Äëbased diffusion language models, retrieval‚Äëaugmented generators) and training paradigms (e.g., multi‚Äëtask fine‚Äëtuning, reinforcement learning from human feedback) raises questions about the generality and robustness of traditional scaling relationships.\n\n\n1.2. Objectives\nThe primary objective of this report is to systematically investigate the scaling behavior of contemporary LLMs across a broad spectrum of model sizes, data regimes, and compute budgets. Specifically, we aim to:\n\nQuantify Scaling Relationships ‚Äì Derive empirical power‚Äëlaw exponents for loss, downstream task performance, and inference latency as functions of parameter count, training token count, and FLOPs, respectively.\n\nAssess Regime Boundaries ‚Äì Identify the transition points between the pre‚Äëtraining, scaling, and post‚Äëtraining regimes, and examine how factors such as token‚Äëtype distribution, optimizer choice, and regularization affect these boundaries.\n\nEvaluate Generalization Across Architectures ‚Äì Test whether the identified scaling laws hold for diverse model families (e.g., dense transformers, sparsely‚Äëgated mixture‚Äëof‚Äëexperts, retrieval‚Äëaugmented models) and for a variety of downstream tasks (language modeling, reasoning, code generation, multilingual benchmarks).\n\nProvide Practical Recommendations ‚Äì Translate the findings into actionable guidance for model selection, data collection, and compute budgeting under fixed performance targets.\n\n\n\n1.3. Scope\nThe scope of this report is deliberately bounded to ensure depth and reproducibility:\n\n\n\n\n\n\n\n\nDimension\nInclusion\nExclusion\n\n\n\n\nModel Families\nDense transformer decoders (GPT‚Äëstyle) up to ~1‚ÄØT parameters; sparsely‚Äëgated MoE variants with up to ~10‚ÄØB active parameters; retrieval‚Äëaugmented generators with external knowledge bases.\nNon‚Äëtransformer architectures (e.g., recurrent, convolutional) and models that rely on fundamentally different tokenization schemes (e.g., byte‚Äëpair encoding vs.¬†character‚Äëlevel).\n\n\nTraining Regimes\nPre‚Äëtraining on curated web‚Äëscale corpora (English‚Äëcentric and multilingual); multi‚Äëtask fine‚Äëtuning; RLHF fine‚Äëtuning for alignment.\nTraining on proprietary, non‚Äëpublic datasets that are unavailable for audit; on‚Äëdevice continual learning beyond the pre‚Äëtraining phase.\n\n\nCompute & Data Metrics\nParameter count, total FLOPs, token count, and effective compute (measured in PF‚Äëdays).\nEnergy consumption beyond FLOP accounting, hardware‚Äëspecific latency measurements (unless explicitly tied to FLOP equivalence).\n\n\nEvaluation Metrics\nPer‚Äëtoken cross‚Äëentropy loss, perplexity, and a curated suite of downstream benchmarks (e.g., MMLU, GSM‚Äë8K, BIG‚ÄëBench, XGLUE).\nProprietary enterprise metrics that require confidential data or are not publicly benchmarked.\n\n\nTemporal Horizon\nModels released up to June‚ÄØ2024 (including publicly disclosed checkpoints).\nFuture models or those released after this date, unless they are open‚Äësource and meet the inclusion criteria.\n\n\n\nAll experiments reported herein will be reproducible using publicly available checkpoints and standard training scripts (e.g., Hugging Face Transformers, DeepSpeed, FairScale). Where proprietary data is used for illustrative purposes, we will provide synthetic proxies that preserve the statistical properties of the original corpora.\n\n\n1.4. Structure of the Report\nThe remainder of the report is organized as follows:\n\nRelated Work ‚Äì A review of seminal scaling‚Äëlaw studies, recent extensions, and gaps in the literature.\n\nExperimental Methodology ‚Äì Details on model configurations, data pipelines, training schedules, and evaluation protocols.\n\nEmpirical Findings ‚Äì Presentation and analysis of scaling exponents, regime transitions, and cross‚Äëarchitecture comparisons.\n\nDiscussion ‚Äì Interpretation of results, implications for model design and deployment, and limitations of the current study.\n\nConclusions and Recommendations ‚Äì Summary of key insights and actionable guidance for researchers and practitioners.\n\nBy systematically characterizing how performance scales with model size, data, and compute, this report seeks to provide a comprehensive, empirically grounded roadmap for leveraging scaling laws as a predictive tool in the development of next‚Äëgeneration LLMs.\n\n2. Background and Description\n\n\n\n2.1. Evolution of Large Language Models\nLarge language models (LLMs) are a class of neural‚Äënetwork‚Äëbased systems that have dramatically reshaped natural‚Äëlanguage processing (NLP) and, more broadly, artificial intelligence (AI) over the past decade. Their evolution can be traced through three interrelated milestones:\n\n\n\n\n\n\n\n\n\nMilestone\nYear\nModel / Architecture\nKey Advances\n\n\n\n\nEarly Distributed Representations\n2013‚Äë2015\nWord2Vec, GloVe, FastText\nIntroduced dense, context‚Äëaware embeddings that made vector‚Äëspace semantics tractable for downstream tasks.\n\n\nTransformer Paradigm\n2017\nAttention Is All You Need (Vaswani et‚ÄØal.)\nReplaced recurrent and convolutional layers with self‚Äëattention, enabling parallel computation and scalable context handling.\n\n\nPre‚Äëtraining at Scale\n2018‚Äë2020\nOpenAI GPT‚Äë1/2, Google BERT, Microsoft Turing‚ÄëNLG\nDemonstrated that massive unsupervised pre‚Äëtraining on heterogeneous text corpora yields emergent linguistic abilities that transfer to a wide range of downstream tasks.\n\n\nMassive Parameter Regimes\n2020‚Äë2023\nGPT‚Äë3 (175‚ÄØB), Megatron‚ÄëTuring‚ÄëNLG (530‚ÄØB), PaLM‚Äë2 (up to 540‚ÄØB)\nShowed that increasing model size‚Äîboth in parameters and training compute‚Äîproduces systematic gains in few‚Äëshot learning, reasoning, and multilingual competence.\n\n\nMultimodal & Structured Integration\n2023‚Äëpresent\nGPT‚Äë4‚ÄëV, LLaMA‚Äë2‚ÄëChat, Gemini, Claude‚Äë3\nExtends LLMs beyond pure text to incorporate images, code, tables, and structured knowledge, while refining alignment and safety mechanisms.\n\n\n\nThe trajectory is characterized not merely by a quantitative increase in parameter count, but by a qualitative shift in capability: from models that excel at narrow, supervised tasks to systems that exhibit emergent properties such as chain‚Äëof‚Äëthought reasoning, code synthesis, and cross‚Äëmodal understanding. This shift has been enabled by three synergistic developments:\n\nData‚Äëcentric scaling ‚Äì curated, high‚Äëquality corpora (e.g., The Pile, Common Crawl, filtered Wikipedia) that provide richer linguistic diversity.\n\nCompute‚Äëefficient training ‚Äì techniques such as mixed‚Äëprecision arithmetic, gradient checkpointing, and optimizer variants (e.g., AdamW) that make training billions of parameters feasible on commodity hardware clusters.\n\nArchitectural refinements ‚Äì layer‚Äënorm variants, rotary positional embeddings, and sparsity‚Äëaware attention mechanisms that improve stability and reduce memory footprints.\n\nCollectively, these advances have positioned LLMs as the foundational substrate for a new generation of AI‚Äëdriven applications, ranging from conversational agents and content generation to scientific discovery and automated reasoning.\n\n\n\n2.2. Definition of Scaling Laws\nScaling laws are empirical relationships that describe how the performance of a neural‚Äënetwork model‚Äîtypically measured by a downstream benchmark metric‚Äîimproves as a function of three controllable resources:\n\nModel size ‚Äì usually expressed in terms of the number of parameters, (N).\n\nTraining compute ‚Äì the total amount of floating‚Äëpoint operations (FLOPs) expended during training, (C).\n\nDataset size ‚Äì the number of training tokens or examples, (D).\n\nIn their simplest form, scaling laws can be written as:\n[ (N, C, D) A , N^{-} , C^{-} , D^{-}, ]\nwhere () denotes the loss (or error) on a held‚Äëout validation set, and (A, , , ) are positive constants estimated from experimental data. More commonly, researchers express error (e.g., perplexity) as a power‚Äëlaw function of the effective compute per parameter:\n[ ()^{-}, ]\nwith () representing the scaling exponent that captures the diminishing returns of adding more compute.\nKey properties of these laws include:\n\nPower‚Äëlaw behavior: Performance improves smoothly and predictably as a function of scale, rather than exhibiting abrupt phase transitions.\n\nOptimal allocation: Given a fixed budget (B = C N), the error is minimized when compute and model size are balanced according to the exponents (, ).\n\nGeneralization to new tasks: Scaling laws observed on language‚Äëmodel pre‚Äëtraining loss often transfer to downstream few‚Äëshot performance, suggesting that the same underlying resource‚Äìerror relationship governs both pre‚Äëtraining and fine‚Äëtuning regimes.\n\nThese empirical regularities have become a guiding principle for research planning, allowing practitioners to forecast the trade‚Äëoffs between model size, data collection, and compute allocation before committing to expensive training runs.\n\n\n\n2.3. Historical Perspective: Power‚ÄëLaw Relationships in AI\nThe notion that complex systems exhibit power‚Äëlaw scaling predates modern deep learning and has recurrently surfaced across AI subfields:\n\n\n\n\n\n\n\n\n\nEra\nDomain\nPower‚ÄëLaw Manifestation\nInsight Gained\n\n\n\n\n1970s‚Äì1980s\nStatistical Physics\nDistribution of energy states in Ising models\nIntroduced the concept of scale‚Äëfree behavior, later adapted to characterize parameter distributions in neural networks.\n\n\n1990s\nConnectionist Learning\nScaling of required training examples with network depth\nEarly work on capacity showed that the number of trainable parameters must grow polynomially with task complexity.\n\n\n2000s\nSpeech Recognition\nRelationship between acoustic model size and word error rate\nDemonstrated that larger acoustic models reduced error roughly as a power of model size, foreshadowing later LLM scaling.\n\n\n2010s\nImage Classification\nAccuracy vs.¬†number of layers / filters\nEmpirical studies (e.g., Krizhevsky et‚ÄØal., 2012) revealed diminishing error improvements with additional layers, prompting the adoption of residual connections and deeper architectures.\n\n\n2020s\nLarge Language Models\nLoss vs.¬†parameters, tokens, and FLOPs\nSystematic studies (e.g., Kaplan et‚ÄØal., 2020; Hoffmann et‚ÄØal., 2022) quantified scaling exponents, establishing that model performance follows a predictable power‚Äëlaw with respect to each resource dimension.\n\n\n\nThe historical thread linking these observations is the recurring pattern that error or error‚Äërelevant metrics decrease as a power of the underlying resource. In early AI, this manifested as a need for exponentially more training data to achieve linear gains in accuracy. With the advent of deep, over‚Äëparameterized networks, the relationship softened to a polynomial (often square‚Äëroot) scaling, enabling more efficient utilization of compute.\nThe modern scaling law literature formalizes this intuition:\n\nKaplan et‚ÄØal.¬†(2020) introduced a simple power‚Äëlaw model linking loss to model size, dataset size, and compute, showing that optimal performance is achieved when (N C^{1/2}) and (D N).\n\nHoffmann et‚ÄØal.¬†(2022) extended the analysis to the Chinchilla regime, proving that beyond a certain point, allocating more compute to data yields greater returns than enlarging the model.\n\nChinchilla & PaLM‚Äë2 studies empirically validated that training a 70‚ÄØB‚Äëparameter model on 1.4‚ÄØ√ó‚ÄØthe data used for a 175‚ÄØB model yields comparable downstream performance, underscoring the practical relevance of scaling‚Äëlaw‚Äëguided resource allocation.\n\nThese historical insights collectively illustrate a unifying principle: the performance of AI systems obeys power‚Äëlaw scaling with respect to the fundamental resources of model capacity, data, and compute. Recognizing and leveraging this principle has become a cornerstone of contemporary AI research, informing everything from architecture design to budgeting of large‚Äëscale training campaigns.\n\nThe above subsections synthesize the current scholarly understanding of how large language models have evolved, how scaling laws formalize the relationship between resources and performance, and how power‚Äëlaw scaling has recurred throughout the broader history of artificial intelligence.\n\n3. Theoretical Foundations and Description\nThe performance of complex engineered and natural systems is frequently observed to obey scaling relationships that can be captured succinctly by power‚Äëlaw functions. In this section we lay out the mathematical scaffolding that underpins our analysis, beginning with the formulation of power‚Äëlaw models for performance versus resource metrics, followed by a systematic derivation of the associated scaling exponents, and finally by situating these results within the broader frameworks of statistical mechanics and information theory.\n\n\n\n3.1. Power‚Äëlaw Modeling of Performance vs.¬†Resource Metrics\nLet (P) denote a performance indicator (e.g., throughput, error rate, energy consumption) and let (R) represent a measurable resource input (e.g., number of processing nodes, bandwidth, material stock). Empirical observations across a wide class of systems reveal that, over a broad intermediate regime, the relationship can be approximated by\n[ P(R) ;; C,R^{},, ]\nwhere\n\n(C&gt;0) is a system‚Äëspecific prefactor that encapsulates baseline efficiency, design constants, or normalization factors, and\n\n() is the scaling exponent that quantifies how sensitively performance responds to changes in the resource pool.\n\nEquation (3.1) is deliberately generic; specific instantiations may involve logarithmic corrections, cut‚Äëoffs, or multi‚Äëscale regimes, but the power‚Äëlaw form remains the leading-order approximation in the asymptotic limit of large (R). The logarithm of both sides yields a linear relationship amenable to regression:\n[ P = C + R . ]\nThus, a log‚Äìlog plot of (P) versus (R) should exhibit a straight line with slope () in the scaling window, providing a straightforward diagnostic for power‚Äëlaw behavior.\n\n\n\n3.2. Derivation of Scaling Exponents\nTo extract () analytically, we consider a representative stochastic growth process that is known to generate power‚Äëlaw asymptotics. Suppose the incremental improvement (P) obtained by adding a marginal amount (R) of resource follows a scale‚Äëinvariant rule\n[ P ;; (R)^{},, ]\nwith () a characteristic exponent of the underlying dynamics. In a continuous limit, the differential form\n[ ;; R^{} ]\nintegrates to\n[ P(R) ;; ^{R} R‚Äô^{},dR‚Äô ;; R^{},, ]\nprovided the integration starts from a non‚Äëzero lower bound and the upper bound lies within the asymptotic regime. Consequently, the scaling exponent governing the performance‚Äìresource relationship is simply\n[ . ]\nIn many models‚Äîsuch as preferential attachment, self‚Äëorganized criticality, or queueing networks with heavy‚Äëtailed service times‚Äî() can be derived from first principles. For instance, in a preferential‚Äëattachment process where the probability of acquiring additional resources is proportional to the current performance, one obtains (= ), leading to (= ). In queueing systems with Poisson arrivals and exponential service times, the exponent often emerges as (= 1 - ) where (k) is the shape parameter of the service‚Äëtime distribution. These derivations illustrate how the exponent is not an empirical fitting parameter per se, but rather a fingerprint of the underlying microscopic dynamics.\n\n\n\n3.3. Connection to Statistical Mechanics and Information Theory\nThe power‚Äëlaw form (3.1) resonates deeply with concepts from statistical mechanics and information theory, where scale invariance and entropy maximization give rise to analogous scaling laws.\n\nStatistical Mechanics Perspective ‚Äì Near critical points, macroscopic observables often exhibit power‚Äëlaw dependencies on control parameters (e.g., magnetization vs.¬†temperature). The renormalization‚Äëgroup (RG) framework explains that such dependencies are universal, arising from the fixed‚Äëpoint structure of the RG flow. By mapping the resource variable (R) onto a temperature‚Äëlike control parameter and the performance variable (P) onto an order parameter, the exponent () can be identified with a critical exponent associated with a relevant RG eigenvalue. This viewpoint justifies the robustness of power‚Äëlaw scaling across disparate domains: the same universality class yields the same () irrespective of microscopic details.\nInformation‚ÄëTheoretic Perspective ‚Äì From the standpoint of Shannon entropy, the distribution of resource allocations that maximizes entropy under constraints of fixed mean and variance is a power‚Äëlaw (Pareto) distribution. When performance is interpreted as a function of the entropy of the underlying stochastic process, the scaling exponent () can be linked to the exponent governing the tail of this entropy distribution. Moreover, the Kolmogorov‚ÄìSinai entropy of a dynamical system quantifies the rate of information production; in systems where information production scales sub‚Äëlinearly with resource consumption, the exponent () emerges as the ratio of information‚Äëproduction rate to resource‚Äëconsumption rate. Thus, () can be interpreted as a measure of efficiency of information processing in the system.\n\nThese connections provide a unifying lens: the power‚Äëlaw exponent is not merely a phenomenological fit but a manifestation of deep structural properties‚Äîscale invariance, critical fluctuations, and optimal information encoding‚Äîthat are common to many complex systems.\n\nSummary ‚Äì Section‚ÄØ3.1 introduced the generic power‚Äëlaw ansatz (P(R)=C R^{}) and highlighted its diagnostic utility via log‚Äìlog linearization. Section‚ÄØ3.2 demonstrated how () can be derived from scale‚Äëinvariant growth dynamics, establishing a direct link to microscopic exponents (). Finally, Section‚ÄØ3.3 situated these results within the theoretical constructs of statistical mechanics (critical phenomena, renormalization‚Äëgroup universality) and information theory (entropy maximization, information‚Äëproduction rates), underscoring the profound conceptual underpinnings of the observed scaling behavior.\nThese foundations set the stage for the empirical analysis presented in the subsequent sections, where we validate the power‚Äëlaw predictions against experimental data and explore the implications of the derived exponents for system design and optimization.\n\n4. Empirical Evidence and Description\nThe empirical foundation of this study rests on a systematic exploration of how three core axes of model design‚Äîtraining compute, model size, and data characteristics‚Äîinteract with downstream performance across a spectrum of benchmark tasks. The evidence presented below draws on a curated set of experiments that span from controlled ablations to large‚Äëscale case studies of contemporary foundation models. Each subsection details the methodology, key observations, and their implications for scaling laws and practical deployment.\n\n\n\n4.1. Training Compute vs.¬†Validation Loss Curves\nObjective. To quantify the relationship between the total amount of compute expended during pre‚Äëtraining (measured in FLOPs) and the achievable validation loss on a held‚Äëout dataset.\nMethodology.\n- A series of transformer‚Äëbased models were trained from scratch on the same base corpus (e.g., a 300‚ÄØB‚Äëtoken English text collection).\n- Compute budgets were selected to span three orders of magnitude: 10‚Åπ, 10¬π‚Å∞, 10¬π¬π, 10¬π¬≤, and 10¬π¬≥ FLOPs.\n- For each budget, training was run until either a fixed number of epochs or a target loss plateau was reached; early‚Äëstopping was applied based on a moving‚Äëaverage of validation loss.\n- Validation loss was recorded at regular intervals (every 0.1‚ÄØ% of total compute) to generate smooth loss curves.\nKey Findings.\n| Compute (FLOPs) | Validation Loss (perplexity) | Observed Trend | |‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî‚Äî-| | 10‚Åπ | 150‚ÄØ√ó | High variance, unstable training | | 10¬π‚Å∞ | 45‚ÄØ√ó | Rapid initial improvement, diminishing returns after ~5‚ÄØB tokens | | 10¬π¬π | 22‚ÄØ√ó | Near‚Äëlinear reduction in loss up to ~10‚ÄØB tokens | | 10¬π¬≤ | 12‚ÄØ√ó | Plateau begins; additional compute yields &lt;0.5‚ÄØ√ó loss reduction | | 10¬π¬≥ | 11‚ÄØ√ó | Marginal gain; marginal cost increase &gt;10√ó |\n\nPower‚Äëlaw behavior: The log‚Äëlog plot of validation loss versus compute follows a slope of approximately ‚Äì0.07, consistent with prior scaling‚Äëlaw analyses (e.g., Kaplan et al., 2020).\n\nDiminishing returns: Beyond ~10¬π¬≤ FLOPs, each additional 10√ó compute translates to less than a 0.2√ó reduction in loss, indicating a saturation point for the given data distribution.\n\nStability considerations: Higher compute regimes exhibited lower gradient variance, enabling larger batch sizes and more stable optimizer schedules, which further contributed to smoother loss curves.\n\nImplications. The compute‚Äëloss relationship suggests that, for a fixed dataset, there exists an ‚Äúoptimal‚Äù compute budget where marginal gains are outweighed by diminishing returns. Practitioners can therefore allocate resources more efficiently by targeting compute levels that bring loss below a task‚Äëspecific threshold rather than pursuing maximal compute indiscriminately.\n\n\n\n4.2. Model Size vs.¬†Downstream Benchmark Performance\nObjective. To assess how scaling model parameters influences performance on a suite of downstream benchmarks (e.g., GLUE, SuperGLUE, BIG‚ÄëBench, and domain‚Äëspecific QA/translation tasks).\nMethodology.\n- Five model families were constructed with parameter counts ranging from 125‚ÄØM to 175‚ÄØB, keeping architecture (depth, width, attention heads) proportional.\n- All models were trained for an identical number of tokens (‚âà300‚ÄØB) using the same optimizer and learning‚Äërate schedule.\n- After pre‚Äëtraining, each model was fine‚Äëtuned on each benchmark for a fixed budget (e.g., 10‚ÄØk steps) and evaluated using the standard metric for that task.\nObserved Patterns.\n1. Monotonic improvement: Across almost all benchmarks, performance increased monotonically with model size, with a median relative gain of ~12‚ÄØ% when moving from 1‚ÄØB to 10‚ÄØB parameters.\n2. Task‚Äëspecific scaling exponents: Certain tasks displayed steeper scaling curves (e.g., multi‚Äëhop reasoning tasks exhibited exponent ‚âà0.35, whereas lexical classification tasks showed ‚âà0.15).\n3. Saturation thresholds: For a subset of benchmarks (e.g., natural language inference), performance plateaued around 70‚ÄØB parameters, suggesting that additional capacity yields negligible gains beyond this point.\n4. Cross‚Äëtask transfer: Larger models demonstrated superior zero‚Äëshot transfer to out‚Äëof‚Äëdistribution tasks, often outperforming smaller fine‚Äëtuned baselines by &gt;20‚ÄØ% absolute accuracy.\nStatistical Analysis.\n- A mixed‚Äëeffects regression model was fitted with size (log‚Äëparameter count) as a fixed effect and task as a random effect. The estimated coefficient for size was 0.28 (SE‚ÄØ=‚ÄØ0.02), confirming a statistically significant positive relationship (p‚ÄØ&lt;‚ÄØ0.001).\n- The marginal R¬≤ of the model was 0.42, indicating that size explains a substantial but not exhaustive portion of performance variance; task difficulty and data quality also contributed significantly.\nPractical Takeaway. Deploying a model whose parameter count aligns with the most demanding downstream task yields the greatest overall utility. However, for resource‚Äëconstrained settings, a ‚Äúsweet‚Äëspot‚Äù model (‚âà10‚Äì30‚ÄØB parameters) often balances performance gains with inference cost, especially when the target tasks are not heavily reasoning‚Äëintensive.\n\n\n\n4.3. Dataset Size and Data Quality Effects\nObjective. To disentangle the impact of raw dataset volume from the intrinsic quality of the data on downstream performance.\nExperimental Design.\n- Starting from a base corpus of 300‚ÄØB tokens, we constructed three variants:\n1. Low‚Äëquality, high‚Äëvolume ‚Äì duplicated and noisy web crawl (‚âà1.2‚ÄØT tokens, 30‚ÄØ% duplicate, 15‚ÄØ% profanity).\n2. Medium‚Äëquality, moderate‚Äëvolume ‚Äì filtered to remove exact duplicates and low‚Äëquality HTML (‚âà600‚ÄØB tokens).\n3. High‚Äëquality, low‚Äëvolume ‚Äì curated, human‚Äëannotated text (‚âà150‚ÄØB tokens, &gt;95‚ÄØ% clean).\n- Each variant was used to pre‚Äëtrain a 1.3‚ÄØB‚Äëparameter model for the same compute budget (‚âà10¬π¬π FLOPs).\n- Downstream evaluation was performed on a standardized benchmark suite (e.g., ARC, PIQA, and a domain‚Äëspecific medical QA set).\nFindings.\n| Dataset Variant | Validation Perplexity | Avg. Benchmark Accuracy | |‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì| | Low‚Äëquality, high‚Äëvolume | 18.4 | 68‚ÄØ% | | Medium‚Äëquality, moderate‚Äëvolume | 13.2 | 74‚ÄØ% | | High‚Äëquality, low‚Äëvolume | 11.7 | 78‚ÄØ% |\n\nQuality dominates quantity: Even when the high‚Äëquality set was four times smaller, the resulting model outperformed the low‚Äëquality counterpart by 10‚ÄØ% absolute accuracy.\n\nNoise mitigation: Models trained on noisy data exhibited higher variance in fine‚Äëtuning, leading to poorer calibration and higher error rates on out‚Äëof‚Äëdistribution prompts.\n\nCurriculum effects: When a progressive cleaning pipeline was applied (starting from noisy data and gradually adding higher‚Äëquality subsets), performance improved smoothly, suggesting that controlled exposure to increasing quality can yield synergistic benefits.\n\nInterpretation. These results reinforce the notion that data hygiene is a critical lever for scaling efficiency. Investing in filtering, deduplication, and domain‚Äëspecific curation can reduce the compute needed to achieve a target performance level, especially for tasks that demand precise linguistic understanding.\n\n\n\n4.4. Case Studies\n\n4.4.1. GPT‚Äë2 ‚Üí GPT‚Äë3\n\nScale jump: Parameter count increased from 1.5‚ÄØB (GPT‚Äë2) to 175‚ÄØB (GPT‚Äë3), accompanied by a 3,125√ó increase in training tokens (from 3‚ÄØB to 570‚ÄØB).\n\nEmpirical outcome: GPT‚Äë3 achieved state‚Äëof‚Äëthe‚Äëart zero‚Äëshot performance on 45‚ÄØ% of BIG‚ÄëBench tasks, a 20‚ÄØ% absolute gain over the best fine‚Äëtuned GPT‚Äë2 variants.\n\nKey insight: The scaling law exponent for loss versus compute remained stable (‚âà‚Äì0.07), but the effective downstream benefit per additional parameter rose sharply due to the richer data mixture and longer training horizon.\n\n\n\n4.4.2. PaLM (540‚ÄØB)\n\nTraining regime: 780‚ÄØB tokens, 1.5‚ÄØ√ó‚ÄØ10¬≤‚Å¥ FLOPs, using a mixture of web text, books, and code.\n\nPerformance: Demonstrated emergent capabilities (e.g., multi‚Äëstep arithmetic, few‚Äëshot reasoning) that were absent in smaller siblings. Benchmarks such as TriviaQA and Natural Questions saw relative improvements of 15‚Äì30‚ÄØ% over the 100‚ÄØB‚Äëparameter baseline.\n\nObservation: The model exhibited a double‚Äëdescent curve in terms of compute vs.¬†validation loss, where a temporary increase in loss was observed when moving from 100‚ÄØB to 300‚ÄØB parameters before the final descent at 540‚ÄØB.\n\n\n\n4.4.3. LLaMA (7‚ÄØB, 13‚ÄØB, 33‚ÄØB, 65‚ÄØB)\n\nUniform architecture: All sizes shared the same token embedding dimension scaling rule, facilitating direct size comparisons.\n\nDownstream results: On the MMLU benchmark, accuracy scaled roughly as 0.5‚ÄØ% per 10‚ÄØB parameter increase, with the 65‚ÄØB variant reaching 57‚ÄØ% average accuracy.\n\nData efficiency: When trained on a 1‚ÄëT‚Äëtoken filtered corpus, the 13‚ÄØB model matched the 33‚ÄØB model‚Äôs performance on several tasks, underscoring the importance of high‚Äëquality data.\n\n\n\n4.4.4. GPT‚Äë4 (estimated &gt;1‚ÄØT parameters)\n\nLimited public details: While exact compute figures are undisclosed, external analyses suggest &gt;10‚Å¥‚ÄØPF‚Äëdays of training and a token budget exceeding 13‚ÄØT.\n\nEmpirical evidence: GPT‚Äë4 achieved near‚Äëhuman performance on a broad set of professional exams (e.g., bar, medical licensing) and demonstrated unprecedented few‚Äëshot reasoning on novel tasks.\n\nScaling implications: The observed loss curve plateaued at a perplexity of ~9, indicating that further compute yields diminishing returns unless accompanied by richer data modalities (e.g., multimodal embeddings).\n\nSynthesis. Across these case studies, a consistent pattern emerges: scale amplifies capability, but the magnitude of improvement is mediated by three intertwined factors‚Äîtraining compute, model architecture, and data curation. The most pronounced gains arise when larger compute budgets are coupled with high‚Äëquality, diverse data, enabling emergent behaviors that cannot be predicted from smaller‚Äëscale experiments.\n\n\n\n\n4.5. Summary\n\nCompute‚Äëloss curves reveal a power‚Äëlaw relationship with diminishing returns beyond ~10¬π¬≤ FLOPs for a fixed dataset.\n\nModel size scaling yields monotonic improvements on most benchmarks, yet the rate of gain is task‚Äëdependent and often plateaus around 70‚Äì100‚ÄØB parameters for certain tasks.\n\nData quality can outweigh raw volume; curated, low‚Äënoise corpora produce markedly better downstream performance even when smaller in size.\n\nCase studies from GPT‚Äë2 ‚Üí GPT‚Äë3, PaLM, LLaMA, and GPT‚Äë4 illustrate how coordinated scaling of compute, parameters, and data leads to both incremental and emergent capabilities.\n\nThese empirical observations provide a quantitative backbone for the design of future foundation models, guiding resource allocation toward regimes where marginal gains are maximized while mitigating the costs associated with over‚Äëparameterization or data noise.\n\n5. Practical Implications and Description\nThis section translates the technical findings of the study into concrete actions that practitioners, decision‚Äëmakers, and budgeting teams can apply when selecting, deploying, and operating machine‚Äëlearning systems.\n\n\n\n5.1. Cost‚ÄëEfficiency Trade‚Äëoffs\n\n\n\n\n\n\n\n\n\nDimension\nTypical Trade‚Äëoff\nPractical Consequence\nMitigation Strategies\n\n\n\n\nModel Accuracy vs.¬†Compute Cost\nHigher‚Äëcapacity architectures (e.g., deep transformers, large ensembles) often yield marginal gains in predictive performance but require exponentially more GPU/TPU cycles, memory, and energy.\nDiminishing returns on accuracy can quickly outpace budget constraints, especially for inference‚Äëheavy workloads.\n‚Ä¢ Use progressive model scaling ‚Äì start with a baseline model and only upgrade when the marginal gain exceeds a predefined cost‚Äëbenefit threshold.‚Ä¢ Apply knowledge distillation to compress large models into smaller, cheaper variants.\n\n\nTraining Time vs.¬†Data Utilization\nLonger training epochs improve convergence but increase electricity, cloud‚Äëinstance hours, and labor costs.\nExtended timelines delay product releases and inflate operational expenses.\n‚Ä¢ Adopt early‚Äëstopping and learning‚Äërate schedules that stop training once validation improvement falls below a cost‚Äësensitivity parameter.‚Ä¢ Leverage mixed‚Äëprecision training and gradient checkpointing to cut compute without sacrificing final accuracy.\n\n\nModel Size vs.¬†Deployment Footprint\nLarger models improve performance on complex tasks but increase latency, storage, and memory requirements on edge devices.\nMay necessitate expensive hardware upgrades or limit deployment to data‚Äëcenter environments only.\n‚Ä¢ Prioritize parameter‚Äëefficient architectures (e.g., MobileNet‚ÄëV3, TinyBERT).‚Ä¢ Use quantization (int8/float16) and pruning to shrink model size while preserving accuracy.\n\n\nEnergy Consumption vs.¬†Sustainability Goals\nHigh‚Äëperformance training consumes significant electricity, affecting carbon footprints and potentially incurring carbon‚Äëtax penalties.\nDirect cost impact and reputational risk for environmentally‚Äëconscious organizations.\n‚Ä¢ Schedule training during off‚Äëpeak renewable‚Äëenergy windows.‚Ä¢ Employ carbon‚Äëaware scheduling tools that select low‚Äëcarbon cloud regions.\n\n\n\nKey Takeaway:\nCost‚Äëefficiency is not a single metric but a multi‚Äëdimensional balance. Decision‚Äëmakers should quantify the marginal utility of each additional unit of accuracy, latency, or energy consumption and compare it against the associated financial and ecological costs. A disciplined, data‚Äëdriven cost‚Äëbenefit analysis prevents over‚Äëengineering and ensures that resources are allocated where they deliver the greatest net value.\n\n\n\n5.2. Implications for Model Selection and Deployment\n\nPerformance‚ÄëFirst vs.¬†Cost‚ÄëFirst Paradigms\n\nPerformance‚Äëfirst approaches (e.g., selecting the highest‚Äëaccuracy model regardless of cost) are appropriate when the model is a core differentiator (e.g., proprietary recommendation engine).\n\nCost‚Äëfirst approaches dominate in commodity use‚Äëcases (e.g., fraud detection at scale) where marginal gains are negligible but operational expenses dominate.\n\nModel‚Äëas‚Äëa‚ÄëService (MaaS) Considerations\n\nDeploying models via APIs introduces inference‚Äëcost scaling: each request incurs compute, network, and storage charges.\n\nSelecting a model with a favorable accuracy‚Äëper‚Äëinference‚Äëcost ratio can dramatically improve ROI.\n\nUse dynamic scaling (e.g., serverless functions) and request batching to amortize fixed costs across many queries.\n\nVersioning, Monitoring, and Retraining Pipelines\n\nDeployed models require continuous monitoring for drift, which can trigger costly retraining cycles.\n\nImplement automated drift detection with thresholds tuned to the organization‚Äôs budget tolerance; only retrain when the expected loss in performance exceeds the projected cost of a new training run.\n\nHardware‚ÄëSpecific Optimizations\n\nCertain models (e.g., transformer‚Äëbased language models) are highly optimized on specific accelerators (e.g., NVIDIA GPUs, Google TPUs).\n\nAlign model architecture with the hardware portfolio of the deployment environment to minimize conversion overhead and maximize throughput.\n\nRegulatory and Compliance Constraints\n\nIn regulated domains (e.g., healthcare, finance), model interpretability and auditability may impose additional computational overhead (e.g., post‚Äëhoc explanation layers).\n\nFactor these compliance‚Äërelated costs into the selection matrix early to avoid surprise budget overruns later.\n\n\n\n\n\n5.3. Guidance for Resource Allocation and Budgeting\n\n\n\n\n\n\n\n\nBudgetary Element\nRecommended Allocation Principle\nPractical Implementation\n\n\n\n\nCompute Infrastructure\nAllocate 70‚ÄØ% of compute spend to steady‚Äëstate inference and 30‚ÄØ% to training/experimentation.\n‚Ä¢ Use spot instances or pre‚Äëemptible VMs for training workloads.‚Ä¢ Reserve dedicated instances for latency‚Äëcritical inference services.\n\n\nPersonnel\nReserve 40‚ÄØ% of data‚Äëscience/ML engineering capacity for model optimization (distillation, quantization) and 40‚ÄØ% for pipeline reliability (monitoring, CI/CD). The remaining 20‚ÄØ% supports research & innovation.\n‚Ä¢ Adopt DevOps‚Äëstyle MLops practices: automated testing, version control, and rollback mechanisms.\n\n\nCloud Services\nApply a cost‚Äëcenter tagging strategy; tag all resources by project, environment, and model version to enable granular spend analysis.\n‚Ä¢ Leverage reserved instances for predictable workloads.‚Ä¢ Use budget alerts that trigger when projected monthly spend exceeds a predefined threshold.\n\n\nEnergy & Sustainability\nInclude a carbon‚Äëcost factor (e.g., $/kg‚ÄØCO‚ÇÇ) in the cost model for high‚Äëenergy training jobs.\n‚Ä¢ Schedule heavy training jobs during periods of low grid carbon intensity.‚Ä¢ Purchase green‚Äëenergy credits where feasible to offset unavoidable emissions.\n\n\nContingency Reserve\nMaintain a 10‚Äë15‚ÄØ% contingency fund for unexpected retraining, emergency scaling, or security patches.\n‚Ä¢ Review and adjust the reserve quarterly based on historical variance in training job durations and inference traffic spikes.\n\n\n\nStrategic Checklist for Budget Planning\n\nDefine Success Metrics ‚Äì Establish clear, quantifiable targets (e.g., ‚Äúmaintain inference latency ‚â§‚ÄØ50‚ÄØms at ‚â§‚ÄØ$0.02 per 1‚ÄØk requests‚Äù).\n\nModel‚ÄëCost Matrix ‚Äì Build a spreadsheet that maps each candidate model to:\n\nExpected accuracy / performance.\n\nTraining compute (GPU‚Äëhours, memory).\n\nInference compute (CPU/GPU cycles, memory).\n\nStorage and network egress costs.\n\nEstimated annual operating expense.\n\n\nRun Sensitivity Analyses ‚Äì Vary key parameters (e.g., batch size, quantization level) to see how cost curves respond.\n\nPrioritize ‚ÄúLow‚ÄëHanging Fruit‚Äù ‚Äì Implement quick wins such as model pruning or switching to a cheaper inference backend before committing to large‚Äëscale infrastructure upgrades.\n\nDocument Assumptions ‚Äì Record all cost assumptions (e.g., cloud‚Äëprovider pricing, expected request volume) and revisit them quarterly as market rates evolve.\n\nBottom Line:\nEffective resource allocation hinges on a disciplined, data‚Äëdriven view of both technical performance and financial impact. By embedding cost‚Äëefficiency considerations into every stage‚Äîfrom model selection through to production monitoring‚Äîorganizations can maximize the return on their AI investments while staying within budgetary and sustainability constraints.\n\n6. Limitations and Open Questions\nThe empirical findings presented in this work illuminate several important trends, yet they also expose a set of constraints and unresolved issues that merit further investigation. The subsection below enumerates the principal limitations and outlines the key open questions that arise from each.\n\n\n\n6.1. Deviations from Ideal Power‚ÄëLaw Behavior\n\nEmpirical deviations. In several experimental regimes the observed scaling deviates systematically from the theoretically predicted power‚Äëlaw exponent. Specifically, for input distributions with heavy tails, the exponent appears to saturate at a lower value than anticipated, suggesting the presence of hidden bottlenecks that are not captured by the baseline model.\n\nFinite‚Äësize effects. The power‚Äëlaw regime is only observable over a limited range of scales; beyond this range, discretization and boundary effects dominate, leading to curvature in log‚Äëlog plots. Quantifying the size of the ‚Äúasymptotic window‚Äù remains an open analytical challenge.\n\nModel dependence. The deviations are sensitive to the choice of regularization and initialization strategies. While certain initialization schemes restore power‚Äëlaw behavior, they introduce additional hyper‚Äëparameters whose optimal settings are not yet fully understood.\n\nOpen question: Can a unified theoretical framework be developed that predicts the conditions under which power‚Äëlaw scaling breaks down, and that provides principled remedies (e.g., adaptive regularization) to recover the ideal exponent?\n\n\n\n6.2. Generalization Beyond the Studied Regimes\n\nOut‚Äëof‚Äëdistribution (OOD) inputs. The current experiments focus on a narrow band of input statistics (e.g., Gaussian, low‚Äëfrequency sinusoids). Preliminary tests on OOD datasets reveal a marked degradation in performance, indicating that the learned representations may be over‚Äëfitted to the training distribution.\n\nTemporal and dynamical extensions. Although the static analysis suffices for the present scope, extending the methodology to time‚Äëvarying or sequential inputs raises questions about stability, memory retention, and the emergence of recurrent dynamics.\n\nMulti‚Äëmodal interactions. The interplay between heterogeneous modalities (e.g., vision‚Äëlanguage, multimodal sensor fusion) has not been examined. Preliminary observations suggest that cross‚Äëmodal correlations may either amplify or suppress the power‚Äëlaw signatures observed in unimodal settings.\n\nOpen question: What architectural or training modifications are necessary to ensure robust generalization to unseen input distributions and to maintain power‚Äëlaw scaling in more complex, dynamic, or multimodal contexts?\n\n\n\n6.3. Role of Architectural Innovations and Sparsity\n\nSparse connectivity patterns. While sparse weight matrices have been shown to improve computational efficiency, their impact on the statistical properties of the learned representations is still ambiguous. In some cases, sparsity leads to a flattening of the power‚Äëlaw tail, whereas in others it accentuates it.\n\nNon‚Äëstandard layer designs. Recent architectural innovations‚Äîsuch as gated residual pathways, adaptive activation functions, and hierarchical attention mechanisms‚Äîintroduce additional nonlinearities that can perturb the scaling behavior. Systematic ablation studies are required to isolate which components are responsible for observed deviations.\n\nScalability limits. Scaling these innovations to larger model families (e.g., billions of parameters) may introduce new regimes where the assumptions underlying the power‚Äëlaw analysis no longer hold, particularly concerning memory bandwidth and communication constraints.\n\nOpen question: How can architectural design be guided by scaling laws to deliberately shape the statistical structure of representations, and what trade‚Äëoffs arise when moving from sparse, low‚Äëdimensional prototypes to high‚Äëcapacity, sparsely activated networks?\n\n\n\n6.4. Ethical and Environmental Considerations\n\nEnergy consumption. Training models that exhibit pronounced power‚Äëlaw scaling often requires extensive computational resources, leading to substantial electricity usage and associated carbon emissions. Quantifying the environmental footprint of such training pipelines and exploring energy‚Äëefficient alternatives is an emerging priority.\n\nBias amplification. The statistical regularities captured by power‚Äëlaw models can inadvertently reinforce existing societal biases present in the training data. For instance, skewed frequency distributions may cause over‚Äërepresentation of certain subpopulations, leading to disparate impacts in downstream applications.\n\nTransparency and accountability. The opaque nature of scaling relationships can hinder interpretability, making it difficult to audit model behavior or to certify compliance with fairness and safety standards. Developing explainable metrics that link scaling exponents to ethical outcomes is an open research avenue.\n\nOpen question: What principled frameworks can reconcile the pursuit of improved scaling performance with sustainability goals and ethical safeguards, and how can such frameworks be operationalized in model development pipelines?\n\nSummary. Addressing the limitations and open questions outlined above will be essential for advancing both the theoretical understanding and practical deployment of power‚Äëlaw‚Äëguided methodologies. Future work should aim to (i) refine the theoretical foundations that predict scaling breakdowns, (ii) extend empirical validation to richer input spaces, (iii) systematically dissect the influence of architectural choices and sparsity, and (iv) embed ethical and environmental considerations into the design and evaluation process. Only through a coordinated effort across these dimensions can the full potential of scaling laws be realized in a responsible and sustainable manner.\n\n7. Future Directions\nThe rapid evolution of large‚Äëscale language models has exposed both the promise and the limits of current scaling paradigms. Anticipating the next generation of research and deployment requires a shift from purely empirical growth toward more principled, data‚Äëcentric, and predictive frameworks. The following subsections outline three interrelated avenues that are poised to reshape how we design, evaluate, and operationalize future models.\n\n\n\n7.1. Emerging Scaling Regimes (e.g., Multimodal, Reasoning‚ÄëFocused Models)\n\nMultimodal Integration\n\nConcept: Extending the parameter‚Äëcentric paradigm to incorporate heterogeneous data streams‚Äîtext, vision, audio, and structured knowledge‚Äîwithin a unified architecture.\n\nImplications: Scaling laws must now account for cross‚Äëmodal token budgets and alignment costs (e.g., joint embedding layers, contrastive pre‚Äëtraining). Early evidence suggests that effective model size grows sub‚Äëlinearly with raw parameter count when modalities are balanced, prompting a re‚Äëexamination of ‚Äúbigger‚Äëis‚Äëbetter‚Äù heuristics.\n\nResearch Frontiers: Development of modular token‚Äëfusion mechanisms, dynamic modality weighting, and curriculum‚Äëdriven data mixing strategies that preserve scalability while enhancing multimodal reasoning.\n\nReasoning‚ÄëFocused Architectures\n\nConcept: Designing models whose capacity is explicitly allocated to structured inference (e.g., chain‚Äëof‚Äëthought, symbolic manipulation, program synthesis) rather than merely memorizing surface patterns.\n\nImplications: Scaling regimes shift from ‚Äúparameter‚Äëheavy‚Äù to ‚Äúcompute‚Äëheavy‚Äù regimes, where effective model size is measured in reasoning steps per token and depth of latent deliberation. This gives rise to sparse scaling laws that penalize unnecessary breadth but reward depth.\n\nResearch Frontiers: Exploration of self‚Äëgenerated reasoning traces, reinforcement‚Äëlearning‚Äëfrom‚Äëhuman‚Äëfeedback (RLHF) on logical consistency, and neuro‚Äësymbolic hybrids that can be scaled predictably.\n\n\n\n\n\n7.2. Alternative Formulation of Scaling Laws (e.g., Data‚ÄëAware Scaling)\n\nFrom Parameter‚ÄëCentric to Data‚ÄëCentric Metrics\n\nTraditional scaling laws relate model performance (P) to parameter count (N) and dataset size (D) as (P N^{} D^{}). Recent work proposes data‚Äëefficiency indices that weight each token by its informational gain (e.g., novelty, difficulty, semantic richness).\n\nThis yields a data‚Äëaware scaling law: (P _{i=1}^{D} w_i f(N_i)), where (w_i) encodes token importance and (f) captures diminishing returns of additional parameters on high‚Äëvalue data.\n\nIncorporating Compute Budgets and Training Dynamics\n\nBy treating effective compute (C) (FLOPs) as a third axis, we can express performance as (P = g(N, D, C)) with budget‚Äëaware exponents that reflect optimal allocation across pre‚Äëtraining, fine‚Äëtuning, and in‚Äëcontext learning.\n\nEmpirical studies suggest an optimal trade‚Äëoff surface where marginal gains from extra parameters are outpaced by gains from targeted data augmentation or curriculum scheduling.\n\nPredictive Modelling and Generalisation Bounds\n\nLeveraging statistical learning theory, researchers are deriving generalisation bounds that tie scaling exponents to covering numbers of the data manifold. Such bounds enable pre‚Äëemptive predictions of required (N) and (D) for a target error tolerance, reducing costly trial‚Äëand‚Äëerror experiments.\n\n\n\n\n\n7.3. Potential for Predictive Tools and Automated Scaling\n\nAutomated Scaling Pipelines\n\nToolkits: Emerging frameworks (e.g., ScaleAI, MetaScale) integrate Bayesian optimization, multi‚Äëfidelity simulation, and differentiable architecture search to propose optimal ((N, D, C)) configurations given a performance target and resource constraints.\n\nWorkflow: Users specify a utility function (e.g., cost‚Äëweighted accuracy), and the system iteratively samples scaling configurations, evaluates them on proxy tasks, and refines its policy via reinforcement learning.\n\nPredictive Modelling of Scaling Behaviour\n\nNeural‚Äëaugmented regressors: Models trained on historic scaling experiments can predict the slope of performance curves for unseen model families, enabling early‚Äëstage forecasting of breakpoint behaviours (e.g., transition from data‚Äëlimited to compute‚Äëlimited regimes).\n\nUncertainty Quantification: Probabilistic models (e.g., Gaussian processes with hierarchical priors) provide confidence intervals around predicted scaling exponents, allowing stakeholders to assess risk before committing to massive training runs.\n\nEthical and Operational Implications\n\nPredictive scaling tools democratize access to high‚Äëperforming models by allowing smaller labs to leverage the same scaling insights previously reserved for industry giants.\n\nHowever, they also raise concerns about over‚Äëreliance on extrapolation, potential bias amplification if historical data reflect inequities, and the need for transparent accounting of assumptions (e.g., distribution shift, hardware constraints).\n\n\n\nSummary\nFuture directions in scaling research are converging on three synergistic thrusts: (1) redefining what it means to scale by embedding multimodal and reasoning capabilities into the model fabric; (2) recasting scaling laws to be explicitly data‚Äëaware, compute‚Äëaware, and statistically grounded; and (3) building automated, predictive tooling that can guide resource allocation with quantified uncertainty. Together, these advances promise a more principled and efficient pathway toward the next generation of large‚Äëscale AI systems.\n\n8. Conclusion and Description: Synthesis of Key Insights and Final Take‚Äëaways\n\n\n\n1. Overview of Core Findings\n\nInterdisciplinary Convergence: The project demonstrated that integrating [Domain A], [Domain B], and [Domain C] yields a synergistic framework that outperforms siloed approaches.\n\nEvidence‚ÄëBased Impact: Quantitative metrics (e.g., a 23‚ÄØ% increase in efficiency, 15‚ÄØ% reduction in error rates) and qualitative feedback from stakeholders confirm the tangible benefits of the proposed solution.\n\nScalability & Transferability: The methodology proved adaptable across [Context 1], [Context 2], and [Context 3], suggesting strong potential for broader deployment in similar environments.\n\n\n\n2. Key Insights\n\n\n\n\n\n\n\n\nInsight\nDescription\nImplication\n\n\n\n\n1. Process Alignment\nAligning workflow stages with [specific principle] eliminated bottlenecks.\nStreamlined operations and reduced cycle time by X‚ÄØ%.\n\n\n2. Data‚ÄëDriven Decision Making\nLeveraging real‚Äëtime analytics enabled proactive adjustments.\nImproved predictive accuracy from Y‚ÄØ% ‚Üí Z‚ÄØ%.\n\n\n3. Stakeholder Engagement\nEarly involvement of end‚Äëusers fostered ownership and reduced resistance.\nAdoption rate rose to 85‚ÄØ% within the first quarter.\n\n\n4. Continuous Improvement Loop\nEmbedding feedback mechanisms sustains iterative refinement.\nEstablished a quarterly review cadence that drives ongoing enhancements.\n\n\n\n\n\n3. Final Take‚Äëaways\n\nStrategic Integration Is Critical ‚Äì Combining complementary strengths across disciplines creates a multiplier effect that single‚Äëdomain solutions cannot achieve.\n\nMetrics‚ÄëCentric Approach Enhances Credibility ‚Äì Quantifiable outcomes provide a clear business case for continued investment and replication.\n\nHuman‚ÄëCentric Design Drives Adoption ‚Äì Engaging end‚Äëusers from the outset translates technical gains into practical, sustainable results.\n\nScalable Frameworks Enable Future Growth ‚Äì The modular architecture allows for incremental expansion into new markets or use‚Äëcases without major redesign.\n\nContinuous Feedback Is Non‚ÄëNegotiable ‚Äì Embedding mechanisms for ongoing learning ensures the solution remains relevant amid evolving constraints and opportunities.\n\n\n\n4. Recommendations for Next Steps\n\nPilot Expansion: Deploy the framework in [Target Region/Department] to validate scalability under varied operational conditions.\n\nResource Allocation: Secure additional [budget/skill‚Äëset] to accelerate implementation phases and support training initiatives.\n\nPerformance Monitoring: Establish a dashboard of KPIs (e.g., throughput, error rate, user satisfaction) to track long‚Äëterm impact.\n\nKnowledge Transfer: Develop a playbook documenting best practices, lessons learned, and configuration templates for future teams.\n\nStakeholder Communication: Maintain a regular cadence of updates to keep sponsors, partners, and end‚Äëusers aligned with progress and outcomes.\n\n\nBottom Line: The synthesis of insights confirms that a coordinated, data‚Äëinformed, and stakeholder‚Äëfocused approach not only delivers measurable performance gains but also establishes a resilient foundation for future innovation. By institutionalizing the identified best practices and scaling the framework responsibly, the organization is well positioned to achieve sustained competitive advantage.\n\n9. References and Description\nComprehensive list of peer‚Äëreviewed papers, technical reports, and credible web sources.\n\n\n\n9.1 Purpose\nThe References and Description section serves three primary objectives:\n\nTransparency ‚Äì Provide readers with a clear audit trail of every scholarly and technical source that informed the research, analysis, or design presented in this report.\n\nCredibility ‚Äì Demonstrate that all factual statements, data sets, models, and design decisions are grounded in vetted, peer‚Äëreviewed literature or reputable institutional publications.\n\nReproducibility ‚Äì Enable other researchers to locate, retrieve, and, where appropriate, replicate the underlying evidence that supports the findings and recommendations.\n\n\n\n\n9.2 Scope of Sources\n\n\n\n\n\n\n\n\nCategory\nTypical Content\nExample Sources\n\n\n\n\nPeer‚Äëreviewed journal articles\nOriginal research findings, literature reviews, meta‚Äëanalyses, theoretical frameworks.\nIEEE Transactions on Neural Networks, Journal of Machine Learning Research, Nature Communications\n\n\nConference proceedings\nCutting‚Äëedge results presented at major scientific or engineering conferences.\nProceedings of the International Conference on Machine Learning (ICML), ACM SIGGRAPH\n\n\nTechnical reports & white papers\nIn‚Äëdepth studies from government agencies, industry research labs, or standards bodies.\nNASA Technical Report, Microsoft Research Technical Report, ISO/IEC 27001\n\n\nBooks & book chapters\nAuthoritative syntheses, historical context, or comprehensive theory.\nPattern Recognition and Machine Learning (Bishop), Deep Learning (Goodfellow, Bengio & Courville)\n\n\nCredible web resources\nData repositories, open‚Äësource code bases, authoritative databases, and policy documents.\nUCI Machine Learning Repository, World Health Organization (WHO) Fact Sheets, NASA Earthdata\n\n\nStandards & regulations\nMandatory or widely‚Äëadopted specifications that shape methodology or implementation.\nISO/IEC 17025, IEEE 802.11, EU General Data Protection Regulation (GDPR)\n\n\n\nOnly sources that meet the following criteria are included:\n\nPeer‚Äëreviewed (for journal articles and conference papers) or formally reviewed (for technical reports and standards).\n\nPublicly accessible (or available through institutional subscriptions) and citable with a stable identifier (DOI, URL, or report number).\n\nDirectly relevant to the objectives, methodology, or data used in the current work.\n\n\n\n\n9.3 Organization of the Reference List\nThe references are organized alphabetically by the first author‚Äôs surname (or by the responsible organization for reports). Each entry follows the APA 7th edition format, with the following supplemental fields added to aid navigation:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nDOI / URL\nPersistent identifier or direct link to the source.\n\n\nAccess Date\nDate on which the source was retrieved (required for dynamic web content).\n\n\nVersion / Retrieval Note\nFor datasets or code repositories, the specific version number or commit hash used.\n\n\nKey Findings / Relevance\nA one‚Äësentence annotation summarizing why the source is cited in the report.\n\n\n\nExample entry (APA style with annotation):\n\nSmith, J. A., & Lee, K. (2022). Deep reinforcement learning for autonomous navigation in urban environments. IEEE Transactions on Robotics, 38(4), 2150‚Äë2165. https://doi.org/10.1109/TRO.2022.1234567\nProvides the algorithmic framework and benchmark datasets used for the navigation module described in Section‚ÄØ4.2.\n\n\n\n\n9.4 Annotated Bibliography (Sample)\nBelow is a representative sample of the annotated bibliography that will appear in the final report. (The complete list contains ‚âà‚ÄØ150 entries.)\n\n\n\n\n\n\n\n\n#\nReference\nAnnotation\n\n\n\n\n1\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\nClassic textbook that introduces Bayesian inference, graphical models, and variational methods; foundational for the probabilistic models used in Chapter‚ÄØ3.\n\n\n2\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nComprehensive overview of deep learning architectures; consulted for justification of convolutional network choices in Section‚ÄØ5.1.\n\n\n3\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770‚Äë778.\nIntroduces residual connections that inspired the architecture of the image‚Äëclassification pipeline described in Section‚ÄØ5.3.\n\n\n4\nNASA (2023). Earth Observing System Data and Information System (EOSDIS) ‚Äì Data Holdings.\nProvides the multi‚Äëspectral satellite imagery dataset used for the environmental monitoring case study (Section‚ÄØ6.1).\n\n\n5\nWorld Health Organization. (2022). Global Health Estimates 2022.\nSupplies the baseline mortality and disease‚Äëburden statistics cited in the policy‚Äëimpact analysis (Section‚ÄØ7.2).\n\n\n6\nISO/IEC (2021). ISO/IEC 27001:2022 Information security, cybersecurity and privacy protection ‚Äì Information security management systems ‚Äì Requirements.\nGoverns the security controls implemented in the proposed system architecture (Section‚ÄØ4.4).\n\n\n7\nZhang, Y., et al.¬†(2024). Scalable federated learning for edge devices. Nature Machine Intelligence, 6, 1123‚Äë1135.\nPresents the federated learning framework adopted for privacy‚Äëpreserving model updates (Section‚ÄØ3.5).\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nThe full annotated bibliography will be appended as Appendix‚ÄØA.\n\n\n\n9.5 Verification of Source Quality\nEach source was evaluated against the following quality‚Äëassurance checklist:\n\n\n\n\n\n\n\nCriterion\nAssessment\n\n\n\n\nPeer‚Äëreview status\nConfirmed via journal/conference editorial board or editorial statement.\n\n\nAuthoritativeness\nAuthors hold relevant academic or industry credentials; affiliations are reputable.\n\n\nCurrency\nPublication date ‚â§‚ÄØ5‚ÄØyears unless the work is a seminal, foundational contribution.\n\n\nRelevance\nDirectly cited in the text or used to support a methodological choice.\n\n\nAccessibility\nDOI or stable URL available; no pay‚Äëwall restrictions for readers of the report.\n\n\nConflict of interest\nNo evident commercial bias that would compromise objectivity.\n\n\n\nSources that failed any of these criteria were excluded or replaced with an equivalent alternative.\n\n\n\n9.6 How to Use This Section\n\nFor reviewers: Consult the annotated bibliography to verify that every claim is substantiated by a reliable source.\n\nFor readers: Follow the DOI/URL links to retrieve the original documents for deeper exploration.\n\nFor future work: The reference list serves as a curated starting point for anyone wishing to extend, replicate, or critique the present study.\n\n\n\n\n9.7 Limitations\n\nCoverage bias: While every effort was made to include all pertinent literature up to the cut‚Äëoff date (November‚ÄØ2025), some very recent pre‚Äëprints or region‚Äëspecific reports may not be captured.\n\nLanguage restriction: The bibliography focuses primarily on English‚Äëlanguage sources; non‚ÄëEnglish scholarly works that are directly relevant have been deliberately omitted to maintain consistency in annotation.\n\n\n\n\n9.8 Future Updates\nThe reference list will be periodically reviewed (at least annually) to incorporate newly published peer‚Äëreviewed works, emerging standards, and updated data repositories. Updates will be recorded in a version‚Äëcontrolled changelog (Appendix‚ÄØB) to maintain a transparent evolution of the source base.\n\nEnd of Section‚ÄØ9 ‚Äì References and Description.\n\nAppendices and Description\nThe following appendices supplement the main body of the report. They are organized into four distinct parts, each serving a specific purpose to enhance clarity, reproducibility, and completeness of the presented material.\n\n\n\nA. Glossary of Terms\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\nContext of Use\nNotes\n\n\n\n\nANOVA\nAnalysis of Variance\nStatistical test comparing means across multiple groups\nAssumptions: normality, homogeneity of variance\n\n\nCI\nConfidence Interval\nRange of values that likely contain the population parameter\n95‚ÄØ% CI is reported unless otherwise specified\n\n\nFDR\nFalse Discovery Rate\nProportion of false positives among rejected hypotheses\nUsed when controlling for multiple testing\n\n\nICC\nIntraclass Correlation Coefficient\nMeasure of reliability for clustered data\nValues range from 0 to 1; &gt;0.75 indicates high reliability\n\n\nLME\nLinear Mixed‚ÄëEffects Model\nRegression model accounting for both fixed and random effects\nSoftware: lme4 (R) or lmerTest\n\n\np‚Äëvalue\nProbability value\nSignificance test against the null hypothesis\nReported to three decimal places; ‚Äú&lt;0.001‚Äù when appropriate\n\n\nQ‚Äëstatistic\nQuadratic form statistic\nUsed in goodness‚Äëof‚Äëfit tests for multivariate data\nComputed from residual covariance matrix\n\n\nR¬≤\nCoefficient of Determination\nProportion of variance explained by the model\nAdjusted R¬≤ is reported for models with multiple predictors\n\n\nSD\nStandard Deviation\nMeasure of dispersion around the mean\nReported for all continuous variables\n\n\nSE\nStandard Error\nEstimated standard deviation of a sampling distribution\nUsed for confidence‚Äëinterval construction\n\n\nSkewness\nAsymmetry of a distribution\nIndicates whether the distribution is symmetric\nPositive values indicate right‚Äëskewed data\n\n\nKurtosis\n‚ÄúPeakedness‚Äù of a distribution\nMeasures tail heaviness relative to a normal distribution\nExcess kurtosis is reported (normal = 0)\n\n\n\nAll terms are defined at first appearance in the main text; the glossary provides a quick reference for readers who may encounter them out of context.\n\n\n\nB. Detailed Data Tables\n\n\n\n\n\n\n\n\n\nTable\nDescription\nKey Columns\nSample Row (illustrative)\n\n\n\n\nTable‚ÄØA1\nSummary statistics for all variables (n, mean, SD, min, max)\nVariable, Units, N, Mean, SD, Min, Max, Median\nAge (years), 150, 48.2, 12.5, 22, 78, 46\n\n\nTable‚ÄØA2\nCorrelation matrix (Pearson r) among continuous predictors\nVariable‚ÄØ1, Variable‚ÄØ2, r, p‚Äëvalue\nAge, Income, 0.34, 0.001\n\n\nTable‚ÄØA3\nResults of the primary statistical test (e.g., ANOVA)\nSource, df, F, p, Œ∑¬≤\nTreatment, 2, 5.67, 0.004, 0.036\n\n\nTable‚ÄØA4\nModel coefficients for the final mixed‚Äëeffects model\nFixed Effect, Estimate, SE, t, p, 95‚ÄØ% CI\nIntercept, 3.12, 0.45, 6.93, &lt;0.001, 2.24‚Äì4.00\n\n\nTable‚ÄØA5\nSensitivity analysis results (subgroup analyses)\nSubgroup, N, Effect Size, p‚Äëvalue\nAge‚ÄØ&gt;‚ÄØ65, 38, 0.42, 0.02\n\n\nTable‚ÄØA6\nMissing‚Äëdata summary\nVariable, Missing N, % Missing, Imputation Method\nIncome, 5, 3.3‚ÄØ%, Multiple Imputation\n\n\n\nAll tables are presented in LaTeX format in the manuscript and are also provided as separate Excel files (Appendix‚ÄØB.xlsx) for reader convenience.\n\n\n\nC. Additional Plots and Statistical Analyses\n\n\n\n\n\n\n\n\n\nPlot\nPurpose\nDescription of Content\nLocation in Appendix\n\n\n\n\nFigure‚ÄØC1\nResidual diagnostics for the LME\nNormal‚Äëprobability plot, residual vs.¬†fitted scatter, heteroscedasticity check\nPage‚ÄØA‚Äë12\n\n\nFigure‚ÄØC2\nDistribution of the primary outcome across quartiles\nKernel density estimate with overlay of mean and 95‚ÄØ% CI\nPage‚ÄØA‚Äë13\n\n\nFigure‚ÄØC3\nInteraction plot for the treatment √ó covariate effect\nLine graph showing predicted outcomes at low, medium, and high levels of the covariate\nPage‚ÄØA‚Äë14\n\n\nFigure‚ÄØC4\nForest plot of subgroup effects\nSummary estimates with 95‚ÄØ% CI for each predefined subgroup\nPage‚ÄØA‚Äë15\n\n\nFigure‚ÄØC5\nHeatmap of the correlation matrix\nColor‚Äëcoded matrix with hierarchical clustering of variables\nPage‚ÄØA‚Äë16\n\n\nFigure‚ÄØC6\nKaplan‚ÄëMeier survival curves (if applicable)\nCurves for each categorical group with log‚Äërank test statistics\nPage‚ÄØA‚Äë17\n\n\nFigure‚ÄØC7\nSensitivity analysis ‚Äì ROC curves\nArea under the curve (AUC) with 95‚ÄØ% CI for each model variant\nPage‚ÄØA‚Äë18\n\n\n\nAll plots are generated using ggplot2 (R) or Matplotlib (Python) and are saved in both vector (PDF) and raster (PNG, 300‚ÄØdpi) formats. The complete reproducible code is provided in the supplementary GitHub repository (link in the Data Availability statement).\n\n\n\nD. Glossary of Abbreviations\n\n\n\n\n\n\n\n\n\nAbbreviation\nFull Form\nFirst Appearance (Section/Page)\nMeaning in Report\n\n\n\n\nANOVA\nAnalysis of Variance\n3.2, p.‚ÄØ15\nStatistical test for group differences\n\n\nAUC\nArea Under the Curve\n4.1, p.‚ÄØ27\nPerformance metric for binary classifiers\n\n\nCI\nConfidence Interval\n2.1, p.‚ÄØ8\nInterval estimate of a parameter\n\n\ndf\nDegrees of Freedom\n3.4, p.‚ÄØ19\nParameter that quantifies sample information\n\n\nFDR\nFalse Discovery Rate\n5.3, p.‚ÄØ34\nExpected proportion of false positives\n\n\nICC\nIntraclass Correlation Coefficient\n2.5, p.‚ÄØ22\nReliability measure for clustered data\n\n\nIQR\nInter‚ÄëQuartile Range\n2.3, p.‚ÄØ12\nMeasure of statistical dispersion\n\n\nLME\nLinear Mixed‚ÄëEffects Model\n3.5, p.‚ÄØ23\nRegression model with random effects\n\n\nN\nSample Size\nThroughout\nNumber of observations\n\n\np‚Äëvalue\nProbability value\n2.2, p.‚ÄØ9\nSignificance level for hypothesis testing\n\n\nQ‚Äëstatistic\nQuadratic Form Statistic\n4.2, p.‚ÄØ28\nGoodness‚Äëof‚Äëfit test statistic\n\n\nR¬≤\nCoefficient of Determination\n3.1, p.‚ÄØ13\nProportion of variance explained\n\n\nSE\nStandard Error\n2.4, p.‚ÄØ14\nEstimated standard deviation of a statistic\n\n\nSD\nStandard Deviation\n2.3, p.‚ÄØ12\nMeasure of data variability\n\n\nSPSS\nStatistical Package for the Social Sciences\n2.1, p.‚ÄØ7\nSoftware used for initial analyses\n\n\nIQR\nInter‚ÄëQuartile Range\n2.3, p.‚ÄØ12\n25th‚Äì75th percentile range\n\n\nUCL\nUpper Control Limit\n6.1, p.‚ÄØ41\nThreshold for process control charts\n\n\nWHO\nWorld Health Organization\n1.1, p.‚ÄØ1\nInternational health authority\n\n\n\nAbbreviations are defined at first use in the text; the glossary provides a consolidated reference for quick lookup.\n\nEnd of Appendices\nThese supplementary materials are intended to facilitate full transparency of the analytical workflow, enable independent verification of the results, and provide the reader with all necessary context to interpret the findings without over‚Äëburdening the main manuscript."
  },
  {
    "objectID": "L01/08_workflow_patterns.html#evaluator-optimizer",
    "href": "L01/08_workflow_patterns.html#evaluator-optimizer",
    "title": "Workflows and Agents",
    "section": "Evaluator-optimizer",
    "text": "Evaluator-optimizer\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\nEvaluator-optimizer workflows are commonly used when there‚Äôs particular success criteria for a task, but iteration is required to meet that criteria. For example, there‚Äôs not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\n\n\nfrom pydantic import BaseModel, Field\nfrom typing import Literal \nfrom langgraph.func import entrypoint, task\n\n\n# Schema for structured output to use in evaluation\nclass Feedback(BaseModel):\n    grade: Literal[\"funny\", \"not funny\"] = Field(\n        description=\"Decide if the joke is funny or not.\",\n    )\n    feedback: str = Field(\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\n    )\n\n\n# Augment the LLM with schema for structured output\nevaluator = llm.with_structured_output(Feedback)\n\n\n# Nodes\n@task\ndef llm_call_generator(topic: str, feedback: Feedback):\n    \"\"\"LLM generates a joke\"\"\"\n    if feedback:\n        msg = llm.invoke(\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\n        )\n    else:\n        msg = llm.invoke(f\"Write a joke about {topic}\")\n    return msg.content\n\n\n@task\ndef llm_call_evaluator(joke: str):\n    \"\"\"LLM evaluates the joke\"\"\"\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\n    return feedback\n\n\n@entrypoint()\ndef optimizer_workflow(topic: str):\n    feedback = None\n    while True:\n        joke = llm_call_generator(topic, feedback).result()\n        feedback = llm_call_evaluator(joke).result()\n        if feedback.grade == \"funny\":\n            break\n\n    return joke\n\n\n# Invoke\nfor step in optimizer_workflow.stream(\"mouse\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n\n{'llm_call_generator': 'Why did the mouse get a promotion at the cheese factory?\\n\\nBecause it always delivered the *big* cheese! üê≠üßÄ'}"
  },
  {
    "objectID": "L01/08_workflow_patterns.html#agents",
    "href": "L01/08_workflow_patterns.html#agents",
    "title": "Workflows and Agents",
    "section": "Agents",
    "text": "Agents\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\n\n\n\n\n\n\n\nNote\n\n\n\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\n\n\n\nfrom langchain.tools import tool\n\n\n# Define tools\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a * b\n\n\n@tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Adds `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a + b\n\n\n@tool\ndef divide(a: int, b: int) -&gt; float:\n    \"\"\"Divide `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a / b\n\n\n# Augment the LLM with tools\ntools = [add, multiply, divide]\ntools_by_name = {tool.name: tool for tool in tools}\nllm_with_tools = llm.bind_tools(tools)\n\n\nfrom langgraph.graph import add_messages\nfrom langchain.messages import (\n    SystemMessage,\n    HumanMessage,\n    ToolCall,\n)\nfrom langchain_core.messages import BaseMessage\n\n\n@task\ndef call_llm(messages: list[BaseMessage]):\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\n    return llm_with_tools.invoke(\n        [\n            SystemMessage(\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n            )\n        ]\n        + messages\n    )\n\n\n@task\ndef call_tool(tool_call: ToolCall):\n    \"\"\"Performs the tool call\"\"\"\n    tool = tools_by_name[tool_call[\"name\"]]\n    return tool.invoke(tool_call)\n\n\n@entrypoint()\ndef agent(messages: list[BaseMessage]):\n    llm_response = call_llm(messages).result()\n\n    while True:\n        if not llm_response.tool_calls:\n            break\n\n        # Execute tools\n        tool_result_futures = [\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\n        ]\n        tool_results = [fut.result() for fut in tool_result_futures]\n        messages = add_messages(messages, [llm_response, *tool_results])\n        llm_response = call_llm(messages).result()\n\n    messages = add_messages(messages, llm_response)\n    return messages\n\n\n# Invoke\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\n    print(chunk)\n    print(\"\\n\")\n\n{'call_llm': AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 530, 'total_tokens': 646, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 83, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771681275-L9PdIbxnKPdh7nLN8k3S', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c806e-db5b-7b43-a06e-af491e6c4ff4-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 4}, 'id': 'call_74b1f662907e4881b16fcdfd', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 530, 'output_tokens': 116, 'total_tokens': 646, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 83}})}\n\n\n{'call_tool': ToolMessage(content='7', name='add', tool_call_id='call_74b1f662907e4881b16fcdfd')}\n\n\n{'call_llm': AIMessage(content='The sum of 3 and 4 is **7**. Let me know if you need help with anything else! üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 109, 'prompt_tokens': 580, 'total_tokens': 689, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 82, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771681278-G987J07cb2jyXzn7GauO', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c806e-eb25-7d03-932f-18261e78fac6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 580, 'output_tokens': 109, 'total_tokens': 689, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 82}})}\n\n\n{'agent': [HumanMessage(content='Add 3 and 4.', additional_kwargs={}, response_metadata={}, id='a272ab84-361b-402a-850f-13f32fea27ff'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 530, 'total_tokens': 646, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 83, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771681275-L9PdIbxnKPdh7nLN8k3S', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c806e-db5b-7b43-a06e-af491e6c4ff4-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 4}, 'id': 'call_74b1f662907e4881b16fcdfd', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 530, 'output_tokens': 116, 'total_tokens': 646, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 83}}), ToolMessage(content='7', name='add', id='ec01cc1d-7857-4d31-b025-c861176c6646', tool_call_id='call_74b1f662907e4881b16fcdfd'), AIMessage(content='The sum of 3 and 4 is **7**. Let me know if you need help with anything else! üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 109, 'prompt_tokens': 580, 'total_tokens': 689, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 82, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771681278-G987J07cb2jyXzn7GauO', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c806e-eb25-7d03-932f-18261e78fac6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 580, 'output_tokens': 109, 'total_tokens': 689, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 82}})]}"
  },
  {
    "objectID": "L01/09_memory.html",
    "href": "L01/09_memory.html",
    "title": "Memory",
    "section": "",
    "text": "Focus on reading the following section for getting to know how you can read/write memory in tools:\n\nAccess context\n\nShort-term memory (State)\nContext (Immutable Configuration)\nLong-term memory (Store)"
  },
  {
    "objectID": "L01/Short-term memory.html",
    "href": "L01/Short-term memory.html",
    "title": "Short-term memory",
    "section": "",
    "text": "Short term memory lets your application remember previous interactions within a single thread or conversation.\nA thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\nLong conversations pose a challenge to today‚Äôs LLMs; a full history may not fit inside an LLM‚Äôs context window, resulting in an context loss or errors. Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get ‚Äúdistracted‚Äù by stale or off-topic content, all while suffering from slower response times and higher costs.\nBecause context windows are limited, many applications can benefit from using techniques to remove or ‚Äúforget‚Äù stale information."
  },
  {
    "objectID": "L01/Short-term memory.html#overview",
    "href": "L01/Short-term memory.html#overview",
    "title": "Short-term memory",
    "section": "",
    "text": "Short term memory lets your application remember previous interactions within a single thread or conversation.\nA thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\nLong conversations pose a challenge to today‚Äôs LLMs; a full history may not fit inside an LLM‚Äôs context window, resulting in an context loss or errors. Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get ‚Äúdistracted‚Äù by stale or off-topic content, all while suffering from slower response times and higher costs.\nBecause context windows are limited, many applications can benefit from using techniques to remove or ‚Äúforget‚Äù stale information."
  },
  {
    "objectID": "L01/Short-term memory.html#usage-checkpointer",
    "href": "L01/Short-term memory.html#usage-checkpointer",
    "title": "Short-term memory",
    "section": "Usage: Checkpointer",
    "text": "Usage: Checkpointer\nState is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.\n\nimport os\nfrom langchain_openai import ChatOpenAI\n\n\n# https://openrouter.ai/nvidia/nemotron-3-nano-30b-a3b:free\nmodel_nemotron3_nano = ChatOpenAI(\n    model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n    temperature=0,\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n)\n\n\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nagent = create_agent(\n    model=model_nemotron3_nano,\n    checkpointer=InMemorySaver(),\n)\n\n\nfrom langchain_core.runnables import RunnableConfig\n\nconfig: RunnableConfig = {\n    \"configurable\": {\n        \"thread_id\": \"22\"\n    }\n}\n\n\nfrom langchain.messages import HumanMessage\n\nagent.invoke(\n    input={\"messages\": [HumanMessage(\"Hi! My name is Bob.\")]},\n    config=config,\n)\n\n{'messages': [HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='770fdabc-b944-49d8-92d4-82869fe6a080'),\n  AIMessage(content='Hello, Bob! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 23, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 34, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1772041071-FX512nlXwz3nR37T6Zll', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c95e0-e977-78d1-8fa0-34b8b16796a9-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 23, 'output_tokens': 48, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 34}})]}\n\n\n\nagent.invoke(\n    input={\"messages\": [HumanMessage(\"Do you remember my name?\")]},\n    config=config\n)\n\n{'messages': [HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='770fdabc-b944-49d8-92d4-82869fe6a080'),\n  AIMessage(content='Hello, Bob! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 23, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 34, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1772041071-FX512nlXwz3nR37T6Zll', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c95e0-e977-78d1-8fa0-34b8b16796a9-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 23, 'output_tokens': 48, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 34}}),\n  HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={}, id='188a023b-3cfb-4320-980a-38e22e832b3c'),\n  AIMessage(content='Yes, I remember you‚ÄîBob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 58, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 35, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1772041072-8NmU1cgWG5nZfNVhMbfz', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c95e0-ef3d-72c3-896a-84a386c9b951-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 58, 'output_tokens': 44, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 35}})]}\n\n\n\nIn production\nIn production, use a checkpointer backed by a database:\nuv add langgraph-checkpoint-postgres\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    checkpointer.setup() # auto create tables in PostgresSql\n    agent = create_agent(\n        model=model_nemotron3_nano,\n        checkpointer=checkpointer,\n    )\n\n\n\n\n\n\nNote\n\n\n\nFor more checkpointer options including SQLite, Postgres, and Azure Cosmos DB, see the list of checkpointer libraries in the Persistence documentation."
  },
  {
    "objectID": "L01/Short-term memory.html#longer-conversations",
    "href": "L01/Short-term memory.html#longer-conversations",
    "title": "Short-term memory",
    "section": "Longer conversations",
    "text": "Longer conversations\nLong conversations can exceed the LLM‚Äôs context window. Common solutions are:\n\nTrim messages: Remove first or last N messages (before calling LLM)\nDelete messages: Remove messages from LangGraph state permanently\nSummarize messages: Summarize earlier messages in the history and replace them with a summary\nCustom strategies: Custom strategies (e.g., message filtering, etc.)\n\nCheckout: Common patterns for more details."
  },
  {
    "objectID": "L01/Short-term memory.html#customizing-agent-memory",
    "href": "L01/Short-term memory.html#customizing-agent-memory",
    "title": "Short-term memory",
    "section": "Customizing agent memory",
    "text": "Customizing agent memory\nBy default, agents use AgentState to manage short term memory, specifically the conversation history via a messages key.\nYou can extend AgentState to add additional fields. Custom state schemas are passed to create_agent using the state_schema parameter.\n\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass CustomAgentState(AgentState):\n    user_id: str\n    preferences: dict\n\nagent = create_agent(\n    model=model_nemotron3_nano,\n    state_schema=CustomAgentState,\n    checkpointer=InMemorySaver(),\n)\n\n\n# Custom state can be passed in invoke\nresult = agent.invoke(\n    input={\n        \"messages\": HumanMessage(\"Hello\"),\n        \"user_id\": \"user_123\",\n        \"preferences\": {\"theme\": \"dark\"}\n    },\n    config={\"configurable\": {\"thread_id\": \"3\"}}\n)\n\n\nresult\n\n{'messages': [HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}, id='328e115a-09f2-4f3e-b918-ea68d0927002'),\n  AIMessage(content='Hello! How can I assist you today? üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 17, 'total_tokens': 95, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 71, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1772041155-3QDYNCUGKirXKgdvosKS', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c95e2-314d-7321-9231-eded1572a7ea-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 17, 'output_tokens': 78, 'total_tokens': 95, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 71}})],\n 'user_id': 'user_123',\n 'preferences': {'theme': 'dark'}}"
  },
  {
    "objectID": "L01/Short-term memory.html#access-memory",
    "href": "L01/Short-term memory.html#access-memory",
    "title": "Short-term memory",
    "section": "Access memory",
    "text": "Access memory\nYou can access and modify the short-term memory (state) of an agent in several ways:\n\nTools\n\nRead short-term memory in a tool\nAccess short term memory (state) in a tool using the runtime parameter (typed as ToolRuntime).\nThe runtime parameter is hidden from the tool signature (so the model doesn‚Äôt see it), but the tool can access the state through it.\n\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.tools import tool, ToolRuntime\n\n\nclass CustomState(AgentState):\n    user_id: str\n\n@tool\ndef get_user_info(runtime: ToolRuntime) -&gt; str:\n    \"\"\"Look up user info.\"\"\"\n    user_id = runtime.state[\"user_id\"]\n    if user_id == \"user_123\":\n        return \"User is John Smith\"\n    else:\n        \"Unknown user\"\n\n\nagent = create_agent(\n    model=model_nemotron3_nano,\n    tools=[get_user_info],\n    state_schema=CustomState,\n)\n\n\n\nresult = agent.invoke({\n    \"messages\": \"look up user information\",\n    \"user_id\": \"user_123\"\n})\nprint(result[\"messages\"][-1].content)\n\nThe user information has been retrieved successfully. The user is **John Smith**. Let me know if you need further assistance!\n\n\n\n\nWrite short-term memory from tools\nTo modify the agent‚Äôs short-term memory (state) during execution, you can return state updates directly from the tools.\nThis is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.messages import ToolMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.types import Command\nfrom pydantic import BaseModel\n\n\nclass CustomState(AgentState):\n    user_name: str\n\nclass CustomContext(BaseModel):\n    user_id: str\n\n@tool\ndef update_user_info(\n    runtime: ToolRuntime[CustomContext, CustomState],\n) -&gt; Command:\n    \"\"\"Look up and update user info.\"\"\"\n    user_id = runtime.context.user_id\n    name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n    return Command(update={\n        \"user_name\": name,\n        # update the message history\n        \"messages\": [\n            ToolMessage(\n                \"Successfully looked up user information\",\n                tool_call_id=runtime.tool_call_id\n            )\n        ]\n    })\n\n@tool\ndef greet(\n    runtime: ToolRuntime[CustomContext, CustomState]\n) -&gt; str | Command:\n    \"\"\"Use this to greet the user once you found their info.\"\"\"\n    user_name = runtime.state.get(\"user_name\", None)\n    if user_name is None:\n       return Command(update={\n            \"messages\": [\n                ToolMessage(\n                    \"Please call the 'update_user_info' tool it will get and update the user's name.\",\n                    tool_call_id=runtime.tool_call_id\n                )\n            ]\n        })\n    return f\"Hello {user_name}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[update_user_info, greet],\n    state_schema=CustomState, # [!code highlight]\n    context_schema=CustomContext,\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]},\n    context=CustomContext(user_id=\"user_123\"),\n)"
  },
  {
    "objectID": "L01/xx_retriveal.html",
    "href": "L01/xx_retriveal.html",
    "title": "Retrieval",
    "section": "",
    "text": "Large Language Models (LLMs) are powerful, but they have two key limitations:\nRetrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of Retrieval-Augmented Generation (RAG): enhancing an LLM‚Äôs answers with context-specific information."
  },
  {
    "objectID": "L01/xx_retriveal.html#building-a-knowledge-base",
    "href": "L01/xx_retriveal.html#building-a-knowledge-base",
    "title": "Retrieval",
    "section": "Building a knowledge base",
    "text": "Building a knowledge base\nA knowledge base is a repository of documents or structured data used during retrieval.\nIf you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do not need to rebuild it. You can:\n\nConnect it as a tool for an agent in Agentic RAG.\nQuery it and supply the retrieved content as context to the LLM (2-Step RAG).\n\nIf you need a custom knowledge base, you can use LangChain‚Äôs document loaders and vector stores to build one from your own data."
  },
  {
    "objectID": "L01/xx_retriveal.html#retrieval-pipeline",
    "href": "L01/xx_retriveal.html#retrieval-pipeline",
    "title": "Retrieval",
    "section": "Retrieval pipeline",
    "text": "Retrieval pipeline\nA typical retrieval workflow looks like this:\n\n\n\n\n\nflowchart TD\n  S([\"Sources&lt;br&gt;(Google Drive, Slack, Notion, etc.)\"]) --&gt; L[Document Loaders]\n  L --&gt; A([Documents])\n  A --&gt; B[Split into chunks]\n  B --&gt; C[Turn into embeddings]\n  C --&gt; D[(Vector Store)]\n  Q([User Query]) --&gt; E[Query embedding]\n  E --&gt; D\n  D --&gt; F[Retriever]\n  F --&gt; G[LLM uses retrieved info]\n  G --&gt; H([Answer])\n\n\n\n\n\n\nEach component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app‚Äôs logic."
  },
  {
    "objectID": "L01/xx_retriveal.html#building-blocks",
    "href": "L01/xx_retriveal.html#building-blocks",
    "title": "Retrieval",
    "section": "Building blocks",
    "text": "Building blocks\n\n1. Load\nDocument loaders Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized Document objects.\n\n\n2. Split\nText splitters Break large docs into smaller chunks that will be retrievable individually.\nWhy do we split? ..\n\nA. Navigating Context Window Limits\nEven the most advanced Large Language Models (LLMs) have a context window‚Äîa maximum number of tokens (words/characters) they can ‚Äúsee‚Äù at one time.\n\nThe Problem: Many documents (legal contracts, technical manuals, books) are significantly larger than an LLM‚Äôs context window.\nThe Solution: By splitting a 500-page manual into 500-word chunks, an agent can ‚Äúpull‚Äù only the relevant pieces into its memory to answer a specific query without hitting a technical ceiling.\n\n\n\nB. Improving Retrieval Accuracy (RAG)\nMost agentic systems use Retrieval-Augmented Generation (RAG). This process relies on converting text into ‚Äúembeddings‚Äù (mathematical vectors) to find similarities.\n\nSemantic Density: A 50-page document covers many different topics. If you turn that entire document into one vector, the ‚Äúmeaning‚Äù becomes diluted and fuzzy.\nPrecision: Chunking allows the agent to find the exact paragraph that answers a question. It‚Äôs easier for a system to match the query ‚ÄúWhat is the refund policy?‚Äù to a specific 200-word chunk about refunds than to a 10,000-word general Terms of Service document.\n\n\n\nC. Reducing ‚ÄúLost in the Middle‚Äù Phenomena\nResearch shows that LLMs are best at recalling information located at the very beginning or the very end of a prompt. Information buried in the middle of a massive block of text is often ignored or ‚Äúhallucinated‚Äù away.\nBy providing the agent with several small, concise chunks, you ensure the relevant information stays at the ‚Äútop of mind‚Äù for the model, leading to higher reasoning accuracy.\n\n\nD. Cost and Latency Optimization\nEvery token sent to an LLM costs money and takes time to process.\n\nEfficiency: If an agent needs to know a specific date in a 100MB file, sending the whole file is expensive and slow.\nSpeed: Sending three 500-token chunks is nearly instantaneous and costs a fraction of the price, allowing the agent to move through its task pipeline much faster.\n\n\n\n\n3. Embed\nEmbedding models An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\n\n\n4. Store\nVector stores Specialized databases for storing and searching embeddings.\n\n\n5. Retrieve\nRetrievers A retriever is an interface that returns documents given an unstructured query."
  },
  {
    "objectID": "L01/xx_retriveal.html#rag-architectures",
    "href": "L01/xx_retriveal.html#rag-architectures",
    "title": "Retrieval",
    "section": "RAG architectures",
    "text": "RAG architectures\nRAG can be implemented in multiple ways, depending on your system‚Äôs needs. We outline each type in the sections below.\n\n\n\n\n\n\n\n\n\n\n\nArchitecture\nDescription\nControl\nFlexibility\nLatency\nExample Use Case\n\n\n\n\n2-Step RAG\nRetrieval always happens before generation. Simple and predictable\n‚úÖ High\n‚ùå Low\n‚ö° Fast\nFAQs, documentation bots\n\n\nAgentic RAG\nAn LLM-powered agent decides when and how to retrieve during reasoning\n‚ùå Low\n‚úÖ High\n‚è≥ Variable\nResearch assistants with access to multiple tools\n\n\nHybrid\nCombines characteristics of both approaches with validation steps\n‚öñÔ∏è Medium\n‚öñÔ∏è Medium\n‚è≥ Variable\nDomain-specific Q&A with quality validation\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLatency: Latency is generally more predictable in 2-Step RAG, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps‚Äîsuch as API response times, network delays, or database queries‚Äîwhich can vary based on the tools and infrastructure in use.\n\n\n\n2-step RAG\nIn 2-Step RAG, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.\n\n\n\n\n\ngraph LR\n    A[User Question] --&gt; B[\"Retrieve Relevant Documents\"]\n    B --&gt; C[\"Generate Answer\"]\n    C --&gt; D[Return Answer to User]\n\n    %% Styling\n    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n\n    class A,D startend\n    class B,C process\n\n\n\n\n\n\n See how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation. This tutorial walks through two approaches:\n\nA RAG agent that runs searches with a flexible tool‚Äîgreat for general-purpose use.\nA 2-step RAG chain that requires just one LLM call per query‚Äîfast and efficient for simpler tasks. \n\n\n\nAgentic RAG\nAgentic Retrieval-Augmented Generation (RAG) combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides when and how to retrieve information during the interaction.\n\nThe only thing an agent needs to enable RAG behavior is access to one or more tools that can fetch external knowledge ‚Äî such as documentation loaders, web APIs, or database queries.\n\n\n\n\n\n\ngraph LR\n    A[User Input / Question] --&gt; B[\"Agent (LLM)\"]\n    B --&gt; C{Need external info?}\n    C -- Yes --&gt; D[\"Search using tool(s)\"]\n    D --&gt; H{Enough to answer?}\n    H -- No --&gt; B\n    H -- Yes --&gt; I[Generate final answer]\n    C -- No --&gt; I\n    I --&gt; J[Return to user]\n\n    %% Dark-mode friendly styling\n    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n    classDef decision fill:#f9a825,stroke:#f57f17,stroke-width:2px,color:#000\n    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n\n    class A,J startend\n    class B,D,I process\n    class C,H decision\n\n\n\n\n\n\nimport requests\nfrom langchain.tools import tool\nfrom langchain.chat_models import init_chat_model\nfrom langchain.agents import create_agent\n\n\n@tool\ndef fetch_url(url: str) -&gt; str:\n    \"\"\"Fetch text content from a URL\"\"\"\n    response = requests.get(url, timeout=10.0)\n    response.raise_for_status()\n    return response.text\n\nsystem_prompt = \"\"\"\\\nUse fetch_url when you need to fetch information from a web-page; quote relevant snippets.\n\"\"\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[fetch_url], # A tool for retrieval\n    system_prompt=system_prompt,\n)\n This example implements an Agentic RAG system to assist users in querying LangGraph documentation. The agent begins by loading llms.txt, which lists available documentation URLs, and can then dynamically use a fetch_documentation tool to retrieve and process the relevant content based on the user‚Äôs question.\nimport requests\nfrom langchain.agents import create_agent\nfrom langchain.messages import HumanMessage\nfrom langchain.tools import tool\nfrom markdownify import markdownify\n\n\nALLOWED_DOMAINS = [\"https://langchain-ai.github.io/\"]\nLLMS_TXT = 'https://langchain-ai.github.io/langgraph/llms.txt'\n\n\n@tool\ndef fetch_documentation(url: str) -&gt; str:\n    \"\"\"Fetch and convert documentation from a URL\"\"\"\n    if not any(url.startswith(domain) for domain in ALLOWED_DOMAINS):\n        return (\n            \"Error: URL not allowed. \"\n            f\"Must start with one of: {', '.join(ALLOWED_DOMAINS)}\"\n        )\n    response = requests.get(url, timeout=10.0)\n    response.raise_for_status()\n    return markdownify(response.text)\n\n\n# We will fetch the content of llms.txt, so this can\n# be done ahead of time without requiring an LLM request.\nllms_txt_content = requests.get(LLMS_TXT).text\n\n# System prompt for the agent\nsystem_prompt = f\"\"\"\nYou are an expert Python developer and technical assistant.\nYour primary role is to help users with questions about LangGraph and related tools.\n\nInstructions:\n\n1. If a user asks a question you're unsure about ‚Äî or one that likely involves API usage,\n   behavior, or configuration ‚Äî you MUST use the `fetch_documentation` tool to consult the relevant docs.\n2. When citing documentation, summarize clearly and include relevant context from the content.\n3. Do not use any URLs outside of the allowed domain.\n4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.\n\nYou can access official documentation from the following approved sources:\n\n{llms_txt_content}\n\nYou MUST consult the documentation to get up to date documentation\nbefore answering a user's question about LangGraph.\n\nYour answers should be clear, concise, and technically accurate.\n\"\"\"\n\ntools = [fetch_documentation]\n\nmodel = init_chat_model(\"claude-sonnet-4-0\", max_tokens=32_000)\n\nagent = create_agent(\n    model=model,\n    tools=tools,  # [!code highlight]\n    system_prompt=system_prompt,  # [!code highlight]\n    name=\"Agentic RAG\",\n)\n\nresponse = agent.invoke({\n    'messages': [\n        HumanMessage(content=(\n            \"Write a short example of a langgraph agent using the \"\n            \"prebuilt create react agent. the agent should be able \"\n            \"to look up stock pricing information.\"\n        ))\n    ]\n})\n\nprint(response['messages'][-1].content)\n\n See how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation. This tutorial walks through two approaches:\n\nA RAG agent that runs searches with a flexible tool‚Äîgreat for general-purpose use.\nA 2-step RAG chain that requires just one LLM call per query‚Äîfast and efficient for simpler tasks. \n\n\n\nHybrid RAG\nHybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.\nTypical components include:\n\nQuery enhancement: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.\nRetrieval validation: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.\nAnswer validation: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.\n\nThe architecture often supports multiple iterations between these steps:\n\n\n\n\n\ngraph LR\n    A[User Question] --&gt; B[Query Enhancement]\n    B --&gt; C[Retrieve Documents]\n    C --&gt; D{Sufficient Info?}\n    D -- No --&gt; E[Refine Query]\n    E --&gt; C\n    D -- Yes --&gt; F[Generate Answer]\n    F --&gt; G{Answer Quality OK?}\n    G -- No --&gt; H{Try Different Approach?}\n    H -- Yes --&gt; E\n    H -- No --&gt; I[Return Best Answer]\n    G -- Yes --&gt; I\n    I --&gt; J[Return to User]\n\n    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n    classDef decision fill:#f9a825,stroke:#f57f17,stroke-width:2px,color:#000\n    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n\n    class A,J startend\n    class B,C,E,F,I process\n    class D,G,H decision\n\n\n\n\n\n\nThis architecture is suitable for:\n\nApplications with ambiguous or underspecified queries\nSystems that require validation or quality control steps\nWorkflows involving multiple sources or iterative refinement\n\n An example of Hybrid RAG that combines agentic reasoning with retrieval and self-correction."
  },
  {
    "objectID": "L01/04_semantic-search.html",
    "href": "L01/04_semantic-search.html",
    "title": "Build a semantic search engine with LangChain",
    "section": "",
    "text": "This tutorial will familiarize you with LangChain‚Äôs document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data‚Äì from (vector) databases and other sources ‚Äì for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG.\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.\n\n\nThis guide focuses on retrieval of text data. We will cover the following concepts:\n\nDocuments and document loaders;\nText splitters;\nEmbeddings;\nVector stores and retrievers."
  },
  {
    "objectID": "L01/04_semantic-search.html#overview",
    "href": "L01/04_semantic-search.html#overview",
    "title": "Build a semantic search engine with LangChain",
    "section": "",
    "text": "This tutorial will familiarize you with LangChain‚Äôs document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data‚Äì from (vector) databases and other sources ‚Äì for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG.\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.\n\n\nThis guide focuses on retrieval of text data. We will cover the following concepts:\n\nDocuments and document loaders;\nText splitters;\nEmbeddings;\nVector stores and retrievers."
  },
  {
    "objectID": "L01/04_semantic-search.html#setup",
    "href": "L01/04_semantic-search.html#setup",
    "title": "Build a semantic search engine with LangChain",
    "section": "Setup",
    "text": "Setup\n\nInstallation\nThis tutorial requires the langchain-community and pypdf packages. Using uv:\n!uv add langchain-community pypdf\nFor more details, see our Installation guide."
  },
  {
    "objectID": "L01/04_semantic-search.html#documents-and-document-loaders",
    "href": "L01/04_semantic-search.html#documents-and-document-loaders",
    "title": "Build a semantic search engine with LangChain",
    "section": "1. Documents and document loaders",
    "text": "1. Documents and document loaders\nLangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n\npage_content: a string representing the content;\nmetadata: a dict containing arbitrary metadata;\nid: (optional) a string identifier for the document.\n\nThe metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\nWe can generate sample documents when desired:\n\nfrom langchain_core.documents import Document\n\ndocuments = [\n    Document(\n        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n        metadata={\"source\": \"mammal-pets-doc\"},\n    ),\n    Document(\n        page_content=\"Cats are independent pets that often enjoy their own space.\",\n        metadata={\"source\": \"mammal-pets-doc\"},\n    ),\n]\n\nHowever, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application.\n\nLoading documents\nLet‚Äôs load a PDF into a sequence of Document objects. First, fetch a PDF from arXiv (e.g.¬†the ‚ÄúAttention Is All You Need‚Äù paper) using curl:\ncurl -L -o paper.pdf \"https://arxiv.org/pdf/1706.03762.pdf\"\nThen load it with the PyPDFLoader:\n\nfrom langchain_community.document_loaders import PyPDFLoader\n\nfile_path = \"paper.pdf\"\nloader = PyPDFLoader(file_path)\n\ndocs = loader.load()\n\nprint(len(docs))\n\n15\n\n\nPyPDFLoader loads one Document object per PDF page. For each, we can easily access:\n\nThe string content of the page;\nMetadata containing the file name and page number.\n\n\nprint(f\"{docs[0].page_content[:200]}\\n\")\nprint(docs[0].metadata)\n\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\n\n\n{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n\n\n\n\nSplitting\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not ‚Äúwashed out‚Äù by surrounding text.\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nWe set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute ‚Äústart_index‚Äù.\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=200, add_start_index=True\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(len(all_splits))\n\n52"
  },
  {
    "objectID": "L01/04_semantic-search.html#embeddings",
    "href": "L01/04_semantic-search.html#embeddings",
    "title": "Build a semantic search engine with LangChain",
    "section": "2. Embeddings",
    "text": "2. Embeddings\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\nLangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. We use HuggingFace with the all-mpnet-base-v2 sentence transformer:\nuv add langchain-huggingface sentence-transformers\n\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n\n/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nvector_1 = embeddings.embed_query(all_splits[0].page_content)\nvector_2 = embeddings.embed_query(all_splits[1].page_content)\n\nassert len(vector_1) == len(vector_2)\nprint(f\"Generated vectors of length {len(vector_1)}\\n\")\nprint(vector_1[:10])\n\nGenerated vectors of length 768\n\n[0.00369695364497602, 0.017323777079582214, -0.01369203720241785, 0.00036987385828979313, -0.05261596664786339, -0.0010653170756995678, 0.002715452341362834, -0.02163930982351303, -0.06304091215133667, -0.0036578048020601273]\n\n\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search."
  },
  {
    "objectID": "L01/04_semantic-search.html#vector-stores",
    "href": "L01/04_semantic-search.html#vector-stores",
    "title": "Build a semantic search engine with LangChain",
    "section": "3. Vector stores",
    "text": "3. Vector stores\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\nLangChain includes a suite of integrations with different vector store technologies. For this tutorial we use the in-memory vector store, which is lightweight and requires no extra infrastructure:\n\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nvector_store = InMemoryVectorStore(embeddings)\n\nHaving instantiated our vector store, we can now index the documents.\n\nids = vector_store.add_documents(documents=all_splits)\n\nNote that most vector store implementations will allow you to connect to an existing vector store‚Äì e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail.\nOnce we‚Äôve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\n\nSynchronously and asynchronously;\nBy string query and by vector;\nWith and without returning similarity scores;\nBy similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results).\n\nThe methods will generally include a list of Document objects in their outputs.\nUsage\nEmbeddings typically represent text as a ‚Äúdense‚Äù vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\nReturn documents based on similarity to a string query:\n\nresults = vector_store.similarity_search(\n    \"What is the main architecture proposed in the paper?\"\n)\n\nprint(results[0])\n\npage_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 770}\n\n\nAsync query:\n\nresults = await vector_store.asimilarity_search(\"What is self-attention?\")\n\nprint(results[0])\n\npage_content='3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 1611}\n\n\nReturn scores:\n\n# Note that providers implement different scores; the score here\n# is a distance metric that varies inversely with similarity.\n\nresults = vector_store.similarity_search_with_score(\"How does the Transformer encoder work?\")\ndoc, score = results[0]\nprint(f\"Score: {score}\\n\")\nprint(doc)\n\nScore: 0.668251152922073\n\npage_content='Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 0}\n\n\nReturn documents based on similarity to an embedded query:\n\nembedding = embeddings.embed_query(\"What are the advantages of the Transformer over RNNs?\")\n\nresults = vector_store.similarity_search_by_vector(embedding)\nprint(results[0])\n\npage_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop œµls\ntrain PPL BLEU params\nsteps (dev) (dev) √ó106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'start_index': 0}\n\n\nLearn more:\n\nAPI Reference\nIntegration-specific docs"
  },
  {
    "objectID": "L01/04_semantic-search.html#retrievers",
    "href": "L01/04_semantic-search.html#retrievers",
    "title": "Build a semantic search engine with LangChain",
    "section": "4. Retrievers",
    "text": "4. Retrievers\nVectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\n\nretriever = vector_store.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 1},\n)\n\nretriever.batch(\n    [\n        \"What is the main architecture proposed in the paper?\",\n        \"What is self-attention?\",\n    ],\n)\n\n[[Document(id='3a9a01f8-683c-471c-9dda-ba075d260afd', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 770}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,')],\n [Document(id='417ae00c-a9d5-49ee-b5f3-2a8ecb5cd8ed', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 1611}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3')]]\n\n\nVectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and \"similarity_score_threshold\". We can use the latter to threshold documents output by the retriever by similarity score.\nRetrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial."
  },
  {
    "objectID": "L01/04_semantic-search.html#next-steps",
    "href": "L01/04_semantic-search.html#next-steps",
    "title": "Build a semantic search engine with LangChain",
    "section": "Next steps",
    "text": "Next steps\nYou‚Äôve now seen how to build a semantic search engine over a PDF document.\nFor more on document loaders:\n\nOverview\nAvailable integrations\n\nFor more on embeddings:\n\nOverview\nAvailable integrations\n\nFor more on vector stores:\n\nOverview\nAvailable integrations\n\nFor more on RAG, see:\n\nBuild a Retrieval Augmented Generation (RAG) App"
  },
  {
    "objectID": "L01/04_semantic-search.html#activity",
    "href": "L01/04_semantic-search.html#activity",
    "title": "Build a semantic search engine with LangChain",
    "section": "Activity",
    "text": "Activity\nOver to you: Apply the Load, Embed, Store, and Retrieve, on another document."
  },
  {
    "objectID": "L01/06_components.html",
    "href": "L01/06_components.html",
    "title": "Components of RAG Systems",
    "section": "",
    "text": "Source.\nLangChain‚Äôs power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components."
  },
  {
    "objectID": "L01/06_components.html#core-component-ecosystem",
    "href": "L01/06_components.html#core-component-ecosystem",
    "title": "Components of RAG Systems",
    "section": "Core component ecosystem",
    "text": "Core component ecosystem\nThe diagram below shows how LangChain‚Äôs major components connect to form complete AI applications:\n\n\n\n\n\ngraph TD\n    %% Input processing\n    subgraph \"üì• Input processing\"\n        A[Text input] --&gt; B[Document loaders]\n        B --&gt; C[Text splitters]\n        C --&gt; D[Documents]\n    end\n\n    %% Embedding & storage\n    subgraph \"üî¢ Embedding & storage\"\n        D --&gt; E[Embedding models]\n        E --&gt; F[Vectors]\n        F --&gt; G[(Vector stores)]\n    end\n\n    %% Retrieval\n    subgraph \"üîç Retrieval\"\n        H[User Query] --&gt; I[Embedding models]\n        I --&gt; J[Query vector]\n        J --&gt; K[Retrievers]\n        K --&gt; G\n        G --&gt; L[Relevant context]\n    end\n\n    %% Generation\n    subgraph \"ü§ñ Generation\"\n        M[Chat models] --&gt; N[Tools]\n        N --&gt; O[Tool results]\n        O --&gt; M\n        L --&gt; M\n        M --&gt; P[AI response]\n    end\n\n    %% Orchestration\n    subgraph \"üéØ Orchestration\"\n        Q[Agents] --&gt; M\n        Q --&gt; N\n        Q --&gt; K\n        Q --&gt; R[Memory]\n    end\n\n\n\n\n\n\n\nHow components connect\nEach component layer builds on the previous ones:\n\nInput processing ‚Äì Transform raw data into structured documents\nEmbedding & storage ‚Äì Convert text into searchable vector representations\nRetrieval ‚Äì Find relevant information based on user queries\nGeneration ‚Äì Use AI models to create responses, optionally with tools\nOrchestration ‚Äì Coordinate everything through agents and memory systems"
  },
  {
    "objectID": "L01/06_components.html#component-categories",
    "href": "L01/06_components.html#component-categories",
    "title": "Component architecture",
    "section": "Component categories",
    "text": "Component categories\nLangChain organizes components into these main categories:\n\n\n\nCategory\nPurpose\nKey Components\nUse Cases\n\n\n\n\nModels\nAI reasoning and generation\nChat models, LLMs, Embedding models\nText generation, reasoning, semantic understanding\n\n\nTools\nExternal capabilities\nAPIs, databases, etc.\nWeb search, data access, computations\n\n\nAgents\nOrchestration and reasoning\nReAct agents, tool calling agents\nNondeterministic workflows, decision making\n\n\nMemory\nContext preservation\nMessage history, custom state\nConversations, stateful interactions\n\n\nRetrievers\nInformation access\nVector retrievers, web retrievers\nRAG, knowledge base search\n\n\nDocument processing\nData ingestion\nLoaders, splitters, transformers\nPDF processing, web scraping\n\n\nVector Stores\nSemantic search\nChroma, Pinecone, FAISS\nSimilarity search, embeddings storage"
  },
  {
    "objectID": "L01/06_components.html#common-patterns",
    "href": "L01/06_components.html#common-patterns",
    "title": "Component architecture",
    "section": "Common patterns",
    "text": "Common patterns"
  },
  {
    "objectID": "L01/00_intro.html#component-categories",
    "href": "L01/00_intro.html#component-categories",
    "title": "Introduction: What is Agentic AI?",
    "section": "Component categories",
    "text": "Component categories\nLangChain organizes components into these main categories:\n\n\n\nCategory\nPurpose\nKey Components\nUse Cases\n\n\n\n\nModels\nAI reasoning and generation\nChat models, LLMs, Embedding models\nText generation, reasoning, semantic understanding\n\n\nTools\nExternal capabilities\nAPIs, databases, etc.\nWeb search, data access, computations\n\n\nAgents\nOrchestration and reasoning\nReAct agents, tool calling agents\nNondeterministic workflows, decision making\n\n\nMemory\nContext preservation\nMessage history, custom state\nConversations, stateful interactions\n\n\nRetrievers\nInformation access\nVector retrievers, web retrievers\nRAG, knowledge base search\n\n\nDocument processing\nData ingestion\nLoaders, splitters, transformers\nPDF processing, web scraping\n\n\nVector Stores\nSemantic search\nChroma, Pinecone, FAISS\nSimilarity search, embeddings storage\n\n\n\nExamples of Agentic frameworks are:\n\nLangChain\nVercel‚Äôs AI SDK\nCrewAI\nOpenAI Agents SDK\nGoogle ADK\nLlamaIndex"
  },
  {
    "objectID": "L01/06_RAG_components.html",
    "href": "L01/06_RAG_components.html",
    "title": "Components of RAG Systems",
    "section": "",
    "text": "Source.\nLangChain‚Äôs power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components."
  },
  {
    "objectID": "L01/06_RAG_components.html#core-component-ecosystem",
    "href": "L01/06_RAG_components.html#core-component-ecosystem",
    "title": "Components of RAG Systems",
    "section": "Core component ecosystem",
    "text": "Core component ecosystem\nThe diagram below shows how LangChain‚Äôs major components connect to form complete AI applications:\n\n\n\n\n\ngraph TD\n    %% Input processing\n    subgraph \"üì• Input processing\"\n        A[Text input] --&gt; B[Document loaders]\n        B --&gt; C[Text splitters]\n        C --&gt; D[Documents]\n    end\n\n    %% Embedding & storage\n    subgraph \"üî¢ Embedding & storage\"\n        D --&gt; E[Embedding models]\n        E --&gt; F[Vectors]\n        F --&gt; G[(Vector stores)]\n    end\n\n    %% Retrieval\n    subgraph \"üîç Retrieval\"\n        H[User Query] --&gt; I[Embedding models]\n        I --&gt; J[Query vector]\n        J --&gt; K[Retrievers]\n        K --&gt; G\n        G --&gt; L[Relevant context]\n    end\n\n    %% Generation\n    subgraph \"ü§ñ Generation\"\n        M[Chat models] --&gt; N[Tools]\n        N --&gt; O[Tool results]\n        O --&gt; M\n        L --&gt; M\n        M --&gt; P[AI response]\n    end\n\n    %% Orchestration\n    subgraph \"üéØ Orchestration\"\n        Q[Agents] --&gt; M\n        Q --&gt; N\n        Q --&gt; K\n        Q --&gt; R[Memory]\n    end\n\n\n\n\n\n\n\nHow components connect\nEach component layer builds on the previous ones:\n\nInput processing ‚Äì Transform raw data into structured documents\nEmbedding & storage ‚Äì Convert text into searchable vector representations\nRetrieval ‚Äì Find relevant information based on user queries\nGeneration ‚Äì Use AI models to create responses, optionally with tools\nOrchestration ‚Äì Coordinate everything through agents and memory systems"
  },
  {
    "objectID": "L01/06_RAG_agentic_systems.html",
    "href": "L01/06_RAG_agentic_systems.html",
    "title": "RAG in Agentic Systems",
    "section": "",
    "text": "Source.\nLangChain‚Äôs power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components.\nThe diagram below shows how LangChain‚Äôs major components connect to form complete AI applications:\n\n\n\n\n\ngraph TD\n    %% Input processing\n    subgraph \"üì• Input processing\"\n        A[Text input] --&gt; B[Document loaders]\n        B --&gt; C[Text splitters]\n        C --&gt; D[Documents]\n    end\n\n    %% Embedding & storage\n    subgraph \"üî¢ Embedding & storage\"\n        D --&gt; E[Embedding models]\n        E --&gt; F[Vectors]\n        F --&gt; G[(Vector stores)]\n    end\n\n    %% Retrieval\n    subgraph \"üîç Retrieval\"\n        H[User Query] --&gt; I[Embedding models]\n        I --&gt; J[Query vector]\n        J --&gt; K[Retrievers]\n        K --&gt; G\n        G --&gt; L[Relevant context]\n    end\n\n    %% Generation\n    subgraph \"ü§ñ Generation\"\n        M[Chat models] --&gt; N[Tools]\n        N --&gt; O[Tool results]\n        O --&gt; M\n        L --&gt; M\n        M --&gt; P[AI response]\n    end\n\n    %% Orchestration\n    subgraph \"üéØ Orchestration\"\n        Q[Agents] --&gt; M\n        Q --&gt; N\n        Q --&gt; K\n        Q --&gt; R[Memory]\n    end\n\n\n\n\n\n\nEach component layer builds on the previous ones:\n\nInput processing ‚Äì Transform raw data into structured documents\nEmbedding & storage ‚Äì Convert text into searchable vector representations\nRetrieval ‚Äì Find relevant information based on user queries\nGeneration ‚Äì Use AI models to create responses, optionally with tools\nOrchestration ‚Äì Coordinate everything through agents and memory systems"
  },
  {
    "objectID": "L01/06_RAG_agentic_systems.html#core-component-ecosystem",
    "href": "L01/06_RAG_agentic_systems.html#core-component-ecosystem",
    "title": "RAG in Agentic Systems",
    "section": "Core component ecosystem",
    "text": "Core component ecosystem\nThe diagram below shows how LangChain‚Äôs major components connect to form complete AI applications:\n\n\n\n\n\ngraph TD\n    %% Input processing\n    subgraph \"üì• Input processing\"\n        A[Text input] --&gt; B[Document loaders]\n        B --&gt; C[Text splitters]\n        C --&gt; D[Documents]\n    end\n\n    %% Embedding & storage\n    subgraph \"üî¢ Embedding & storage\"\n        D --&gt; E[Embedding models]\n        E --&gt; F[Vectors]\n        F --&gt; G[(Vector stores)]\n    end\n\n    %% Retrieval\n    subgraph \"üîç Retrieval\"\n        H[User Query] --&gt; I[Embedding models]\n        I --&gt; J[Query vector]\n        J --&gt; K[Retrievers]\n        K --&gt; G\n        G --&gt; L[Relevant context]\n    end\n\n    %% Generation\n    subgraph \"ü§ñ Generation\"\n        M[Chat models] --&gt; N[Tools]\n        N --&gt; O[Tool results]\n        O --&gt; M\n        L --&gt; M\n        M --&gt; P[AI response]\n    end\n\n    %% Orchestration\n    subgraph \"üéØ Orchestration\"\n        Q[Agents] --&gt; M\n        Q --&gt; N\n        Q --&gt; K\n        Q --&gt; R[Memory]\n    end\n\n\n\n\n\n\n\nHow components connect\nEach component layer builds on the previous ones:\n\nInput processing ‚Äì Transform raw data into structured documents\nEmbedding & storage ‚Äì Convert text into searchable vector representations\nRetrieval ‚Äì Find relevant information based on user queries\nGeneration ‚Äì Use AI models to create responses, optionally with tools\nOrchestration ‚Äì Coordinate everything through agents and memory systems"
  },
  {
    "objectID": "L01/use-cases.html#patterns",
    "href": "L01/use-cases.html#patterns",
    "title": "Use Cases",
    "section": "Patterns",
    "text": "Patterns\n\nAgentic RAG"
  }
]