<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Agents in LangChain – Building Agentic AI Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7dc3907ddb6ec99bf1b87f80a83abe8b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Building Agentic AI Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#questions" id="toc-questions" class="nav-link active" data-scroll-target="#questions">Questions</a></li>
  <li><a href="#setup-virtual-environment" id="toc-setup-virtual-environment" class="nav-link" data-scroll-target="#setup-virtual-environment">Setup Virtual Environment</a>
  <ul class="collapse">
  <li><a href="#activate-the-virtual-environment" id="toc-activate-the-virtual-environment" class="nav-link" data-scroll-target="#activate-the-virtual-environment">Activate the virtual environment</a></li>
  <li><a href="#install-dependencies" id="toc-install-dependencies" class="nav-link" data-scroll-target="#install-dependencies">Install dependencies</a></li>
  </ul></li>
  <li><a href="#openrouter" id="toc-openrouter" class="nav-link" data-scroll-target="#openrouter">OpenRouter</a></li>
  <li><a href="#what-is-an-api-key" id="toc-what-is-an-api-key" class="nav-link" data-scroll-target="#what-is-an-api-key">What is an API Key?</a>
  <ul class="collapse">
  <li><a href="#what-why" id="toc-what-why" class="nav-link" data-scroll-target="#what-why">What &amp; Why</a></li>
  <li><a href="#security-risks" id="toc-security-risks" class="nav-link" data-scroll-target="#security-risks">Security Risks</a></li>
  <li><a href="#how-to-set-your-api-key" id="toc-how-to-set-your-api-key" class="nav-link" data-scroll-target="#how-to-set-your-api-key">How to Set Your API Key?</a></li>
  </ul></li>
  <li><a href="#basic-usage" id="toc-basic-usage" class="nav-link" data-scroll-target="#basic-usage">Basic usage</a>
  <ul class="collapse">
  <li><a href="#running-a-model-locally" id="toc-running-a-model-locally" class="nav-link" data-scroll-target="#running-a-model-locally">Running a model locally</a></li>
  </ul></li>
  <li><a href="#key-methods" id="toc-key-methods" class="nav-link" data-scroll-target="#key-methods">Key Methods</a>
  <ul class="collapse">
  <li><a href="#invoke" id="toc-invoke" class="nav-link" data-scroll-target="#invoke">1. Invoke</a></li>
  </ul></li>
  <li><a href="#messages" id="toc-messages" class="nav-link" data-scroll-target="#messages">Messages</a>
  <ul class="collapse">
  <li><a href="#metadata" id="toc-metadata" class="nav-link" data-scroll-target="#metadata">Metadata</a></li>
  <li><a href="#conversation" id="toc-conversation" class="nav-link" data-scroll-target="#conversation">Conversation</a></li>
  <li><a href="#streaming-and-chunks" id="toc-streaming-and-chunks" class="nav-link" data-scroll-target="#streaming-and-chunks">2. Streaming and chunks</a></li>
  <li><a href="#batch" id="toc-batch" class="nav-link" data-scroll-target="#batch">3. Batch</a></li>
  </ul></li>
  <li><a href="#structured-output" id="toc-structured-output" class="nav-link" data-scroll-target="#structured-output">Structured output</a>
  <ul class="collapse">
  <li><a href="#benefits-of-structured-output" id="toc-benefits-of-structured-output" class="nav-link" data-scroll-target="#benefits-of-structured-output">Benefits of Structured Output</a></li>
  </ul></li>
  <li><a href="#llms-and-augmentations" id="toc-llms-and-augmentations" class="nav-link" data-scroll-target="#llms-and-augmentations">LLMs and augmentations</a></li>
  <li><a href="#tool-calling" id="toc-tool-calling" class="nav-link" data-scroll-target="#tool-calling">Tool calling</a>
  <ul class="collapse">
  <li><a href="#internet-search-tool" id="toc-internet-search-tool" class="nav-link" data-scroll-target="#internet-search-tool">Internet Search Tool</a></li>
  </ul></li>
  <li><a href="#create-an-agent" id="toc-create-an-agent" class="nav-link" data-scroll-target="#create-an-agent">Create an Agent</a>
  <ul class="collapse">
  <li><a href="#invoke-the-agent" id="toc-invoke-the-agent" class="nav-link" data-scroll-target="#invoke-the-agent">Invoke the agent</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#activity" id="toc-activity" class="nav-link" data-scroll-target="#activity">Activity</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="01_agents.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Agents in LangChain</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.</p>
<section id="questions" class="level2">
<h2 class="anchored" data-anchor-id="questions">Questions</h2>
<ul>
<li>What is an <em>Agent</em> in LangChain and how to make one?</li>
<li>What’s the relationship between an LLM and an Agent?</li>
<li>What can agents do?</li>
<li>How do I compare and select the best model for my agent?</li>
<li>Can I run an agent locally without a provider?</li>
</ul>
</section>
<section id="setup-virtual-environment" class="level2">
<h2 class="anchored" data-anchor-id="setup-virtual-environment">Setup Virtual Environment</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> init</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> venv <span class="at">-p</span> 3.12</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<section id="activate-the-virtual-environment" class="level3">
<h3 class="anchored" data-anchor-id="activate-the-virtual-environment">Activate the virtual environment</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="">Windows</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">MacOS / Linux</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">.venv\Scripts\activate.bat</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> .venv/bin/activate</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
</section>
<section id="install-dependencies" class="level3">
<h3 class="anchored" data-anchor-id="install-dependencies">Install dependencies</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> add ipykernel</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> add langchain langchain_openai langchain_community </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Note: <strong>Agents</strong> require <a href="https://openrouter.ai/models?fmt=cards&amp;supported_parameters=tools"><strong>a model that supports tool calling</strong></a>.</p>
</section>
</section>
<section id="openrouter" class="level2">
<h2 class="anchored" data-anchor-id="openrouter">OpenRouter</h2>
<p>OpenRouter is the Unified Interface For LLMs.</p>
<p>Key benefits include:</p>
<ul>
<li>One API for all models
<ul>
<li>no subscription to each provider needed</li>
<li>switch easily between models and providers by changing a <code>str</code> value</li>
</ul></li>
<li>Some models are free</li>
<li>Circumvent regional restrictions</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./assets/open_router.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Open Router"><img src="./assets/open_router.png" class="img-fluid figure-img" alt="Open Router"></a></p>
<figcaption>Open Router</figcaption>
</figure>
</div>
<p>If you don’t already have an OpenRouter API key, you can create one for free at: <a href="https://openrouter.ai/keys">OpenRouter</a>.</p>
</section>
<section id="what-is-an-api-key" class="level2">
<h2 class="anchored" data-anchor-id="what-is-an-api-key">What is an API Key?</h2>
<p>Think of an <strong>API Key</strong> as a <strong>hotel key card</strong>.</p>
<ul>
<li><strong>The Hotel (Server):</strong> Has resources (rooms) but keeps them locked.</li>
<li><strong>The Guest (Client):</strong> Wants access.</li>
<li><strong>The Key Card (API Key):</strong> Identifies you and proves you are allowed to enter specific rooms.</li>
</ul>
<hr>
<section id="what-why" class="level3">
<h3 class="anchored" data-anchor-id="what-why">What &amp; Why</h3>
<p>An API key is a unique string of characters used to identify the calling program.</p>
<ul>
<li><strong>Identification:</strong> Keys “authenticate the calling project,” allowing the server to recognize who is asking for data.</li>
<li><strong>Control:</strong> This lets the server track usage for billing and enforce limits (quotas) so one user doesn’t crash the system.</li>
</ul>
<hr>
</section>
<section id="security-risks" class="level3">
<h3 class="anchored" data-anchor-id="security-risks">Security Risks</h3>
<p>If you lose your key, it is like dropping your credit card.</p>
<ul>
<li><strong>Theft:</strong> Attackers can use your key to make requests on your behalf.</li>
<li><strong>Consequences:</strong> You suffer <strong>financial loss</strong> (paying for their usage) or <strong>service denial</strong> (they use up your available quota).</li>
</ul>
<blockquote class="blockquote">
<p><strong>Rule:</strong> Never post keys on public sites like GitHub.</p>
</blockquote>
</section>
<section id="how-to-set-your-api-key" class="level3">
<h3 class="anchored" data-anchor-id="how-to-set-your-api-key">How to Set Your API Key?</h3>
<p>Write your API key into an <code>.env</code> file as an environment variable, as follows:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="va">OPENROUTER_API_KEY</span><span class="op">=</span>...</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<blockquote class="blockquote">
<p>Note: make sure to add it to <code>.gitignore</code> to avoid committing it to the repository.</p>
<p>Note: this is different than the <code>.venv</code> file used for the virtual environment.</p>
</blockquote>
<p>If we use the OpenAI API, we’ll have to add:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="va">OPENAI_API_BASE</span><span class="op">=</span><span class="st">"https://openrouter.ai/api/v1"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>.. such that the model uses OpenRouter instead of the default OpenAI API.</p>
<div id="03e794b8" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>load_dotenv()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># We use OpenRouter for the agent — set OPENROUTER_API_KEY in .env</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Get your key at https://openrouter.ai/keys</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.environ.get(<span class="st">"OPENROUTER_API_KEY"</span>):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">raise</span> <span class="pp">RuntimeError</span>(</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:</span><span class="ch">\n</span><span class="st">"</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"OPENROUTER_API_KEY=your-openrouter-api-key"</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="basic-usage" class="level2">
<h2 class="anchored" data-anchor-id="basic-usage">Basic usage</h2>
<p>Models can be utilized in two ways:</p>
<ol type="1">
<li><strong>With agents</strong> - Models can be dynamically specified when creating an <a href="../oss/python/langchain/agents#model">agent</a>.</li>
<li><strong>Standalone</strong> - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.</li>
</ol>
<p><a href="https://docs.langchain.com/oss/python/langchain/models">Here</a> is a useful how-to for all the things that you can do with chat models, but we’ll show a few highlights below.</p>
<p>There are <a href="https://docs.langchain.com/oss/python/langchain/models#parameters">a few standard parameters</a> that we can set with chat models. Two of the most common are:</p>
<ul>
<li><code>model</code>: the name of the model</li>
<li><code>max_tokens</code>: limits the total number of tokens in the response, effectively controlling how long the output can be.</li>
<li><code>temperature</code>: the sampling temperature
<ul>
<li><strong>Low temperature</strong> (close to 0) is more deterministic and focused outputs. This is good for tasks requiring accuracy or factual responses.</li>
<li><strong>High temperature</strong> (close to 1) is good for creative tasks or generating varied responses.</li>
</ul></li>
</ul>
<p>LangChain supports many models via <a href="https://docs.langchain.com/oss/python/integrations/chat">third-party integrations</a>. By default, the course will use <a href="https://docs.langchain.com/oss/python/integrations/chat/openai">ChatOpenAI</a> because it is both popular and performant.</p>
<div id="44cc8280" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain_openai <span class="im">import</span> ChatOpenAI</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># https://openrouter.ai/openai/gpt-5-nano</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>model_gpt5_nano <span class="op">=</span> ChatOpenAI(</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"openai/gpt-5-nano"</span>,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    base_url<span class="op">=</span><span class="st">"https://openrouter.ai/api/v1"</span>,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    api_key<span class="op">=</span>os.environ.get(<span class="st">"OPENROUTER_API_KEY"</span>),</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># https://openrouter.ai/nvidia/nemotron-3-nano-30b-a3b:free</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>model_nemotron3_nano <span class="op">=</span> ChatOpenAI(</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"nvidia/nemotron-3-nano-30b-a3b:free"</span>,</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    base_url<span class="op">=</span><span class="st">"https://openrouter.ai/api/v1"</span>,</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    api_key<span class="op">=</span>os.environ.get(<span class="st">"OPENROUTER_API_KEY"</span>),</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm</code></pre>
</div>
</div>
<section id="running-a-model-locally" class="level3">
<h3 class="anchored" data-anchor-id="running-a-model-locally">Running a model locally</h3>
<p>LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.</p>
<p><a href="https://docs.langchain.com/oss/python/integrations/chat/ollama">Ollama</a> is one of the easiest ways to run chat and embedding models locally.</p>
</section>
</section>
<section id="key-methods" class="level2">
<h2 class="anchored" data-anchor-id="key-methods">Key Methods</h2>
<ol type="1">
<li><code>.invoke()</code>: results are only sent from the server when generation stops</li>
<li><code>.stream()</code>: results are sent from the server as they are being generated</li>
<li><code>.batch()</code>: send multiple inputs at once</li>
</ol>
<section id="invoke" class="level3">
<h3 class="anchored" data-anchor-id="invoke">1. Invoke</h3>
<p>The most straightforward way to call a model is to use <code>invoke()</code> with a single message or a list of messages:</p>
<div id="3b52c4ad" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>message <span class="op">=</span> model_nemotron3_nano.invoke(<span class="st">"what is Ramadan?"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>.. this returns an <code>AIMessage</code> object:</p>
<div id="b3b68f58" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>message</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>AIMessage(content='**Ramadan** is the ninth month of the Islamic lunar calendar and is observed by Muslims worldwide as a month of fasting, prayer, reflection, and community.  \n\n### Key Features\n\n| Aspect | Description |\n|--------|-------------|\n| **Fasting (Sawm)** | From dawn (Fajr) to sunset (Maghrib) Muslims abstain from food, drink, smoking, and marital relations. The fast is intended to cultivate self‑discipline, empathy for the less fortunate, and spiritual growth. |\n| **Prayer** | Additional nightly prayers called **Tarawih** are performed at the mosque, often completing a recitation of the entire Qur’an over the course of the month. |\n| **Qur’an Recitation** | Many Muslims aim to read or listen to the entire Qur’an during Ramadan, taking advantage of the increased spiritual focus. |\n| **Charity (Zakat &amp; Sadaqah)** | The month emphasizes giving to those in need; many people increase charitable donations and volunteer work. |\n| **Community &amp; Iftar** | The evening meal that breaks the fast, **iftar**, is often shared with family, friends, and sometimes the wider community. It typically starts with dates and water, following the example of the Prophet Muhammad. |\n| **Spiritual Reflection** | Fasting is seen as a way to purify the heart, increase gratitude, and strengthen one’s relationship with God (Allah). |\n\n### Timing\n\n- **Lunar Calendar:** Ramadan follows the Islamic lunar calendar, so it moves about 10–12 days earlier each Gregorian year.  \n- **Duration:** It lasts 29 or 30 days, depending on the sighting of the new moon.  \n\n### End of Ramadan\n\n- The month concludes with **Eid al‑Fitr**, a festive celebration marking the first day of Shawwal (the next lunar month). Eid al‑Fitr includes communal prayers, feasting, giving of gifts, and charitable donations (often called **Zakat al‑Fitr**).  \n\n### Who Observes It?\n\n- **Obligatory** for all adult Muslims who are physically able, though exemptions exist for those who are ill, traveling, pregnant, nursing, or otherwise unable to fast.  \n- Many children begin practicing partial fasts to prepare for adulthood.  \n\n### Cultural Variations\n\n- While the core practices are universal, local customs enrich the observance: special foods for suhoor (pre‑dawn meal), communal iftars, charitable projects, and unique cultural performances.  \n\nIn summary, Ramadan is a deeply spiritual period that combines physical discipline through fasting with heightened devotion, community bonding, and charitable giving, culminating in the joyous celebration of Eid al‑Fitr.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 646, 'prompt_tokens': 21, 'total_tokens': 667, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 67, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771843067-CE39pAmJGHoeBrlPaCGT', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c8a13-9ed6-7273-9973-db9b1588cee6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 21, 'output_tokens': 646, 'total_tokens': 667, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 67}})</code></pre>
</div>
</div>
<p>.. which has a <code>content</code> property, which includes the generated response text:</p>
<div id="4efc494d" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(message.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>**Ramadan** is the ninth month of the Islamic lunar calendar and is observed by Muslims worldwide as a month of fasting, prayer, reflection, and community.  

### Key Features

| Aspect | Description |
|--------|-------------|
| **Fasting (Sawm)** | From dawn (Fajr) to sunset (Maghrib) Muslims abstain from food, drink, smoking, and marital relations. The fast is intended to cultivate self‑discipline, empathy for the less fortunate, and spiritual growth. |
| **Prayer** | Additional nightly prayers called **Tarawih** are performed at the mosque, often completing a recitation of the entire Qur’an over the course of the month. |
| **Qur’an Recitation** | Many Muslims aim to read or listen to the entire Qur’an during Ramadan, taking advantage of the increased spiritual focus. |
| **Charity (Zakat &amp; Sadaqah)** | The month emphasizes giving to those in need; many people increase charitable donations and volunteer work. |
| **Community &amp; Iftar** | The evening meal that breaks the fast, **iftar**, is often shared with family, friends, and sometimes the wider community. It typically starts with dates and water, following the example of the Prophet Muhammad. |
| **Spiritual Reflection** | Fasting is seen as a way to purify the heart, increase gratitude, and strengthen one’s relationship with God (Allah). |

### Timing

- **Lunar Calendar:** Ramadan follows the Islamic lunar calendar, so it moves about 10–12 days earlier each Gregorian year.  
- **Duration:** It lasts 29 or 30 days, depending on the sighting of the new moon.  

### End of Ramadan

- The month concludes with **Eid al‑Fitr**, a festive celebration marking the first day of Shawwal (the next lunar month). Eid al‑Fitr includes communal prayers, feasting, giving of gifts, and charitable donations (often called **Zakat al‑Fitr**).  

### Who Observes It?

- **Obligatory** for all adult Muslims who are physically able, though exemptions exist for those who are ill, traveling, pregnant, nursing, or otherwise unable to fast.  
- Many children begin practicing partial fasts to prepare for adulthood.  

### Cultural Variations

- While the core practices are universal, local customs enrich the observance: special foods for suhoor (pre‑dawn meal), communal iftars, charitable projects, and unique cultural performances.  

In summary, Ramadan is a deeply spiritual period that combines physical discipline through fasting with heightened devotion, community bonding, and charitable giving, culminating in the joyous celebration of Eid al‑Fitr.</code></pre>
</div>
</div>
</section>
</section>
<section id="messages" class="level2">
<h2 class="anchored" data-anchor-id="messages">Messages</h2>
<p><strong>Messages</strong> are the fundamental unit of context for models in LangChain. They represent <strong>the input and output of models</strong>, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.</p>
<p><em>Messages</em> are objects that contain three things:</p>
<ul>
<li><strong>Content</strong>: Actual model response: text, images, audio, documents, etc.</li>
<li><strong>Metadata</strong>: Optional fields such as response information, message IDs, and token usage</li>
<li><strong>Role</strong>: Identifies the message type. One of:
<ol type="1">
<li><a href="https://docs.langchain.com/oss/python/langchain/messages#system-message"><code>SystemMessage</code></a>: Tells the model how to behave and provide context for interactions</li>
<li><a href="https://docs.langchain.com/oss/python/langchain/messages#human-message"><code>HumanMessage</code></a>: Represents user input and interactions with the model</li>
<li><a href="https://docs.langchain.com/oss/python/langchain/messages#ai-message"><code>AIMessage</code></a>: Responses generated by the model, including text content, tool calls, and metadata</li>
<li><a href="https://docs.langchain.com/oss/python/langchain/messages#tool-message"><code>ToolMessage</code></a>: Represents the outputs of <a href="https://docs.langchain.com/oss/python/langchain/models#tool-calling">tool calls</a></li>
</ol></li>
</ul>
<div id="04d81826" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.messages <span class="im">import</span> (</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    SystemMessage,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    HumanMessage,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    AIMessage</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="af799dea" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>system_prompt <span class="op">=</span> SystemMessage(<span class="st">"You always answer with 10 concise words, no more."</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>user_prompt <span class="op">=</span> HumanMessage(<span class="st">"How do I make an Agent in Python?"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>messages <span class="op">=</span> [</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    system_prompt,</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    user_prompt,</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> model_nemotron3_nano.invoke(messages)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="90059d86" class="cell" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(message)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>langchain_core.messages.ai.AIMessage</code></pre>
</div>
</div>
<div id="a98b793f" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Define class inherit from base implement act sense communicate loop</code></pre>
</div>
</div>
<section id="metadata" class="level3">
<h3 class="anchored" data-anchor-id="metadata">Metadata</h3>
<p>An <code>AIMessage</code> can hold token counts and other usage metadata in its <a href="https://reference.langchain.com/python/langchain-core/messages/ai/UsageMetadata"><code>usage_metadata</code></a> field:</p>
<div id="4e5fa81f" class="cell" data-execution_count="28">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>response.usage_metadata</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>{'input_tokens': 38,
 'output_tokens': 299,
 'total_tokens': 337,
 'input_token_details': {'audio': 0, 'cache_read': 0},
 'output_token_details': {'audio': 0, 'reasoning': 276}}</code></pre>
</div>
</div>
</section>
<section id="conversation" class="level3">
<h3 class="anchored" data-anchor-id="conversation">Conversation</h3>
<p>A list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.</p>
<div id="a857bec4" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>conversation <span class="op">=</span> [</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    SystemMessage(<span class="st">"You are a helpful assistant that translates English to Arabic."</span>),</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    HumanMessage(<span class="st">"Translate: I love programming."</span>),</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    AIMessage(<span class="st">"أحب البرمجة."</span>),</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    HumanMessage(<span class="st">"I love building applications."</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>message <span class="op">=</span> model_nemotron3_nano.invoke(conversation)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="102a9304" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(message.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>أحب بناء التطبيقات.</code></pre>
</div>
</div>
</section>
<section id="streaming-and-chunks" class="level3">
<h3 class="anchored" data-anchor-id="streaming-and-chunks">2. Streaming and chunks</h3>
<p>Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.</p>
<p>Calling <code>stream()</code> returns an iterator that yields output <a href="https://reference.langchain.com/python/langchain-core/messages/ai/AIMessageChunk"><code>AIMessageChunk</code></a> objects as they are produced. You can use a loop to process each chunk in real-time.</p>
<div id="3c795917" class="cell" data-execution_count="31">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>chunks <span class="op">=</span> []</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> chunk <span class="kw">in</span> model_nemotron3_nano.stream(<span class="st">"what is Ramadan? keep it short, keep it simple."</span>):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    chunks.append(chunk)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(chunk.text, end<span class="op">=</span><span class="st">""</span>, flush<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ramadan is the ninth month of the Islamic lunar calendar. During it, Muslims fast from sunrise to sunset—no food, drink, or other physical needs—while also increasing prayer, charity, and reflection. The month ends with the celebration of Eid al‑Fitr.content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019c8ac4-24a9-7243-b435-06c1fabbe92f' tool_calls=[] invalid_tool_calls=[] tool_call_chunks=[]</code></pre>
</div>
</div>
<div id="7487c446" class="cell" data-execution_count="38">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ch <span class="kw">in</span> chunks[<span class="dv">50</span>:<span class="dv">60</span>]:</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(ch.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> of
 the
 Islamic
 lunar
 calendar
.
 During
 it
,
 Muslims</code></pre>
</div>
</div>
</section>
<section id="batch" class="level3">
<h3 class="anchored" data-anchor-id="batch">3. Batch</h3>
<p>Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:</p>
<div id="b1cd4f85" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>responses <span class="op">=</span> model_nemotron3_nano.batch([</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is the capital of Saudi Arabia?"</span>,</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is 2 + 8"</span>,</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Is the sky blue or is it our perception? give a short and concise answer"</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> response <span class="kw">in</span> responses:</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(response)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>content='The capital of Saudi Arabia is **Riyadh**.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 24, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 26, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587230-rws1sKd4yd2Ir45jBE92', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-dab8-7670-aebf-be1a19d73902-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 24, 'output_tokens': 38, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 26}}
content='2\u202f+\u202f8\u202f=\u202f10.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 23, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 22, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587230-FEcwZ8VMp5p1Qgvgnrqr', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-daba-76d3-856b-d4204b17db9a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 23, 'output_tokens': 40, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 22}}
content='The sky appears blue because molecules in the atmosphere scatter short‑wavelength (blue) sunlight—an objective physical effect that our visual system interprets as the color blue.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 32, 'total_tokens': 206, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 159, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587231-VBn1J8V9CcCv10J2gqBT', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-dabc-7871-bdd8-f89b2bed8661-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 32, 'output_tokens': 174, 'total_tokens': 206, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 159}}</code></pre>
</div>
</div>
<div id="2264be1a" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, response <span class="kw">in</span> <span class="bu">enumerate</span>(responses):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(response.content)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">100</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>The capital of Saudi Arabia is **Riyadh**.
====================================================================================================
2 + 8 = 10.
====================================================================================================
The sky appears blue because molecules in the atmosphere scatter short‑wavelength (blue) sunlight—an objective physical effect that our visual system interprets as the color blue.
====================================================================================================</code></pre>
</div>
</div>
</section>
</section>
<section id="structured-output" class="level2">
<h2 class="anchored" data-anchor-id="structured-output">Structured output</h2>
<p>Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.</p>
<p><a href="https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage">Pydantic models</a> provide the richest feature set with field validation, descriptions, and nested structures.</p>
<div id="4765e1b9" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel, Field</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Movie(BaseModel):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A movie with details."""</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    title: <span class="bu">str</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"The title of the movie"</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    year: <span class="bu">int</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"The year the movie was released"</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    director: <span class="bu">str</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"The director of the movie"</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    rating: <span class="bu">float</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"The movie's rating out of 10"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="49f79423" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>model_with_structure <span class="op">=</span> model_nemotron3_nano.with_structured_output(Movie)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> model_with_structure.invoke(</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Provide details about the movie Inception"</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="634b07db" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Title:"</span>, response.title)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Year:"</span>, response.year)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Director:"</span>, response.director)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Rating:"</span>, response.rating)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Title: Inception
Year: 2010
Director: Christopher Nolan
Rating: 8.8</code></pre>
</div>
</div>
<p>Another example: <strong>Refining Search Query</strong></p>
<div id="a1a53f93" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Schema for structured output</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel, Field</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SearchQuery(BaseModel):</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    search_query: <span class="bu">str</span> <span class="op">=</span> Field(<span class="va">None</span>, description<span class="op">=</span><span class="st">"Query that is optimized web search."</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    justification: <span class="bu">str</span> <span class="op">=</span> Field(</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>, description<span class="op">=</span><span class="st">"Why this query is relevant to the user's request."</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Augment the LLM with schema for structured output</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>structured_llm <span class="op">=</span> model_nemotron3_nano.with_structured_output(SearchQuery)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="b177435e" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Invoke the augmented LLM</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> structured_llm.invoke(</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How does Calcium CT score relate to high cholesterol?"</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="f23f226f" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"search_query:"</span>, output.search_query)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"justification:"</span>, output.justification)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>search_query: Calcium CT score relationship with high cholesterol atherosclerosis risk
justification: The user wants to understand how a coronary artery calcium (CAC) score derived from CT imaging connects to elevated blood cholesterol levels. This requires information on the pathophysiology of atherosclerotic plaque formation, the role of lipid disorders, and how CAC scoring is used clinically to assess cardiovascular risk in the context of cholesterol.</code></pre>
</div>
</div>
<section id="benefits-of-structured-output" class="level3">
<h3 class="anchored" data-anchor-id="benefits-of-structured-output">Benefits of Structured Output</h3>
<p>Models are trained specifically to structure their outputs, because benefits where paramount:</p>
<ul>
<li><strong>Cost</strong>: the model doesn’t generate extra text.</li>
<li><strong>Accuracy</strong>: you get only what you want; no more, no less.</li>
<li><strong>Programming</strong>: structre can be parsed and used in programs. Example: tool calling.</li>
</ul>
</section>
</section>
<section id="llms-and-augmentations" class="level2">
<h2 class="anchored" data-anchor-id="llms-and-augmentations">LLMs and augmentations</h2>
<p>Workflows and agentic systems are based on LLMs and the various augmentations you add to them. <a href="https://docs.langchain.com/oss/python/langchain/tools">Tool calling</a>, <a href="https://docs.langchain.com/oss/python/langchain/structured-output">structured outputs</a>, and <a href="https://docs.langchain.com/oss/python/langchain/short-term-memory">short term memory</a> are a few options for tailoring LLMs to your needs.</p>
<p><img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?fit=max&amp;auto=format&amp;n=-_xGPoyjhyiDWTPJ&amp;q=85&amp;s=7ea9656f46649b3ebac19e8309ae9006" alt="LLM augmentations" data-path="oss/images/augmented_llm.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=280&amp;fit=max&amp;auto=format&amp;n=-_xGPoyjhyiDWTPJ&amp;q=85&amp;s=53613048c1b8bd3241bd27900a872ead 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=560&amp;fit=max&amp;auto=format&amp;n=-_xGPoyjhyiDWTPJ&amp;q=85&amp;s=7ba1f4427fd847bd410541ae38d66d40 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=840&amp;fit=max&amp;auto=format&amp;n=-_xGPoyjhyiDWTPJ&amp;q=85&amp;s=503822cf29a28500deb56f463b4244e4 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=1100&amp;fit=max&amp;auto=format&amp;n=-_xGPoyjhyiDWTPJ&amp;q=85&amp;s=279e0440278d3a26b73c72695636272e 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=1650&amp;fit=max&amp;auto=format&amp;n=-_xGPoyjhyiDWTPJ&amp;q=85&amp;s=d936838b98bc9dce25168e2b2cfd23d0 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=2500&amp;fit=max&amp;auto=format&amp;n=-_xGPoyjhyiDWTPJ&amp;q=85&amp;s=fa2115f972bc1152b5e03ae590600fa3 2500w"></p>
</section>
<section id="tool-calling" class="level2">
<h2 class="anchored" data-anchor-id="tool-calling">Tool calling</h2>
<p>Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:</p>
<ol type="1">
<li>A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)</li>
<li>A function or coroutine to execute.</li>
</ol>
<p>Note: A <em>coroutine</em> is a method that can suspend execution and resume at a later time</p>
<div id="fc180bd3" class="cell" data-execution_count="79">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.tools <span class="im">import</span> tool</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a tool</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="at">@tool</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multiply(a: <span class="bu">int</span>, b: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Multiplies two numbers."""</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">*</span> b</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="7451c7e0" class="cell" data-execution_count="80">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bind tools to the model</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>llm_with_tools <span class="op">=</span> model_nemotron3_nano.bind_tools([multiply])</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>system_prompt <span class="op">=</span> <span class="st">"You are a helpful assistant that can use tools to perform calculations."</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Step 1: Model generates tool calls</p>
<div id="4fe926d6" class="cell" data-execution_count="81">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What is 2 times 3?"</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>messages <span class="op">=</span> [</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    SystemMessage(system_prompt),</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    HumanMessage(question),</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="f64fa1fc" class="cell" data-execution_count="82">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>ai_msg <span class="op">=</span> llm_with_tools.invoke(messages)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="b08fe7bc" class="cell" data-execution_count="83">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(ai_msg)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>langchain_core.messages.ai.AIMessage</code></pre>
</div>
</div>
<div id="bc86ae50" class="cell" data-execution_count="84">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the tool call</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>ai_msg.tool_calls</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="84">
<pre><code>[{'name': 'multiply',
  'args': {'a': 2, 'b': 3},
  'id': 'call_f60c3d347ce746c29de453f5',
  'type': 'tool_call'}]</code></pre>
</div>
</div>
<p>Step 2: Execute tools and collect results</p>
<div id="dc6a7181" class="cell" data-execution_count="85">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tool_call <span class="kw">in</span> ai_msg.tool_calls:</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Execute the tool with the generated arguments</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    tool_msg <span class="op">=</span> multiply.invoke(tool_call)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    messages.append(tool_msg)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="7da38312" class="cell" data-execution_count="86">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(tool_msg)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="86">
<pre><code>langchain_core.messages.tool.ToolMessage</code></pre>
</div>
</div>
<div id="e7a95817" class="cell" data-execution_count="87">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>messages</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>[SystemMessage(content='You are a helpful assistant that can use tools to perform calculations.', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='What is 2 times 3?', additional_kwargs={}, response_metadata={}),
 ToolMessage(content='6', name='multiply', tool_call_id='call_f60c3d347ce746c29de453f5')]</code></pre>
</div>
</div>
<p>Step 3: Pass results back to model for final response</p>
<div id="c66ffd91" class="cell" data-execution_count="90">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>final_response <span class="op">=</span> model_nemotron3_nano.invoke(messages)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="e4a4a1c0" class="cell" data-execution_count="92">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(final_response.text)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>The result of 2 times 3 is **6**. 

This is a basic multiplication fact:  
$ 2 \times 3 = 6 $.  

Let me know if you'd like further clarification! 😊</code></pre>
</div>
</div>
<p>We’ll say how this multi-step code instructcion is wrapped inside a <code>create_agent</code> function in langchain.</p>
<section id="internet-search-tool" class="level3">
<h3 class="anchored" data-anchor-id="internet-search-tool">Internet Search Tool</h3>
<p><a href="https://www.tavily.com/"><strong>Tavily</strong></a> is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it’s easy to sign up and offers a generous free tier.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb59"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> add tavily-python</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<section id="set-up-tavily-api-for-web-search-free" class="level4">
<h4 class="anchored" data-anchor-id="set-up-tavily-api-for-web-search-free">Set up Tavily API for web search (Free)</h4>
<ul>
<li><p>Tavily Search API is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results.</p></li>
<li><p>You can sign up for an API key <a href="https://tavily.com/">here</a>. It’s easy to sign up and offers a very generous free tier. Some lessons (in Module 4) will use Tavily.</p></li>
<li><p>Set <code>TAVILY_API_KEY</code> in your environment.</p></li>
</ul>
<div id="bc32a40c" class="cell" data-execution_count="97">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tavily <span class="im">import</span> TavilyClient</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>tavily_client <span class="op">=</span> TavilyClient(api_key<span class="op">=</span>os.environ[<span class="st">"TAVILY_API_KEY"</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="37d3979c" class="cell" data-execution_count="99">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Literal</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> internet_search(</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    query: <span class="bu">str</span>,</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    max_results: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>    topic: Literal[<span class="st">"general"</span>, <span class="st">"news"</span>, <span class="st">"finance"</span>] <span class="op">=</span> <span class="st">"general"</span>,</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    include_raw_content: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Run a web search"""</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tavily_client.search(</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>        query,</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>        max_results<span class="op">=</span>max_results,</span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>        include_raw_content<span class="op">=</span>include_raw_content,</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>        topic<span class="op">=</span>topic,</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="3c1d5380" class="cell" data-execution_count="100">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> internet_search(<span class="st">"What is LangGraph?"</span>, max_results<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>result</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="100">
<pre><code>{'query': 'What is LangGraph?',
 'follow_up_questions': None,
 'answer': None,
 'images': [],
 'results': [{'url': 'https://www.ibm.com/think/topics/langgraph',
   'title': 'What is LangGraph? - IBM',
   'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent’s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.',
   'score': 0.95539,
   'raw_content': None},
  {'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',
   'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',
   'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions. LangGraph Studio is a visual development environment for LangChain’s LangGraph framework, simplifying the development of complex agentic applications built with LangChain components.',
   'score': 0.942385,
   'raw_content': None},
  {'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/',
   'title': 'What is LangGraph? - GeeksforGeeks',
   'content': 'LangGraph is an open-source framework built by LangChain that streamlines the creation and management of AI agent workflows. At its core, LangGraph combines large language models (LLMs) with graph-based architectures allowing developers to map, organize and optimize how AI agents interact and make decisions. By treating workflows as interconnected nodes and edges, LangGraph offers a scalable, transparent and developer-friendly way to design advanced AI systems ranging from simple chatbots to multi-agent system. The diagram below shows how LangGraph structures its agent-based workflow using distinct tools and stages. By designing workflows, users combine multiple nodes into powerful, dynamic AI processes. * ****langgraph:**** Framework for building graph-based AI workflows. ### Step 6: Build LangGraph Workflow. * Build the workflow graph using LangGraph, adding nodes for classification and response, connecting them with edges and compiling the app. * Send each input through the workflow graph and returns the bot’s response, either a greeting or an AI-powered answer. + Machine Learning Interview Questions and Answers15+ min read.',
   'score': 0.9393874,
   'raw_content': None}],
 'response_time': 0.93,
 'request_id': '42d8adb7-c9b7-49b5-afcb-fc4749da5892'}</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="create-an-agent" class="level2">
<h2 class="anchored" data-anchor-id="create-an-agent">Create an Agent</h2>
<p>Agents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.</p>
<p>An LLM Agent runs tools in a loop to achieve a goal. An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./assets/agent_loop.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Agent Loop"><img src="./assets/agent_loop.png" class="img-fluid figure-img" alt="Agent Loop"></a></p>
<figcaption>Agent Loop</figcaption>
</figure>
</div>
<p><code>create_agent</code> provides a production-ready agent implementation.</p>
<div id="065e6367" class="cell" data-execution_count="121">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.agents <span class="im">import</span> create_agent</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="co"># System prompt to steer the agent to be an expert researcher</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>AGENT_PROMPT <span class="op">=</span> <span class="st">"""You are an expert researcher. Your job is to conduct thorough research and then write a polished report.</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="st">You have access to an internet search tool as your primary means of gathering information.</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="st">Keep it short and concise.</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="st">## `internet_search`</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="st">Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> create_agent(</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model_nemotron3_nano,</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>    tools<span class="op">=</span>[internet_search],</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>    system_prompt<span class="op">=</span>AGENT_PROMPT</span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="invoke-the-agent" class="level3">
<h3 class="anchored" data-anchor-id="invoke-the-agent">Invoke the agent</h3>
<div id="e633e4c7" class="cell" data-execution_count="122">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> agent.invoke({</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"messages"</span>: [</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>        HumanMessage(<span class="st">"Explain agentic AI in a tweet"</span>)</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>})</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="be1de6bd" class="cell" data-execution_count="123">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>result</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="123">
<pre><code>{'messages': [HumanMessage(content='Explain agentic AI in a tweet', additional_kwargs={}, response_metadata={}, id='eb3b86d8-3111-4646-bd11-7f2b3d24e2cb'),
  AIMessage(content='Agentic AI refers to AI systems that can autonomously set goals, plan, and act to achieve them without constant human input—think of AI agents that decide, adapt, and execute tasks like a digital personal assistant on steroids.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 916, 'prompt_tokens': 456, 'total_tokens': 1372, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 651, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771856080-ALnud8RDRnzK2VkU71XS', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c8ada-2c1c-7260-b40d-69d3d37a726b-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 456, 'output_tokens': 916, 'total_tokens': 1372, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 651}})]}</code></pre>
</div>
</div>
<div id="eff95675" class="cell" data-execution_count="124">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the agent's response</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result[<span class="st">"messages"</span>][<span class="op">-</span><span class="dv">1</span>].content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Agentic AI refers to AI systems that can autonomously set goals, plan, and act to achieve them without constant human input—think of AI agents that decide, adapt, and execute tasks like a digital personal assistant on steroids.</code></pre>
</div>
</div>
<p>Let’s ask about something that needs search:</p>
<div id="07cdc1d2" class="cell" data-execution_count="125">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> agent.invoke({</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"messages"</span>: [</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>        HumanMessage(<span class="st">"What is the difference between LangChain, LangGraph and LangSmith?"</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>})</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="ed81815a" class="cell" data-execution_count="128">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>result</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="128">
<pre><code>{'messages': [HumanMessage(content='What is the difference between LangChain, LangGraph and LangSmith?', additional_kwargs={}, response_metadata={}, id='7f8c9005-eb8e-4072-86e0-64dd26bec6a6'),
  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 123, 'prompt_tokens': 462, 'total_tokens': 585, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 66, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771856086-8OPWhAITZng1xT0GmOTD', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c8ada-45c5-7e13-8c63-b2bdba91d977-0', tool_calls=[{'name': 'internet_search', 'args': {'max_results': 5, 'topic': 'general', 'query': 'LangChain LangGraph LangSmith differences', 'include_raw_content': True}, 'id': 'call_8a3e988aa40842cd95601cea', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 462, 'output_tokens': 123, 'total_tokens': 585, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 66}}),
  ToolMessage(content='{"query": "LangChain LangGraph LangSmith differences", "follow_up_questions": null, "answer": null, "images": [], "results": [{"url": "https://dev.to/rajkundalia/langchain-vs-langgraph-vs-langsmith-understanding-the-ecosystem-3m5o", "title": "LangChain vs LangGraph vs LangSmith - DEV Community", "content": "* **LangChain** provides the foundational building blocks for creating LLM applications through modular components and a unified interface for working with different AI providers. * **LangGraph** extends this foundation with **stateful, graph-based orchestration** for complex multi-agent workflows requiring loops, branching, and persistent state. * **LangSmith** completes the picture by offering **observability, tracing, and evaluation** tools for debugging and monitoring LLM applications in production. * **LangGraph** when you need sophisticated state management and agent coordination. What began as simple prompt–response interactions has grown into **multi-step workflows** involving retrieval systems, tool usage, autonomous agents, and long-running processes. LangChain is the **core framework** for building LLM-powered applications. If your application doesn’t need complex branching or shared long-lived state, **LangChain is the right tool**. ## LangGraph: Stateful Agent Orchestration. | LangChain | Composition | Linear workflows, RAG, simple agents |. | LangGraph | Orchestration | Branching, loops, shared state, multi-agent |. LangFlow provides a **visual, drag-and-drop** interface for building LangChain workflows.", "score": 0.918838, "raw_content": " [Skip to content](#main-content)\\n\\n[Log in](https://dev.to/enter?signup_subforem=1)    [Create account](https://dev.to/enter?signup_subforem=1&amp;state=new-user)\\n\\n## DEV Community\\n\\n[Raj Kundalia](/rajkundalia)\\n\\nPosted on\\n\\n# LangChain vs LangGraph vs LangSmith: Understanding the Ecosystem\\n\\n[#langchain](/t/langchain) [#langsmith](/t/langsmith) [#langgraph](/t/langgraph)\\n\\n&gt; Building LLM apps isn’t just about prompts anymore.  \\n&gt;  It’s about **composition**, **orchestration**, and **observability**.\\n\\n---\\n\\n## TL;DR\\n\\n* **LangChain** provides the foundational building blocks for creating LLM applications through modular components and a unified interface for working with different AI providers.\\n* **LangGraph** extends this foundation with **stateful, graph-based orchestration** for complex multi-agent workflows requiring loops, branching, and persistent state.\\n* **LangSmith** completes the picture by offering **observability, tracing, and evaluation** tools for debugging and monitoring LLM applications in production.\\n\\n**Use:**\\n\\n* **LangChain** for straightforward chains and RAG systems\\n* **LangGraph** when you need sophisticated state management and agent coordination\\n* **LangSmith** throughout development and production for visibility into behavior\\n\\n### Hands-on GitHub Repositories\\n\\n* **LangChain RAG Project** → &lt;https://github.com/rajkundalia/langchain-rag-project&gt;\\n* **LangGraph Analyzer** → &lt;https://github.com/rajkundalia/langgraph-analyzer&gt;\\n* **LangSmith Learning** → &lt;https://github.com/rajkundalia/langsmith-learning&gt;\\n\\n---\\n\\n## Introduction\\n\\nThe landscape of LLM application development has evolved rapidly since 2022.\\n\\nWhat began as simple prompt–response interactions has grown into **multi-step workflows** involving retrieval systems, tool usage, autonomous agents, and long-running processes. This evolution introduced **new problems at each stage** of the development lifecycle.\\n\\n* **The composition problem** → How do you connect prompts, models, tools, and data?\\n* **The orchestration problem** → How do you manage branching, retries, loops, and shared state?\\n* **The observability problem** → How do you debug, evaluate, and monitor these systems?\\n\\nThe LangChain ecosystem emerged to address each layer:\\n\\n| Problem | Tool | Year |\\n| --- | --- | --- |\\n| Composition | LangChain | 2022 |\\n| Orchestration | LangGraph | 2024 |\\n| Observability | LangSmith | 2023–2024 |\\n\\nEach tool targets a **specific layer** in the LLM application stack.\\n\\n---\\n\\n## LangChain: The Foundation\\n\\nLangChain is the **core framework** for building LLM-powered applications.\\n\\nIts primary goal is abstraction: different LLM providers expose different APIs, capabilities, and quirks. LangChain hides these differences behind a **unified interface**.\\n\\n### Core Building Blocks\\n\\nLangChain is composed of modular, swappable components:\\n\\n* **Prompts** – Templates and structured inputs for models\\n* **Models** – OpenAI, Anthropic, Google, or local LLMs\\n* **Memory** – Conversation history and contextual state\\n* **Tools** – Function calls to external systems\\n* **Retrievers** – Vector databases and RAG pipelines\\n\\n---\\n\\n### LCEL: LangChain Expression Language\\n\\nWhat ties everything together is **LCEL**.\\n\\nLCEL introduces a **declarative, pipe-based syntax** for composing chains:\\n\\n```\\nprompt | model | output_parser \\n```\\n\\nInstead of writing imperative glue code, you describe **data flow**.\\n\\n### Why LCEL Matters\\n\\nLCEL enables:\\n\\n* Automatic async, streaming, and batch execution\\n* Built-in LangSmith tracing\\n* Parallel execution of independent steps\\n* A unified `Runnable` interface (`invoke`, `batch`, `stream`)\\n\\nThis makes chains **faster**, **cleaner**, and easier to reason about.\\n\\n---\\n\\n### Multi-Provider Support\\n\\nLangChain supports dozens of LLM providers and integrations.\\n\\nYou can switch providers by changing **one line of configuration**, enabling:\\n\\n* Vendor independence\\n* A/B testing across models\\n* Cost and latency optimization\\n\\n---\\n\\n### When LangChain Is Enough\\n\\nUse LangChain when your workflow is primarily:\\n\\n```\\nInput → Process → Output \\n```\\n\\nTypical use cases include:\\n\\n* Chatbots with memory\\n* RAG-based Q&amp;A systems\\n* Natural language → SQL generation\\n* Linear tool pipelines\\n\\nIf your application doesn’t need complex branching or shared long-lived state, **LangChain is the right tool**.\\n\\n## \\n\\n## LangGraph: Stateful Agent Orchestration\\n\\nLangGraph solves the **orchestration problem**.\\n\\nAs soon as your application needs to:\\n\\n* make decisions,\\n* loop,\\n* retry,\\n* or coordinate multiple agents, linear chains start to break down.\\n\\n---\\n\\n### Graph-Based Architecture\\n\\nLangGraph models your application as a **directed graph**:\\n\\n* **Nodes** → processing steps or agents\\n* **Edges** → execution flow between nodes\\n\\nThis enables patterns that are hard or impossible with chains:\\n\\n* Loops and retries\\n* Conditional branching\\n* Parallel execution\\n* Shared, persistent state\\n\\n---\\n\\n### State as a First-Class Concept\\n\\nEvery LangGraph workflow operates on a **shared state object**.\\n\\n* Nodes receive the current state\\n* They compute updates\\n* Updates are merged back into state\\n\\nThis allows multiple agents to collaborate naturally.\\n\\n**Example:**\\n\\n* Research agent gathers sources\\n* Fact-checking agent validates claims\\n* Synthesis agent produces the final answer\\n\\nAll without complex message passing.\\n\\n---\\n\\n### Conditional Routing\\n\\nLangGraph supports **conditional edges**.\\n\\nA function decides which node runs next based on runtime state:\\n\\n* Route customer queries to specialist agents\\n* Loop back when required information is missing\\n* Retry until success conditions are met\\n\\n---\\n\\n### Persistence &amp; Checkpointing\\n\\nLangGraph includes built-in **checkpointing**:\\n\\n* Persist state across restarts\\n* Resume long-running workflows\\n* Support human-in-the-loop pauses\\n* Enable time-travel debugging\\n\\nThis is critical for production-grade agent systems.\\n\\n---\\n\\n### Visualization Support\\n\\nLangGraph workflows are inspectable and exportable:\\n\\n* Mermaid diagrams for documentation\\n* PNG images for presentations\\n* ASCII graphs for terminal debugging\\n\\nThis makes complex agent systems **understandable and communicable**.\\n\\n---\\n\\n### When You Need LangGraph\\n\\nChoose LangGraph when you need:\\n\\n* Explicit shared state\\n* Runtime decision-making\\n* Retry and failure recovery\\n* Multi-agent coordination\\n* Long-running workflows\\n\\nA classic example is an **autonomous research agent** that iteratively searches, reads, verifies, and synthesizes information.\\n\\n## \\n\\n## LangSmith: The Observability Layer\\n\\nLangSmith answers the question:\\n\\n&gt; “What is my LLM application actually doing?”\\n\\nIt doesn’t build workflows — it **illuminates them**.\\n\\n---\\n\\n### Tracing Everything\\n\\nLangSmith captures full execution traces:\\n\\n* Prompts and responses\\n* Token usage and latency\\n* Component call stacks\\n* Errors and retries\\n\\nYou can drill down from:\\n\\n* a full workflow run → to a single LLM call.\\n\\nThis makes debugging *dramatically* easier.\\n\\n---\\n\\n### Evaluation &amp; Regression Testing\\n\\nLangSmith allows you to:\\n\\n* Create evaluation datasets\\n* Run structured tests\\n* Track quality metrics\\n* Compare prompts and models\\n\\nThis enables **regression testing** for LLM apps — a must-have for production systems.\\n\\n---\\n\\n### Production Monitoring\\n\\nIn production, LangSmith tracks:\\n\\n* Response times\\n* Error rates\\n* Token and cost trends\\n* Usage by workflow or user\\n\\nAlerts help you catch issues early and optimize costs.\\n\\n---\\n\\n### Framework-Agnostic\\n\\nWhile LangSmith integrates seamlessly with LangChain and LangGraph, it’s **not limited to them**.\\n\\nYou can instrument *any* LLM application with LangSmith.\\n\\n## \\n\\n## Quick Comparison\\n\\n| Tool | Solves | Use When |\\n| --- | --- | --- |\\n| LangChain | Composition | Linear workflows, RAG, simple agents |\\n| LangGraph | Orchestration | Branching, loops, shared state, multi-agent |\\n| LangSmith | Observability | Debugging, evaluation, production monitoring |\\n\\n## \\n\\n## The Broader Ecosystem\\n\\n### LangFlow\\n\\nLangFlow provides a **visual, drag-and-drop** interface for building LangChain workflows.\\n\\n* Great for prototyping\\n* Helpful for non-technical collaboration\\n* Often exported to code for production\\n\\n---\\n\\n### Model Context Protocol (MCP)\\n\\nMCP (by Anthropic) standardizes **tool and resource access** for LLMs.\\n\\n* Works at the tool/retriever layer\\n* Complements LangChain and LangGraph\\n* Reduces custom integration effort\\n* Framework-agnostic\\n\\nMCP does **not** replace orchestration tools — it enhances connectivity.\\n\\n---\\n\\n## Conclusion\\n\\nThe LangChain ecosystem is **layered, not competitive**.\\n\\n* **LangChain** builds the core logic\\n* **LangGraph** manages complex workflows\\n* **LangSmith** makes everything observable\\n\\nMost serious LLM applications will use **more than one** of these tools.\\n\\nStart simple, add complexity only when needed, and **never ship without observability**.\\n\\n---\\n\\n## Further Reading &amp; Resources\\n\\n* &lt;https://www.datacamp.com/tutorial/langchain-vs-langgraph-vs-langsmith-vs-langflow&gt;\\n* &lt;https://www.datacamp.com/tutorial/langgraph-tutorial&gt;\\n* &lt;https://www.datacamp.com/tutorial/langgraph-agents&gt;\\n* &lt;https://www.techvoot.com/blog/langchain-vs-langgraph-vs-langflow-vs-langsmith-2025&gt;\\n\\n**Video**  \\n &lt;https://www.youtube.com/watch?v=vJOGC8QJZJQ&gt;\\n\\n**Academy Finxter Series (Excellent Deep Dive)**\\n\\n* &lt;https://academy.finxter.com/langchain-langsmith-and-langgraph/&gt;\\n* &lt;https://academy.finxter.com/langsmith-and-writing-tools/&gt;\\n* &lt;https://academy.finxter.com/langgraph/&gt;\\n* &lt;https://academy.finxter.com/multi-agent-teams-preparation/&gt;\\n* &lt;https://academy.finxter.com/setting-up-our-multi-agent-team/&gt;\\n* &lt;https://academy.finxter.com/web-research-and-asynchronous-tools/&gt;\\n\\n## Top comments (0)\\n\\nSubscribe\\n\\nFor further actions, you may consider blocking this person and/or [reporting abuse](/report-abuse)\\n\\nWe\'re a place where coders share, stay up-to-date and grow their careers.\\n\\n[Log in](https://dev.to/enter?signup_subforem=1)   [Create account](https://dev.to/enter?signup_subforem=1&amp;state=new-user)\\n\\n "}, {"url": "https://medium.com/@anshuman4luv/langchain-vs-langgraph-vs-langflow-vs-langsmith-a-detailed-comparison-74bc0d7ddaa9", "title": "LangChain vs LangGraph vs LangFlow vs LangSmith - Medium", "content": "# LangChain vs LangGraph vs LangFlow vs LangSmith: A Detailed Comparison. In the rapidly evolving world of AI, building applications powered by advanced language models like GPT-4 or Llama 3 has become more accessible, thanks to frameworks like **LangChain**, **LangGraph**, **LangFlow**, and **LangSmith**. LangChain is an open-source framework designed to streamline the development of applications leveraging language models. * **Chains**: Link multiple tasks such as API calls, LLM queries, and data processing. LangGraph builds on LangChain’s capabilities, focusing on orchestrating complex, multi-agent interactions. * **Cyclical Workflows**: Support iterative processes where tasks feed back into earlier stages. Imagine building a task management assistant agent. LangGraph represents this as a graph where each action (e.g., add task, complete task, summarize) is a node. LangGraph’s flexibility allows dynamic routing and revisiting nodes, making it ideal for stateful, interactive systems. LangFlow: Visual Design for LLM Applications. LangSmith is a monitoring and testing tool for LLM applications in production. 2. **Choose LangGraph** for applications involving multiple agents with interdependent tasks.", "score": 0.915091, "raw_content": "[Sitemap](/sitemap/sitemap.xml)\\n\\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source%3DmobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Sign in](/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40anshuman4luv%2Flangchain-vs-langgraph-vs-langflow-vs-langsmith-a-detailed-comparison-74bc0d7ddaa9&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Search](/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Sign in](/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40anshuman4luv%2Flangchain-vs-langgraph-vs-langflow-vs-langsmith-a-detailed-comparison-74bc0d7ddaa9&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n# LangChain vs LangGraph vs LangFlow vs LangSmith: A Detailed Comparison\\n\\n[Anshuman](/@anshuman4luv?source=post_page---byline--74bc0d7ddaa9---------------------------------------)\\n\\n3 min read\\n\\n·\\n\\nJan 17, 2025\\n\\n--\\n\\nIn the rapidly evolving world of AI, building applications powered by advanced language models like GPT-4 or Llama 3 has become more accessible, thanks to frameworks like **LangChain**, **LangGraph**, **LangFlow**, and **LangSmith**. While these tools share some similarities, they cater to different aspects of AI application development. This guide provides a clear comparison to help you choose the right tool for your project.\\n\\n## 1. LangChain: The Backbone of LLM Workflows\\n\\nLangChain is an open-source framework designed to streamline the development of applications leveraging language models. It offers modular components to connect various tasks, enabling a seamless development experience.\\n\\n### Key Features:\\n\\n* **LLM Integration**: Compatible with both open-source and closed-source models.\\n* **Prompt Management**: Supports dynamic prompt creation and templating.\\n* **Chains**: Link multiple tasks such as API calls, LLM queries, and data processing.\\n* **Memory**: Store contextual information for personalized, long-term interactions.\\n* **Agents**: Implement decision-making logic for dynamic task execution.\\n* **Data Integration**: Connect external data sources via document loaders and vector databases.\\n\\n### Example Use Case:\\n\\nA customer service chatbot that:\\n\\n1. Uses llm (large language model) to generate responses.\\n2. Dynamically retrieves product information from a database.\\n3. Stores user interactions for continuity in future conversations.\\n\\n## 2. LangGraph: Coordinating Multi-Agent Workflows\\n\\nLangGraph builds on LangChain’s capabilities, focusing on orchestrating complex, multi-agent interactions. It’s particularly useful for applications requiring multiple components to collaborate intelligently.\\n\\n### Key Concepts:\\n\\n* **Nodes and Edges**: Define tasks (nodes) and their relationships (edges) in a graph structure.\\n* **State Management**: Share and update a global state across agents.\\n* **Cyclical Workflows**: Support iterative processes where tasks feed back into earlier stages.\\n\\n### Detailed Example:\\n\\nImagine building a task management assistant agent. The workflow involves processing user inputs to:\\n\\n1. Add tasks.\\n2. Complete tasks.\\n3. Summarize tasks.\\n\\nLangGraph represents this as a graph where each action (e.g., add task, complete task, summarize) is a node. The user input is processed in a central “Process Input” node, which routes the request to the appropriate action node. A **state component** maintains the task list across interactions:\\n\\n* The “Add Task” node adds tasks to the state.\\n* The “Complete Task” node updates the state to mark tasks as done.\\n* The “Summarize” node generates a report using an LLM based on the current state.\\n\\nLangGraph’s flexibility allows dynamic routing and revisiting nodes, making it ideal for stateful, interactive systems.\\n\\n### Ideal Use Case:\\n\\nA research assistant system where:\\n\\n1. One agent retrieves data.\\n2. Another analyzes it.\\n3. A third summarizes the findings.\\n\\nLangGraph’s graph-based architecture ensures smooth coordination and data flow.\\n\\n## 3. LangFlow: Visual Design for LLM Applications\\n\\nLangFlow simplifies the prototyping process by providing a visual, drag-and-drop interface to design and test workflows. It’s ideal for non-developers or teams aiming to rapidly iterate on ideas.\\n\\n### Key Features:\\n\\n* Visual workflow builder.\\n* Pre-configured components for common tasks.\\n* Integration with LangChain’s core functionalities.\\n\\n### Ideal Use Case:\\n\\nA prototype for a document summarization tool where:\\n\\n1. Users upload a document.\\n2. An LLM extracts key points.\\n3. Results are displayed in a user-friendly format.\\n\\nLangFlow allows teams to test workflows without extensive coding.\\n\\n## 4. LangSmith: Ensuring Reliability in Production\\n\\nLangSmith is a monitoring and testing tool for LLM applications in production. It helps developers ensure their systems are efficient, reliable, and cost-effective.\\n\\n### Key Features:\\n\\n* **Performance Metrics**: Track token usage, API latency, and error rates.\\n* **Error Logging**: Identify and debug issues in real-time.\\n* **Cost Optimization**: Monitor resource usage to minimize expenses.\\n\\n### Ideal Use Case:\\n\\nA customer support system handling high volumes of queries. LangSmith provides insights into:\\n\\n1. Token consumption trends.\\n2. Latency spikes during peak hours.\\n3. Error patterns affecting user experience.\\n\\n## Comparison Table\\n\\n## How to Choose the Right Tool\\n\\n1. **Start with LangChain** if your focus is on creating a robust LLM-powered application.\\n2. **Choose LangGraph** for applications involving multiple agents with interdependent tasks.\\n3. **Use LangFlow** for quick prototyping or collaborating with non-technical stakeholders.\\n4. **Implement LangSmith** when scaling applications to production to ensure reliability and cost-efficiency.\\n\\nBy leveraging these tools effectively, you can streamline your AI development process and focus on delivering impactful solutions.\\n\\n[## Written by Anshuman](/@anshuman4luv?source=post_page---post_author_info--74bc0d7ddaa9---------------------------------------)\\n\\n[42 followers](/@anshuman4luv/followers?source=post_page---post_author_info--74bc0d7ddaa9---------------------------------------)\\n\\n·[1 following](/@anshuman4luv/following?source=post_page---post_author_info--74bc0d7ddaa9---------------------------------------)\\n\\n## Responses (2)\\n\\n[Text to speech](https://speechify.com/medium?source=post_page-----74bc0d7ddaa9---------------------------------------)"}, {"url": "https://aws.plainenglish.io/langchain-vs-langgraph-vs-langsmith-vs-langflow-understanding-through-a-realtime-project-2c3efd1606e7", "title": "LangChain vs LangGraph vs LangSmith vs LangFlow", "content": "How They Work Together in One Project · LangChain is the code foundation. · LangGraph is the workflow controller. · LangSmith is the debug and", "score": 0.90432173, "raw_content": null}, {"url": "https://codebasics.io/blog/what-are-the-differences-between-langchain-langgraph-and-langsmith", "title": "LangChain vs LangGraph vs LangSmith: Which One Should You Use?", "content": "6. When Should You Use LangChain, LangGraph, or LangSmith? Whether you\'re designing a chatbot, building a multi-step agentic workflow, or deploying a production-grade AI product—choosing the correct tooling is more important. In this blog post, we deep dive into the three powerful tools LangChain vs LangGraph vs LangSmith which are frequently used together but serve distinctly different purposes. While LangChain and LangGraph help you build apps, LangSmith helps you understand and improve them in production. When Should You Use LangChain, LangGraph, or LangSmith? Choosing between LangChain, LangGraph, and LangSmith depends on your project\'s workflow complexity, debugging needs, and production scale. Watch this YouTube video that clearly explains how LangChain, LangGraph, and LangSmith fit into modern LLM app development. Yes. LangSmith seamlessly integrates with LangChain and LangGraph, providing monitoring and observability for both simple and complex workflows. While LangChain and LangGraph help you build apps, LangSmith is the best tool for observability and prompt evaluation in production.", "score": 0.8958969, "raw_content": "Transform Your Career in 2026  12-Month Gen AI &amp; DS Bootcamp. Live Mentorship, Real Projects, Real Skills.  [Explore Program!](https://codebasics.io/genai-bootcamp-3.0)\\n\\n[Explore Program!](https://codebasics.io/genai-bootcamp-3.0)\\n\\n# What Are the Differences Between LangChain, LangGraph, and LangSmith?\\n\\nAI &amp; Data Science\\n\\nAug 04, 2025 | By Codebasics Team\\n\\n### Table of Contents\\n\\n1. Introduction\\n\\n2. What is LangChain\\n\\n2.1 Key Features of the LangChain Framework\\n\\n3. What is LangGraph\\n\\n3.1 When LangGraph Stands Out\\n\\n4. What is LangSmith\\n\\n4.1 Why LangSmith is Essential\\n\\n5. LangChain vs LangGraph vs LangSmith – A Detailed Comparison\\n\\n6. When Should You Use LangChain, LangGraph, or LangSmith?\\n\\n7. Final Thoughts\\n\\n8. FAQs\\n\\n## 1. Introduction\\n\\nAs the development of the [Large Language Model](https://en.wikipedia.org/wiki/Large_language_model) (LLM) matures, developers and product teams are facing more complex challenges in building robust, scalable AI-powered applications. Whether you\'re designing a chatbot, building a multi-step agentic workflow, or deploying a production-grade AI product—choosing the correct tooling is more important.\\n\\nIn this blog post, we deep dive into the three powerful tools LangChain vs LangGraph vs LangSmith which are frequently used together but serve distinctly different purposes. If you’re unsure or wondering how to select the right tool for your project or how they work together then this guide is for you.\\n\\n## 2. What is LangChain?\\n\\n[LangChain](../../resources/langchain-crash-course) is presented as a Python-based framework that simplifies the creation of LLM-powered applications, particularly for straightforward, linear workflows where the LLM performs predefined tasks—such as question-answering, summarization, or document retrieval.\\n\\n### 2.1 Key Features of the LangChain Framework:\\n\\n* **Modularity**: Chains, tools, prompts, and memory are modularized for flexibility.\\n* **Ease of Use**: Ideal for developers building LLM apps with minimal agentic complexity.\\n* **Tool Integration**: Easily connects with APIs, vector stores, databases, and more.\\n* **Best For**: Chatbots, document Q&amp;A, SQL query generation, RAG pipelines.\\n\\n## 3. What is LangGraph?\\n\\nLangGraph is introduced as a more advanced, stateful, graph-based framework built on top of LangChain. It is designed to orchestrate complex, multi-step, stateful agentic workflows that involve autonomous decision-making, retries, and iterative processes, often represented as a graph.\\n\\n### 3.1 When LangGraph Stands Out:\\n\\n* **State Machines**: Constructs workflows as DAGs (Directed Acyclic Graphs).\\n* **Asynchronous Agent Support**: Enables multiple agents to collaborate or loop.\\n* **Advanced Control Flow**: Perfect for situations where you need feedback loops, branching logic, and retries.\\n* **Best For**: AI agents, autonomous systems, iterative planning, complex tools orchestration.\\n\\n## 4. What is LangSmith?\\n\\nLangSmith is a monitoring and debugging platform purpose-built for LLM applications. While LangChain and LangGraph help you build apps, LangSmith helps you understand and improve them in production.\\n\\n### 4.1 Why LangSmith is Essential:\\n\\n* **Tracing and Debugging**: Visualize how prompts, models, and chains interact.\\n* **Evaluation**: Use human or automated grading to test model performance.\\n* **Prompt Experiments**: Run A/B tests to optimize system prompts.\\n* **Best For**: Teams deploying apps at scale who need transparency and version control.\\n\\n## 5. LangChain vs LangGraph vs LangSmith – A Detailed Comparison\\n\\nHere is the detailed comparison of LangChain, LangGraph, and LangSmith:\\n\\n| **Feature** | **LangChain** | **LangGraph** | **LangSmith** |\\n| --- | --- | --- | --- |\\n| **Purpose** | Build linear LLM applications | Handle complex, stateful workflows | Debug, monitor, and evaluate LLM apps |\\n| **Ideal For** | Simple chatbots, Q&amp;A tools | Autonomous agents, iterative workflows | Production-grade observability |\\n| **Control Flow** | Sequential | Graph-based, branching &amp; looping | Not applicable (used for monitoring) |\\n| **Workflow Complexity** | Basic | Advanced | Not Applicable |\\n| **Agent Support** | Basic agent capabilities | Full agent support with state mgmt | Full support for agent monitoring |\\n| **Integration** | Python, JS, APIs, vector DBs | Built on LangChain, supports same | LangChain &amp; LangGraph compatible |\\n| **Deployment Focus** | Prototypes, MVPs | Production agents | Production debugging &amp; evaluation |\\n| **Visualization** | No | Partial (via LangSmith) | Full tracing and logs |\\n| **Evaluation Tools** | None | None | Prompt tests, metrics, user grading |\\n\\n## 6. When Should You Use LangChain, LangGraph, or LangSmith?\\n\\nChoosing between LangChain, LangGraph, and LangSmith depends on your project\'s workflow complexity, debugging needs, and production scale.\\n\\n**Use LangChain if:**\\n\\n* You\'re creating a simple chatbot, summarizing a document, or RAG system.\\n* You prefer simplicity and fast prototyping.\\n* Your use case doesn’t require advanced memory, loops, or retries.\\n\\n**Use LangGraph if:**\\n\\n* You need to create multi-agent, multi-step, or self-correcting workflows.\\n* Your LLM app requires state management and branching logic.\\n* You’re building production-grade agents or autonomous systems.\\n\\n**Use LangSmith if:**\\n\\n* You\'re ready to move to production and need visibility into what your app is doing.\\n* You want to evaluate different prompts, models, or tools.\\n* You\'re managing LLM behavior across teams or projects.\\n\\n[Watch this YouTube video](https://www.youtube.com/watch?v=vJOGC8QJZJQ) that clearly explains how LangChain, LangGraph, and LangSmith fit into modern LLM app development.\\n\\n## 7. Final Thoughts\\n\\nLangChain, LangGraph, and LangSmith aren’t competitors—they\'re complementary. Think of them as a [full-stack toolkit](https://www.geeksforgeeks.org/blogs/full-stack-development-tools/) for modern LLM app development:\\n\\n* LangChain helps you build.\\n* LangGraph helps you orchestrate.\\n* LangSmith helps you optimize.\\n\\nUnderstanding where each tool fits allows you to architect better, debug smarter, and scale faster.\\n\\n*Note: The features and comparisons mentioned are based on the capabilities of LangChain, LangGraph, and LangSmith as of July 2025. These tools are evolving quickly, so we recommend checking their official documentation for the most up-to-date information.*\\n\\n## FAQs\\n\\n**1. Can you use LangSmith with LangChain?**  \\nYes. LangSmith seamlessly integrates with LangChain and LangGraph, providing monitoring and observability for both simple and complex workflows.\\n\\n**2. Is LangGraph better than LangChain?**  \\nNot necessarily. LangGraph is more powerful for advanced workflows, but LangChain is easier and faster for simple applications. Choose based on your use case.\\n\\n**3. Which is best for production debugging: LangSmith vs others?**  \\nLangSmith is purpose-built for debugging and monitoring. While LangChain and LangGraph help you build apps, LangSmith is the best tool for observability and prompt evaluation in production.\\n\\n**4. Can LangChain be used for fine-tuning?**  \\nNo, LangChain is not for fine-tuning models. It\'s designed to build LLM applications, but fine-tuning is done using frameworks like Hugging Face or OpenAI\'s API.\\n\\n**5. Should I learn LangGraph instead of LangChain?**  \\nLearn LangChain if you need simple, linear workflows like chatbots or Q&amp;A systems. Learn LangGraph if you need more complex workflows with multi-step logic, decision-making, or autonomous agents. Start with LangChain, and move to LangGraph if your projects become more complex.\\n\\n#### Share With Friends\\n\\n[8 Must-Have Skills to Get a Data Analyst Job in 2024](/blog/8-must-have-skills-to-get-a-data-analyst-job) [How to Learn SQL and Python for Data Engineering: A Complete Step-by-Step Guide](/blog/how-to-learn-sql-and-python-for-data-engineering-a-complete-step-by-step-guide)\\n\\nIn Demand\\n\\nUS$291\\n\\n[Gen AI &amp; Data Science Bootcamp 3.0: With Practical Job Placement Support &amp; Virtual Internship](https://codebasics.io/bootcamps/gen-ai-data-science-bootcamp-with-virtual-internship)  [Become a high-paying AI-enabled Data Scientist by learning the secrets of the industry taught by data scientist hiring managers with 8+ years of international experience in data industry.](https://codebasics.io/bootcamps/gen-ai-data-science-bootcamp-with-virtual-internship)\\n\\n#### Categories\\n\\n* [Data Analysis](https://codebasics.io/blogs/data-analysis)\\n* [Talent Management](https://codebasics.io/blogs/talent-management)\\n* [AI &amp; Data Science](https://codebasics.io/blogs/ai-data-science66cec6148d22d)\\n* [Data Science](https://codebasics.io/blogs/data-science)\\n* [Deep Learning](https://codebasics.io/blogs/deep-learning)\\n* [Artificial Intelligence](https://codebasics.io/blogs/artificial-intelligence66629f6dc7fb7)\\n* [Python](https://codebasics.io/blogs/python)\\n* [Data Engineering](https://codebasics.io/blogs/data-engineering692ec50b6daa5)\\n* [Machine Learning](https://codebasics.io/blogs/machine-learning669f54cfb1cad)\\n* [Programming](https://codebasics.io/blogs/programming)\\n\\n#### Related Blogs\\n\\nFeb 11, 2026\\n\\n[How Gen AI Will Revolutionize Data Science in 2026](/blog/how-gen-ai-will-revolutionize-data-science-in-2026)\\n\\nMay 27, 2025\\n\\n[What is Agentic AI and How Does it Work?](/blog/what-is-agentic-ai-and-how-does-it-work)\\n\\nMay 20, 2025\\n\\n[How to Start an AI Career in 2025 – Roles &amp; Skills Explained](/blog/how-to-start-an-ai-career-in-2025-roles-skills-explained)\\n\\nMay 08, 2025\\n\\n[Model Context Protocol Explained: Streamlining AI Integration and Development](/blog/model-context-protocol-explained-streamlining-ai-integration-and-development)\\n\\nApr 23, 2025\\n\\n[AI Agents: The Ultimate Productivity Tool Revolutionizing Automation in 2025](/blog/ai-agents-the-ultimate-productivity-tool-revolutionizing-automation-in-2025)\\n\\nLearning knows no limits. Here’s to your journey of seamless learning. Pick your preferred course from the list of paid &amp; free resources.\\n\\nQuick Links\\n\\n* [Courses](https://codebasics.io/courses)\\n* [Blogs](https://codebasics.io/blogs)\\n* [Data Challenges](/challenges/resume-project-challenge)\\n* [Hire Talent](https://codebasics.io/hiring-partners)\\n* [Contact Us](https://codebasics.io/contact-us)\\n\\nHelp &amp; Support\\n\\n* [Refund Policy](https://codebasics.io/refund-policy)\\n* [Terms &amp; Conditions](https://codebasics.io/terms-and-conditions)\\n* [Privacy Policy](https://codebasics.io/privacy-policy)\\n* [Certificate Verification](https://codebasics.io/certificate_validation)\\n* [Business Inquiries](/cdn-cgi/l/email-protection#37175542445e5952444477545853525556445e5444195e58)\\n\\nCourse Topics\\n\\n* [AI &amp; Data Science](https://codebasics.io/categories/data-science)\\n* [Exploratory Data Analysis (EDA)](https://codebasics.io/categories/exploratory-data-analysis-eda)\\n* [Career Advice](https://codebasics.io/categories/career-advice)\\n* [Conversations](https://codebasics.io/categories/conversations)\\n* [Data Analysis](https://codebasics.io/categories/data-analysis)\\n\\n* [Deep Learning](https://codebasics.io/categories/deep-learning)\\n* [Julia](https://codebasics.io/categories/julia)\\n* [Jupyter Notebook](https://codebasics.io/categories/jupyter-notebook)\\n* [Machine Learning](https://codebasics.io/categories/machine-learning)\\n* [Matplotlib](https://codebasics.io/categories/matplotlib)\\n\\n© 2026 [Codebasics](https://codebasics.io). Owned and operated by LearnerX EdTech Private Limited.   \\n Technology partner:  [AtliQ Technologies](https://www.atliq.com/)\\n\\n##### Login\\n\\n[Log in with Google](https://codebasics.io/google)\\n\\n[Log in with Linkedin](https://codebasics.io/linkedin)\\n\\nDon\'t have an account? [Register Now!](https://codebasics.io/register)\\n\\nBy signing up, you agree to  \\n our [Terms and Conditions](https://codebasics.io/terms-and-conditions) and [Privacy Policy](https://codebasics.io/privacy-policy).\\n\\n[Talk to us](https://forms.office.com/r/0d6zvJeYa1 \\"Talk to us\\")  [Chat with us](https://wa.me/918977530886?text=Hi%20Codebasics!\\n\\n \\"Chat with us\\")\\n\\n "}, {"url": "https://www.reddit.com/r/Rag/comments/1mxs81z/finally_figured_out_the_langchain_vs_langgraph_vs/", "title": "Finally figured out the LangChain vs LangGraph vs LangSmith ...", "content": "# Finally figured out the LangChain vs LangGraph vs LangSmith confusion - here\'s what I learned. After weeks of being confused about when to use LangChain, LangGraph, or LangSmith (and honestly making some poor choices), I decided to dive deep and create a breakdown. The TLDR: They\'re not competitors - they\'re actually designed to work together, but each serves a very specific purpose that most tutorials don\'t explain clearly. 🔗 Full breakdown: LangSmith vs LangChain vs LangGraph The REAL Difference for Developers. The game-changer for me was understanding that you can (and often should) use them together. LangChain for the basics, LangGraph for complex flows, LangSmith to see what\'s actually happening under the hood. Anyone else been through this confusion? What\'s your go-to setup for production LLM apps? Would love to hear how others are structuring their GenAI projects - especially if you\'ve found better alternatives or have war stories about debugging LLM applications 😅.", "score": 0.8923472, "raw_content": "           \\n\\n  \\n            \\n   \\n\\n[Go to Rag](/r/Rag/)   \\n\\n[r/Rag](/r/Rag/)   •\\n\\n# Finally figured out the LangChain vs LangGraph vs LangSmith confusion - here\'s what I learned\\n\\nAfter weeks of being confused about when to use LangChain, LangGraph, or LangSmith (and honestly making some poor choices), I decided to dive deep and create a breakdown.\\n\\nThe TLDR: They\'re not competitors - they\'re actually designed to work together, but each serves a very specific purpose that most tutorials don\'t explain clearly.\\n\\n🔗 Full breakdown: [LangSmith vs LangChain vs LangGraph The REAL Difference for Developers](https://www.youtube.com/watch?v=DGxf0X1GdtQ)\\n\\nThe game-changer for me was understanding that you can (and often should) use them together. LangChain for the basics, LangGraph for complex flows, LangSmith to see what\'s actually happening under the hood.\\n\\nAnyone else been through this confusion? What\'s your go-to setup for production LLM apps?\\n\\nWould love to hear how others are structuring their GenAI projects - especially if you\'ve found better alternatives or have war stories about debugging LLM applications 😅\\n\\n          \\n\\n[Reddit Rules](https://www.redditinc.com/policies/content-policy)    [Privacy Policy](https://www.reddit.com/policies/privacy-policy)    [User Agreement](https://www.redditinc.com/policies/user-agreement)    [Your Privacy Choices](https://support.reddithelp.com/hc/articles/43980704794004)    [Accessibility](https://support.reddithelp.com/hc/sections/38303584022676-Accessibility)   [Reddit, Inc. © 2026. All rights reserved.](https://redditinc.com)\\n\\n                                    "}], "response_time": 5.43, "request_id": "54b07b6b-5d65-42aa-83ab-1694988b125d"}', name='internet_search', id='0380801e-3706-41ee-adad-e4593fbb87a6', tool_call_id='call_8a3e988aa40842cd95601cea'),
  AIMessage(content='**LangChain\u202fvs\u202fLangGraph\u202fvs\u202fLangSmith – What They Do and When to Use Them**\n\n| Aspect | **LangChain** | **LangGraph** | **LangSmith** |\n|--------|---------------|---------------|---------------|\n| **Primary Goal** | Provide a **modular toolbox** for building LLM‑powered apps (prompts, models, memory, tools, RAG, etc.). | Add **stateful, graph‑based orchestration** for complex, multi‑step workflows that need branching, loops, retries, and shared state. | Offer **observability, tracing, evaluation, and monitoring** for any LLM application (especially those built with LangChain/LangGraph). |\n| **Core Concept** | **Chains** – linear pipelines of components (`prompt → model → parser`). | **Graphs** – nodes (steps/agents) and edges (control flow) that can loop, branch, and retain a mutable state. | **Traces** – full execution logs that let you drill from a workflow run down to a single LLM call, plus evaluation &amp; metrics. |\n| **Typical Use‑Cases** | • Simple chatbots, Q&amp;A, RAG pipelines&lt;br&gt;• Linear tool usage (e.g., fetch‑then‑summarize)&lt;br&gt;• Prototyping &amp; MVPs | • Multi‑agent systems&lt;br&gt;• Workflows with feedback loops, retries, or conditional routing&lt;br&gt;• Long‑running processes that need persistent state | • Debugging production pipelines&lt;br&gt;• A/B‑testing prompts/models&lt;br&gt;• Monitoring latency, cost, error rates&lt;br&gt;• Regression testing of LLM behavior |\n| **When to Choose It** | Your app fits a **straight‑through** flow and does not need complex state or branching. | You need **shared state, dynamic routing, or multi‑agent coordination** (e.g., research assistants, autonomous planners). | You are moving to **production** and need visibility, debugging, and evaluation capabilities. |\n| **Integration** | Works with any LLM provider via a unified interface. | Built on top of LangChain; can be used alongside it. | Agnostic – can instrument LangChain, LangGraph, or any custom LLM code. |\n| **Key Benefits** | • Rapid prototyping&lt;br&gt;• Vendor‑agnostic model switching&lt;br&gt;• Built‑in async/streaming via LCEL | • Explicit state management&lt;br&gt;• Conditional edges &amp; loops&lt;br&gt;• Persistence &amp; checkpointing for long‑running agents | • End‑to‑end tracing&lt;br&gt;• Prompt &amp; model evaluation tools&lt;br&gt;• Production‑grade monitoring &amp; alerting |\n\n### TL;DR Summary\n- **LangChain** = *building blocks* for basic LLM apps.  \n- **LangGraph** = *orchestrator* for sophisticated, stateful, multi‑agent workflows.  \n- **LangSmith** = *debugger &amp; observability layer* that makes those apps production‑ready.\n\nIn practice, a typical production stack uses **all three together**: build with LangChain, orchestrate complex flows with LangGraph, and monitor/evaluate everything with LangSmith.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 722, 'prompt_tokens': 9878, 'total_tokens': 10600, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 67, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771856094-Dwpy8GjlZFprVUGX6Yh0', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c8ada-64de-7c50-b22f-0d0678c3c586-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 9878, 'output_tokens': 722, 'total_tokens': 10600, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 67}})]}</code></pre>
</div>
</div>
<div id="0540d7d2" class="cell" data-execution_count="126">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(result[<span class="st">"messages"</span>][<span class="op">-</span><span class="dv">2</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="126">
<pre><code>langchain_core.messages.tool.ToolMessage</code></pre>
</div>
</div>
<div id="a7b2bb15" class="cell" data-execution_count="127">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the agent's response</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result[<span class="st">"messages"</span>][<span class="op">-</span><span class="dv">1</span>].content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>**LangChain vs LangGraph vs LangSmith – What They Do and When to Use Them**

| Aspect | **LangChain** | **LangGraph** | **LangSmith** |
|--------|---------------|---------------|---------------|
| **Primary Goal** | Provide a **modular toolbox** for building LLM‑powered apps (prompts, models, memory, tools, RAG, etc.). | Add **stateful, graph‑based orchestration** for complex, multi‑step workflows that need branching, loops, retries, and shared state. | Offer **observability, tracing, evaluation, and monitoring** for any LLM application (especially those built with LangChain/LangGraph). |
| **Core Concept** | **Chains** – linear pipelines of components (`prompt → model → parser`). | **Graphs** – nodes (steps/agents) and edges (control flow) that can loop, branch, and retain a mutable state. | **Traces** – full execution logs that let you drill from a workflow run down to a single LLM call, plus evaluation &amp; metrics. |
| **Typical Use‑Cases** | • Simple chatbots, Q&amp;A, RAG pipelines&lt;br&gt;• Linear tool usage (e.g., fetch‑then‑summarize)&lt;br&gt;• Prototyping &amp; MVPs | • Multi‑agent systems&lt;br&gt;• Workflows with feedback loops, retries, or conditional routing&lt;br&gt;• Long‑running processes that need persistent state | • Debugging production pipelines&lt;br&gt;• A/B‑testing prompts/models&lt;br&gt;• Monitoring latency, cost, error rates&lt;br&gt;• Regression testing of LLM behavior |
| **When to Choose It** | Your app fits a **straight‑through** flow and does not need complex state or branching. | You need **shared state, dynamic routing, or multi‑agent coordination** (e.g., research assistants, autonomous planners). | You are moving to **production** and need visibility, debugging, and evaluation capabilities. |
| **Integration** | Works with any LLM provider via a unified interface. | Built on top of LangChain; can be used alongside it. | Agnostic – can instrument LangChain, LangGraph, or any custom LLM code. |
| **Key Benefits** | • Rapid prototyping&lt;br&gt;• Vendor‑agnostic model switching&lt;br&gt;• Built‑in async/streaming via LCEL | • Explicit state management&lt;br&gt;• Conditional edges &amp; loops&lt;br&gt;• Persistence &amp; checkpointing for long‑running agents | • End‑to‑end tracing&lt;br&gt;• Prompt &amp; model evaluation tools&lt;br&gt;• Production‑grade monitoring &amp; alerting |

### TL;DR Summary
- **LangChain** = *building blocks* for basic LLM apps.  
- **LangGraph** = *orchestrator* for sophisticated, stateful, multi‑agent workflows.  
- **LangSmith** = *debugger &amp; observability layer* that makes those apps production‑ready.

In practice, a typical production stack uses **all three together**: build with LangChain, orchestrate complex flows with LangGraph, and monitor/evaluate everything with LangSmith.</code></pre>
</div>
</div>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ul>
<li>Three key methods for models: invoke, stream, and batch.</li>
<li>LLMs can be configured to responsd in a structured format</li>
<li>Agent = Model + Tools</li>
<li>Models (LLMs) are the brain-power of agents</li>
<li>Tools connect the LLM to the outside world.</li>
</ul>
</section>
<section id="activity" class="level2">
<h2 class="anchored" data-anchor-id="activity">Activity</h2>
<p><strong>Over to you</strong>: The previous model returned one search result. Can you make an Agent that uses the content from the top-3 ranking sites, to answer the question?</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>