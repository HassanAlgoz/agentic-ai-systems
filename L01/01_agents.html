<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Agents in LangChain ‚Äì Building Agentic AI Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7dc3907ddb6ec99bf1b87f80a83abe8b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Building Agentic AI Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#questions" id="toc-questions" class="nav-link active" data-scroll-target="#questions">Questions</a></li>
  <li><a href="#setup-virtual-environment" id="toc-setup-virtual-environment" class="nav-link" data-scroll-target="#setup-virtual-environment">Setup Virtual Environment</a>
  <ul class="collapse">
  <li><a href="#activate-the-virtual-environment" id="toc-activate-the-virtual-environment" class="nav-link" data-scroll-target="#activate-the-virtual-environment">Activate the virtual environment</a></li>
  </ul></li>
  <li><a href="#what-is-an-api-key" id="toc-what-is-an-api-key" class="nav-link" data-scroll-target="#what-is-an-api-key">What is an <strong>API Key</strong>?</a>
  <ul class="collapse">
  <li><a href="#what-why" id="toc-what-why" class="nav-link" data-scroll-target="#what-why">What &amp; Why</a></li>
  <li><a href="#security-risks" id="toc-security-risks" class="nav-link" data-scroll-target="#security-risks">Security Risks</a></li>
  <li><a href="#how-to-set-your-api-key" id="toc-how-to-set-your-api-key" class="nav-link" data-scroll-target="#how-to-set-your-api-key">How to Set Your API Key?</a></li>
  <li><a href="#sign-up-and-set-langsmith-api-free" id="toc-sign-up-and-set-langsmith-api-free" class="nav-link" data-scroll-target="#sign-up-and-set-langsmith-api-free">Sign up and Set LangSmith API (Free)</a></li>
  <li><a href="#set-up-tavily-api-for-web-search-free" id="toc-set-up-tavily-api-for-web-search-free" class="nav-link" data-scroll-target="#set-up-tavily-api-for-web-search-free">Set up Tavily API for web search (Free)</a></li>
  <li><a href="#install-dependencies" id="toc-install-dependencies" class="nav-link" data-scroll-target="#install-dependencies">Install dependencies</a></li>
  </ul></li>
  <li><a href="#models" id="toc-models" class="nav-link" data-scroll-target="#models">Models</a>
  <ul class="collapse">
  <li><a href="#choosing-between-models" id="toc-choosing-between-models" class="nav-link" data-scroll-target="#choosing-between-models">Choosing between models</a></li>
  <li><a href="#basic-usage" id="toc-basic-usage" class="nav-link" data-scroll-target="#basic-usage">Basic usage</a>
  <ul class="collapse">
  <li><a href="#running-a-model-locally" id="toc-running-a-model-locally" class="nav-link" data-scroll-target="#running-a-model-locally">Running a model locally</a></li>
  </ul></li>
  <li><a href="#key-methods" id="toc-key-methods" class="nav-link" data-scroll-target="#key-methods">Key Methods</a>
  <ul class="collapse">
  <li><a href="#invoke" id="toc-invoke" class="nav-link" data-scroll-target="#invoke">1. Invoke</a></li>
  <li><a href="#stream" id="toc-stream" class="nav-link" data-scroll-target="#stream">2. Stream</a></li>
  <li><a href="#batch" id="toc-batch" class="nav-link" data-scroll-target="#batch">3. Batch</a></li>
  </ul></li>
  <li><a href="#structured-output" id="toc-structured-output" class="nav-link" data-scroll-target="#structured-output">Structured output</a></li>
  <li><a href="#tool-calling" id="toc-tool-calling" class="nav-link" data-scroll-target="#tool-calling">Tool calling</a>
  <ul class="collapse">
  <li><a href="#tool-input-schemas" id="toc-tool-input-schemas" class="nav-link" data-scroll-target="#tool-input-schemas">Tool Input Schemas</a></li>
  </ul></li>
  <li><a href="#search-tools" id="toc-search-tools" class="nav-link" data-scroll-target="#search-tools">Search Tools</a></li>
  <li><a href="#create-an-agent" id="toc-create-an-agent" class="nav-link" data-scroll-target="#create-an-agent">Create an Agent</a>
  <ul class="collapse">
  <li><a href="#invoke-1" id="toc-invoke-1" class="nav-link" data-scroll-target="#invoke-1">Invoke</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#activity" id="toc-activity" class="nav-link" data-scroll-target="#activity">Activity</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="01_agents.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Agents in LangChain</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.</p>
<section id="questions" class="level2">
<h2 class="anchored" data-anchor-id="questions">Questions</h2>
<ul>
<li>What is an Agent in LangChain and how to make one?</li>
<li>What‚Äôs the relationship between an LLM and an Agent?</li>
<li>What can agents do?</li>
<li>Can I run an agent locally without a provider?</li>
</ul>
</section>
<section id="setup-virtual-environment" class="level2">
<h2 class="anchored" data-anchor-id="setup-virtual-environment">Setup Virtual Environment</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> init</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> venv <span class="at">-p</span> 3.12</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<section id="activate-the-virtual-environment" class="level3">
<h3 class="anchored" data-anchor-id="activate-the-virtual-environment">Activate the virtual environment</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="">Windows</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">MacOS / Linux</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">.venv\Scripts\activate.bat</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> .venv/bin/activate</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
</section>
</section>
<section id="what-is-an-api-key" class="level2">
<h2 class="anchored" data-anchor-id="what-is-an-api-key">What is an <strong>API Key</strong>?</h2>
<p>Think of an API Key as a <strong>hotel key card</strong>.</p>
<ul>
<li><strong>The Hotel (Server):</strong> Has resources (rooms) but keeps them locked.</li>
<li><strong>The Guest (Client):</strong> Wants access.</li>
<li><strong>The Key Card (API Key):</strong> Identifies you and proves you are allowed to enter specific rooms.</li>
</ul>
<hr>
<section id="what-why" class="level3">
<h3 class="anchored" data-anchor-id="what-why">What &amp; Why</h3>
<p>An API key is a unique string of characters used to identify the calling program.</p>
<ul>
<li><strong>Identification:</strong> Keys ‚Äúauthenticate the calling project,‚Äù allowing the server to recognize who is asking for data.</li>
<li><strong>Control:</strong> This lets the server track usage for billing and enforce limits (quotas) so one user doesn‚Äôt crash the system.</li>
</ul>
<hr>
</section>
<section id="security-risks" class="level3">
<h3 class="anchored" data-anchor-id="security-risks">Security Risks</h3>
<p>If you lose your key, it is like dropping your credit card.</p>
<ul>
<li><strong>Theft:</strong> Attackers can use your key to make requests on your behalf.</li>
<li><strong>Consequences:</strong> You suffer <strong>financial loss</strong> (paying for their usage) or <strong>service denial</strong> (they use up your available quota).</li>
</ul>
<blockquote class="blockquote">
<p><strong>Rule:</strong> Never post keys on public sites like GitHub.</p>
</blockquote>
</section>
<section id="how-to-set-your-api-key" class="level3">
<h3 class="anchored" data-anchor-id="how-to-set-your-api-key">How to Set Your API Key?</h3>
<p>This project uses OpenRouter (<strong>The Unified Interface For LLMs</strong>), via LiteLLM to access the DeepSeek model, which requires an API key. If you don‚Äôt already have an OpenRouter API key, you can create one for free at: <a href="https://openrouter.ai/keys">OpenRouter</a>.</p>
<p>Write your API key into an <code>.env</code> file as an environment variable, as follows:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="va">OPENROUTER_API_KEY</span><span class="op">=</span>...</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<blockquote class="blockquote">
<p>Note: make sure to add it to <code>.gitignore</code> to avoid committing it to the repository.</p>
<p>Note: this is different than the <code>.venv</code> file used for the virtual environment.</p>
</blockquote>
<p>If we use the OpenAI API, we‚Äôll have to add:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="va">OPENAI_API_BASE</span><span class="op">=</span><span class="st">"https://openrouter.ai/api/v1"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>.. such that the model uses OpenRouter instead of the default OpenAI API.</p>
</section>
<section id="sign-up-and-set-langsmith-api-free" class="level3">
<h3 class="anchored" data-anchor-id="sign-up-and-set-langsmith-api-free">Sign up and Set LangSmith API (Free)</h3>
<ul>
<li>Cost:</li>
<li>Sign up for LangSmith <a href="https://docs.langchain.com/langsmith/create-account-api-key#create-an-account-and-api-key">here</a>, find out more about LangSmith and how to use it within your workflow <a href="https://www.langchain.com/langsmith">here</a>.</li>
<li>Set <code>LANGSMITH_API_KEY</code>, <code>LANGSMITH_TRACING_V2="true"</code> <code>LANGSMITH_PROJECT="langchain-academy"</code>in your environment</li>
<li>If you are on the EU instance also set <code>LANGSMITH_ENDPOINT</code>=‚Äúhttps://eu.api.smith.langchain.com‚Äù as well.</li>
</ul>
</section>
<section id="set-up-tavily-api-for-web-search-free" class="level3">
<h3 class="anchored" data-anchor-id="set-up-tavily-api-for-web-search-free">Set up Tavily API for web search (Free)</h3>
<ul>
<li><p>Tavily Search API is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results.</p></li>
<li><p>You can sign up for an API key <a href="https://tavily.com/">here</a>. It‚Äôs easy to sign up and offers a very generous free tier. Some lessons (in Module 4) will use Tavily.</p></li>
<li><p>Set <code>TAVILY_API_KEY</code> in your environment.</p></li>
</ul>
</section>
<section id="install-dependencies" class="level3">
<h3 class="anchored" data-anchor-id="install-dependencies">Install dependencies</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> add langchain tavily-python langchain_openai langchain_community </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div id="f56162cc" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>load_dotenv()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># We use OpenRouter for the agent ‚Äî set OPENROUTER_API_KEY in .env</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Get your key at https://openrouter.ai/keys</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.environ.get(<span class="st">"OPENROUTER_API_KEY"</span>):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">raise</span> <span class="pp">RuntimeError</span>(</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:</span><span class="ch">\n</span><span class="st">"</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"OPENROUTER_API_KEY=your-openrouter-api-key"</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="models" class="level1">
<h1>Models</h1>
<p><a href="https://docs.langchain.com/oss/python/langchain/models">LLMs</a> are powerful AI tools that can interpret and generate text like humans. They‚Äôre versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.</p>
<p>The quality and capabilities of the model you choose directly impact your agent‚Äôs baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.</p>
<section id="choosing-between-models" class="level3">
<h3 class="anchored" data-anchor-id="choosing-between-models">Choosing between models</h3>
<ul>
<li><a href="https://openrouter.ai/models">Models | OpenRouter.ai</a></li>
<li><a href="https://llm-stats.com/">LLM Stats</a></li>
<li><a href="https://artificialanalysis.ai/models/recommend">Model Recommendation | Artficial Analysis</a>
<ul>
<li><a href="https://artificialanalysis.ai/text-to-speech/leaderboard">TTS | Artficial Analysis</a></li>
</ul></li>
<li><a href="https://arena.ai/leaderboard/text-to-image">Arena.ai</a></li>
<li><a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB: Embedding Leaderboard</a></li>
<li><a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">Open ASR</a></li>
</ul>
<p>Note: <strong>Agents</strong> require <a href="https://openrouter.ai/models?fmt=cards&amp;supported_parameters=tools"><strong>a model that supports tool calling</strong></a>.</p>
</section>
<section id="basic-usage" class="level2">
<h2 class="anchored" data-anchor-id="basic-usage">Basic usage</h2>
<p>Models can be utilized in two ways:</p>
<ol type="1">
<li><strong>With agents</strong> - Models can be dynamically specified when creating an <a href="../oss/python/langchain/agents#model">agent</a>.</li>
<li><strong>Standalone</strong> - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.</li>
</ol>
<p><a href="https://docs.langchain.com/oss/python/langchain/models">Here</a> is a useful how-to for all the things that you can do with chat models, but we‚Äôll show a few highlights below.</p>
<p>There are <a href="https://docs.langchain.com/oss/python/langchain/models#parameters">a few standard parameters</a> that we can set with chat models. Two of the most common are:</p>
<ul>
<li><code>model</code>: the name of the model</li>
<li><code>temperature</code>: the sampling temperature</li>
<li><code>max_tokens</code>: the maximum number of tokens to generate</li>
</ul>
<p><code>Temperature</code> controls the randomness or creativity of the model‚Äôs output where:</p>
<ul>
<li><strong>Low temperature</strong> (close to 0) is more deterministic and focused outputs. This is good for tasks requiring accuracy or factual responses.</li>
<li><strong>High temperature</strong> (close to 1) is good for creative tasks or generating varied responses.</li>
</ul>
<p><code>max_tokens</code> limits the total number of tokens in the response, effectively controlling how long the output can be.</p>
<p>LangChain supports many models via <a href="https://docs.langchain.com/oss/python/integrations/chat">third-party integrations</a>. By default, the course will use <a href="https://docs.langchain.com/oss/python/integrations/chat/openai">ChatOpenAI</a> because it is both popular and performant.</p>
<div id="44cc8280" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain_openai <span class="im">import</span> ChatOpenAI</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># https://openrouter.ai/openai/gpt-5-nano</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>model_gpt5_nano <span class="op">=</span> ChatOpenAI(</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"openai/gpt-5-nano"</span>,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    base_url<span class="op">=</span><span class="st">"https://openrouter.ai/api/v1"</span>,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    api_key<span class="op">=</span>os.environ.get(<span class="st">"OPENROUTER_API_KEY"</span>),</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># https://openrouter.ai/nvidia/nemotron-3-nano-30b-a3b:free</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>model_nemotron3_nano <span class="op">=</span> ChatOpenAI(</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"nvidia/nemotron-3-nano-30b-a3b:free"</span>,</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    base_url<span class="op">=</span><span class="st">"https://openrouter.ai/api/v1"</span>,</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    api_key<span class="op">=</span>os.environ.get(<span class="st">"OPENROUTER_API_KEY"</span>),</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm</code></pre>
</div>
</div>
<section id="running-a-model-locally" class="level3">
<h3 class="anchored" data-anchor-id="running-a-model-locally">Running a model locally</h3>
<p>LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.</p>
<p><a href="https://docs.langchain.com/oss/python/integrations/chat/ollama">Ollama</a> is one of the easiest ways to run chat and embedding models locally.</p>
</section>
</section>
<section id="key-methods" class="level2">
<h2 class="anchored" data-anchor-id="key-methods">Key Methods</h2>
<ol type="1">
<li>Invoke</li>
<li>Stream</li>
<li>Batch</li>
</ol>
<section id="invoke" class="level3">
<h3 class="anchored" data-anchor-id="invoke">1. Invoke</h3>
<p>The most straightforward way to call a model is to use <code>invoke()</code> with a single message or a list of messages:</p>
<div id="3b52c4ad" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>message <span class="op">=</span> model_nemotron3_nano.invoke(<span class="st">"why do parrots talk?"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>.. this returns an <code>AIMessage</code> object:</p>
<div id="b3b68f58" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>message</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>AIMessage(content='Parrots ‚Äútalk‚Äù because they‚Äôre one of the few animals that can **learn and reproduce complex vocalizations**‚Äîa skill called vocal learning. Here‚Äôs a quick rundown of why and how they do it:\n\n| Reason | What it means for parrots |\n|--------|--------------------------|\n| **Social bonding** | In the wild, parrots use calls to stay in contact with flock members, coordinate movement, and reinforce pair bonds. Mimicking the sounds of their companions (including human speech) helps them stay socially connected. |\n| **Territory &amp; status** | Some species use distinctive vocalizations to claim space or signal dominance. A parrot that can produce a clear, attention‚Äëgrabbing ‚Äúspeech‚Äù may gain more social leverage. |\n| **Mental stimulation** | Parrots are highly intelligent; they need cognitive challenges. Learning new sounds is a form of problem‚Äësolving that keeps their brains active and reduces boredom‚Äërelated behaviors (like feather‚Äëplucking). |\n| **Mimicry as a survival tool** | In the wild, many parrots imitate the calls of other species (e.g., hawks, other birds) to deter predators or to blend into mixed‚Äëspecies flocks. This ability extends to human speech when they live with people. |\n| **Neural specialization** | Parrots possess a brain region called the **song system** that is analogous to the vocal‚Äëlearning circuits of songbirds and humans. This system allows them to store, modify, and reproduce complex sound patterns with high fidelity. |\n\n### How they actually ‚Äútalk‚Äù\n\n1. **Listening &amp; Encoding** ‚Äì A parrot constantly hears sounds in its environment and stores a mental ‚Äútemplate‚Äù of each one.\n2. **Practice &amp; Refinement** ‚Äì Using its syrinx (the bird equivalent of vocal cords), it tries out different variations, gradually shaping the output to match the stored template.\n3. **Feedback Loop** ‚Äì The bird hears its own attempt, compares it to the template, and tweaks pitch, rhythm, or timing until it‚Äôs close enough.\n4. **Reinforcement** ‚Äì Positive attention from humans (treats, praise, interaction) reinforces the behavior, encouraging the parrot to repeat the sound.\n\n### Why some parrots sound more ‚Äúhuman‚Äëlike‚Äù\n\n- **Species differences** ‚Äì African Grey, Amazon, and some Cockatoos have especially flexible vocal repertoires.\n- **Individual exposure** ‚Äì The more varied and frequent the human speech they hear, the richer their mimicry can become.\n- **Training &amp; interaction** ‚Äì Repetition, consistent cues, and reward-based training help solidify specific words or phrases.\n\n### Bottom line\n\nParrots ‚Äútalk‚Äù not because they understand language the way we do, but because they‚Äôre **social, intelligent mimics** equipped with a unique vocal‚Äëlearning brain circuit. By copying the sounds that matter most to them‚Äîwhether it‚Äôs a flock mate‚Äôs call, a predator‚Äôs alarm, or a human‚Äôs voice‚Äîthey enhance their social world and keep their minds sharp. \n\nIf you‚Äôre curious about getting a talking parrot, remember that the ability to mimic speech varies widely among individuals and species, and it thrives best with plenty of social interaction, mental enrichment, and consistent, positive training.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 731, 'prompt_tokens': 22, 'total_tokens': 753, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 58, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771586508-ixswTWdhOMhqmt9bxk5a', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c7ac8-d0ab-75d1-aeeb-5bcfce7f31bb-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 22, 'output_tokens': 731, 'total_tokens': 753, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 58}})</code></pre>
</div>
</div>
<p>.. which has a <code>content</code> property, which includes the generated response text:</p>
<div id="4efc494d" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(message.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parrots ‚Äútalk‚Äù because they‚Äôre one of the few animals that can **learn and reproduce complex vocalizations**‚Äîa skill called vocal learning. Here‚Äôs a quick rundown of why and how they do it:

| Reason | What it means for parrots |
|--------|--------------------------|
| **Social bonding** | In the wild, parrots use calls to stay in contact with flock members, coordinate movement, and reinforce pair bonds. Mimicking the sounds of their companions (including human speech) helps them stay socially connected. |
| **Territory &amp; status** | Some species use distinctive vocalizations to claim space or signal dominance. A parrot that can produce a clear, attention‚Äëgrabbing ‚Äúspeech‚Äù may gain more social leverage. |
| **Mental stimulation** | Parrots are highly intelligent; they need cognitive challenges. Learning new sounds is a form of problem‚Äësolving that keeps their brains active and reduces boredom‚Äërelated behaviors (like feather‚Äëplucking). |
| **Mimicry as a survival tool** | In the wild, many parrots imitate the calls of other species (e.g., hawks, other birds) to deter predators or to blend into mixed‚Äëspecies flocks. This ability extends to human speech when they live with people. |
| **Neural specialization** | Parrots possess a brain region called the **song system** that is analogous to the vocal‚Äëlearning circuits of songbirds and humans. This system allows them to store, modify, and reproduce complex sound patterns with high fidelity. |

### How they actually ‚Äútalk‚Äù

1. **Listening &amp; Encoding** ‚Äì A parrot constantly hears sounds in its environment and stores a mental ‚Äútemplate‚Äù of each one.
2. **Practice &amp; Refinement** ‚Äì Using its syrinx (the bird equivalent of vocal cords), it tries out different variations, gradually shaping the output to match the stored template.
3. **Feedback Loop** ‚Äì The bird hears its own attempt, compares it to the template, and tweaks pitch, rhythm, or timing until it‚Äôs close enough.
4. **Reinforcement** ‚Äì Positive attention from humans (treats, praise, interaction) reinforces the behavior, encouraging the parrot to repeat the sound.

### Why some parrots sound more ‚Äúhuman‚Äëlike‚Äù

- **Species differences** ‚Äì African Grey, Amazon, and some Cockatoos have especially flexible vocal repertoires.
- **Individual exposure** ‚Äì The more varied and frequent the human speech they hear, the richer their mimicry can become.
- **Training &amp; interaction** ‚Äì Repetition, consistent cues, and reward-based training help solidify specific words or phrases.

### Bottom line

Parrots ‚Äútalk‚Äù not because they understand language the way we do, but because they‚Äôre **social, intelligent mimics** equipped with a unique vocal‚Äëlearning brain circuit. By copying the sounds that matter most to them‚Äîwhether it‚Äôs a flock mate‚Äôs call, a predator‚Äôs alarm, or a human‚Äôs voice‚Äîthey enhance their social world and keep their minds sharp. 

If you‚Äôre curious about getting a talking parrot, remember that the ability to mimic speech varies widely among individuals and species, and it thrives best with plenty of social interaction, mental enrichment, and consistent, positive training.</code></pre>
</div>
</div>
<p>A list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.</p>
<div id="39f4c8fc" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.messages <span class="im">import</span> SystemMessage, HumanMessage, AIMessage</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>conversation <span class="op">=</span> [    </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    SystemMessage(content<span class="op">=</span><span class="st">"You are a helpful assistant that translates English to Arabic."</span>),</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    HumanMessage(content<span class="op">=</span><span class="st">"Translate: I love programming."</span>),</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    AIMessage(content<span class="op">=</span><span class="st">"ÿ£ÿ≠ÿ® ÿßŸÑÿ®ÿ±ŸÖÿ¨ÿ©."</span>),</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    HumanMessage(content<span class="op">=</span><span class="st">"I love building applications."</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>message <span class="op">=</span> model_nemotron3_nano.invoke(conversation)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(message.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>ÿ£ÿ≠ÿ® ÿ™ÿ∑ŸàŸäÿ± ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™.</code></pre>
</div>
</div>
</section>
<section id="stream" class="level3">
<h3 class="anchored" data-anchor-id="stream">2. Stream</h3>
<p>Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.</p>
<p>Calling <code>stream()</code> returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:</p>
<div id="3c795917" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> chunk <span class="kw">in</span> model_nemotron3_nano.stream(<span class="st">"Why do parrots have colorful feathers?"</span>):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(chunk.text, end<span class="op">=</span><span class="st">"|"</span>, flush<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Par|ro|ts| are| famous| for| their| vivid| plum|age|,| and| that| color|ation| isn|‚Äôt| just| for| show| ‚Äî| it| serves| several| important| functions| that| have| been| shaped| by| evolution|.| Here|‚Äôs| a| quick| rund|own| of| the| main| reasons|:

||| Reason| || How| it| works| || Why| it| matters| for| parro|ts| |
|||--------|||------------|--|||----------------|------------||
||| **|Sex|ual| selection|**| || Bright|,| contrasting| colors| signal| health|,| good| genetics|,| and| strong| immune| systems|.| Males| and| females| often| use| plum|age| to| attract| mates| or| to| assess| rivals|.| || In| many| par|rot| species|,| brighter| males| are| preferred| by| females|,| leading| to| stronger| reproductive| success| for| those| with| more| vivid| feathers|.| |
||| **|Species| and| individual| recognition|**| || Dist|inct| color| patterns| help| individuals| identify| members| of| their| own| species| (|and| sometimes| specific| mates| or| offspring|).| || In| dense|,| multi|‚Äë|species| fl|ocks|,| a| unique| ‚Äú|signature|‚Äù| color|ation| reduces| the| chance| of mistaken| identity and helps| keep| the| group| cohesive|.| |
||| **|Social| signaling|**| || Color| can| convey| status|,| dominance|,| or| readiness| to| breed|.| Some| parro|ts| change| the| intensity| or| hue| of| their| feathers| during| courts|hip| or| when| threatened|.| || A| sudden| flash| of| bright| color| can| start|le| a| predator| or| signal| aggression| to| a| rival|,| while| a| muted| palette| may| indicate| submission| or| non|‚Äë|th|reat|.| |
||| **|Cam|oufl|age| in| specific| habitats|**| || While| many| parro|ts| are| conspicuous|,| some| (|e|.g|.,| forest|‚Äë|dw|elling| species|)| have| gre|ens| and| brown|s| that| blend| with| foli|age|.| || In| these| cases|,| color|ation| is| an| adaptation| to| the| visual| background| of| their| environment|,| helping| them| avoid| predators| while| still| being| visible| to| cons|pecific|s|.| |
||| **|D|iet|‚Äë|derived| pigments|**| || Parro|ts| cannot| synthesize| many| bright| pigments| themselves|;| they| obtain| red|s|,| or|anges|,| and| yellow|s| from| carot|eno|ids| in| fruits|,| flowers|,| and| seeds|.| || A| diet| rich| in| these| pigments| not| only| fuels| health| but| also| directly| influences| how| vivid| a| bird|‚Äôs| feathers| appear|.| |
||| **|Struct|ural| color|ation|**| || Mic|roscopic| structures| in| feather| barb|ules| scatter| light|,| producing| ir|ides|cent| blues| and| gre|ens| that| are| not| pigment|‚Äë|based|.| || This| creates| sh|immer|ing| effects| that| can| be| more| eye|‚Äë|catch|ing| than| pigment| alone|,| especially| under| different| lighting| conditions|.| |

|###| Put|ting| it| together|
|-| **|E|volution|ary| pressure|**:| Bright| colors| increase| an| individual|‚Äôs| chances| of| finding| a| mate| and| defending| territory|,| which| boost|s| reproductive| success|.
|-| **|Ec|ological| context|**:| The| same| colors| that| attract| mates| can| also| serve| as| warning| signals| or| camoufl|age|,| depending| on| the| species|‚Äô| habitat| and| behavior|.
|-| **|Phys|iological| limits|**:| Because| parro|ts| rely| on| dietary| pigments| and| structural| feather| features|,| their| color| palette| is| tightly| linked| to| what| they| eat| and| how| their| feathers| are| built.

|So|,| the| spectacular| rainbow| of| par|rot| feathers| is| a| multif|unctional| adaptation| ‚Äî| primarily| a| tool| for| communication| and| reproduction|,| fine|‚Äë|t|uned| by| diet|,| habitat|,| and| the| physics| of| light|.||| üåà||||ü¶ú||||</code></pre>
</div>
</div>
</section>
<section id="batch" class="level3">
<h3 class="anchored" data-anchor-id="batch">3. Batch</h3>
<p>Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:</p>
<div id="b1cd4f85" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>responses <span class="op">=</span> model_nemotron3_nano.batch([</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is the capital of Saudi Arabia?"</span>,</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is 2 + 8"</span>,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Is the sky blue or is it our perception? give a short and concise answer"</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> response <span class="kw">in</span> responses:</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(response)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>content='The capital of Saudi Arabia is **Riyadh**.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 24, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 26, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587230-rws1sKd4yd2Ir45jBE92', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-dab8-7670-aebf-be1a19d73902-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 24, 'output_tokens': 38, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 26}}
content='2\u202f+\u202f8\u202f=\u202f10.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 23, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 22, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587230-FEcwZ8VMp5p1Qgvgnrqr', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-daba-76d3-856b-d4204b17db9a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 23, 'output_tokens': 40, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 22}}
content='The sky appears blue because molecules in the atmosphere scatter short‚Äëwavelength (blue) sunlight‚Äîan objective physical effect that our visual system interprets as the color blue.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 32, 'total_tokens': 206, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 159, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587231-VBn1J8V9CcCv10J2gqBT', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-dabc-7871-bdd8-f89b2bed8661-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 32, 'output_tokens': 174, 'total_tokens': 206, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 159}}</code></pre>
</div>
</div>
<div id="2264be1a" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, response <span class="kw">in</span> <span class="bu">enumerate</span>(responses):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(response.content)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">100</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>The capital of Saudi Arabia is **Riyadh**.
====================================================================================================
2‚ÄØ+‚ÄØ8‚ÄØ=‚ÄØ10.
====================================================================================================
The sky appears blue because molecules in the atmosphere scatter short‚Äëwavelength (blue) sunlight‚Äîan objective physical effect that our visual system interprets as the color blue.
====================================================================================================</code></pre>
</div>
</div>
</section>
</section>
<section id="structured-output" class="level2">
<h2 class="anchored" data-anchor-id="structured-output">Structured output</h2>
<p>Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.</p>
<p><a href="https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage">Pydantic models</a> provide the richest feature set with field validation, descriptions, and nested structures.</p>
<div id="4765e1b9" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel, Field</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Movie(BaseModel):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A movie with details."""</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    title: <span class="bu">str</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"The title of the movie"</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    year: <span class="bu">int</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"The year the movie was released"</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    director: <span class="bu">str</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"The director of the movie"</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    rating: <span class="bu">float</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"The movie's rating out of 10"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>title='Inception' year=2010 director='Christopher Nolan' rating=8.8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:
  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=Movie(title='Inception', ...pher Nolan', rating=8.8), input_type=Movie])
  return self.__pydantic_serializer__.to_python(</code></pre>
</div>
</div>
<div id="49f79423" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>model_with_structure <span class="op">=</span> model_nemotron3_nano.with_structured_output(Movie)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> model_with_structure.invoke(<span class="st">"Provide details about the movie Inception"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="634b07db" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Title:"</span>, response.title)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Year:"</span>, response.year)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Director:"</span>, response.director)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Rating:"</span>, response.rating)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Title: Inception
Year: 2010
Director: Christopher Nolan
Rating: 8.8</code></pre>
</div>
</div>
</section>
<section id="tool-calling" class="level2">
<h2 class="anchored" data-anchor-id="tool-calling">Tool calling</h2>
<p>Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:</p>
<ol type="1">
<li>A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)</li>
<li>A function or coroutine to execute.</li>
</ol>
<p>Note: A <em>coroutine</em> is a method that can suspend execution and resume at a later time</p>
<div id="d5d53c5b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.tools <span class="im">import</span> tool</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="at">@tool</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_weather(location: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get the weather at a location."""</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ss">f"It's always sunny in </span><span class="sc">{</span>location<span class="sc">}</span><span class="ss">."</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>model_with_tools <span class="op">=</span> model_nemotron3_nano.bind_tools([get_weather])</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> model_with_tools.invoke(<span class="st">"What's the weather like in the Moon?"</span>)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tool_call <span class="kw">in</span> response.tool_calls:</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># View tool calls made by the model</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Tool: </span><span class="sc">{</span>tool_call[<span class="st">'name'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Args: </span><span class="sc">{</span>tool_call[<span class="st">'args'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tool: get_weather
Args: {'location': 'Boston'}</code></pre>
</div>
</div>
<section id="tool-input-schemas" class="level3">
<h3 class="anchored" data-anchor-id="tool-input-schemas">Tool Input Schemas</h3>
<p>Define complex inputs with Pydantic models or JSON schemas:</p>
<div id="530c429c" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel, Field</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Literal</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> WeatherInput(BaseModel):</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Input for weather queries."""</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    location: <span class="bu">str</span> <span class="op">=</span> Field(description<span class="op">=</span><span class="st">"City name or coordinates"</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    units: Literal[<span class="st">"celsius"</span>, <span class="st">"fahrenheit"</span>] <span class="op">=</span> Field(</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        default<span class="op">=</span><span class="st">"celsius"</span>,</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        description<span class="op">=</span><span class="st">"Temperature unit preference"</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    include_forecast: <span class="bu">bool</span> <span class="op">=</span> Field(</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        default<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        description<span class="op">=</span><span class="st">"Include 5-day forecast"</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="at">@tool</span>(args_schema<span class="op">=</span>WeatherInput)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_weather(location: <span class="bu">str</span>, units: <span class="bu">str</span> <span class="op">=</span> <span class="st">"celsius"</span>, include_forecast: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get current weather and optional forecast."""</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    temp <span class="op">=</span> <span class="dv">22</span> <span class="cf">if</span> units <span class="op">==</span> <span class="st">"celsius"</span> <span class="cf">else</span> <span class="dv">72</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> <span class="ss">f"Current weather in </span><span class="sc">{</span>location<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>temp<span class="sc">}</span><span class="ss"> degrees </span><span class="sc">{</span>units[<span class="dv">0</span>]<span class="sc">.</span>upper()<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> include_forecast:</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>        result <span class="op">+=</span> <span class="st">"</span><span class="ch">\n</span><span class="st">Next 5 days: Sunny"</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="9ec84860" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>model_with_tools <span class="op">=</span> model_nemotron3_nano.bind_tools([get_weather])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="afe66a0d" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> model_with_tools.invoke(</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What's the weather like in the Moon? "</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"in fahrenheit and include the forecast please."</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tool_call <span class="kw">in</span> response.tool_calls:</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># View tool calls made by the model</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Tool: </span><span class="sc">{</span>tool_call[<span class="st">'name'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Args: </span><span class="sc">{</span>tool_call[<span class="st">'args'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tool: get_weather
Args: {'location': 'the Moon', 'units': 'fahrenheit', 'include_forecast': True}</code></pre>
</div>
</div>
</section>
</section>
<section id="search-tools" class="level2">
<h2 class="anchored" data-anchor-id="search-tools">Search Tools</h2>
<p>Tavily is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it‚Äôs easy to sign up and offers a generous free tier.</p>
<div id="3fcc980e" class="cell" data-execution_count="26">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tavily <span class="im">import</span> TavilyClient</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>tavily_client <span class="op">=</span> TavilyClient(api_key<span class="op">=</span>os.environ[<span class="st">"TAVILY_API_KEY"</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="539beacd" class="cell" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> internet_search(</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    query: <span class="bu">str</span>,</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    max_results: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    topic: Literal[<span class="st">"general"</span>, <span class="st">"news"</span>, <span class="st">"finance"</span>] <span class="op">=</span> <span class="st">"general"</span>,</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    include_raw_content: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Run a web search"""</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tavily_client.search(</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        query,</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        max_results<span class="op">=</span>max_results,</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>        include_raw_content<span class="op">=</span>include_raw_content,</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>        topic<span class="op">=</span>topic,</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="45c13907" class="cell" data-execution_count="35">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> internet_search(<span class="st">"What is LangGraph?"</span>, max_results<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>result</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>{'query': 'What is LangGraph?',
 'response_time': 0.61,
 'follow_up_questions': None,
 'answer': None,
 'images': [],
 'results': [{'url': 'https://www.ibm.com/think/topics/langgraph',
   'title': 'What is LangGraph? - IBM',
   'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent‚Äôs state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.',
   'score': 0.95539,
   'raw_content': None},
  {'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/',
   'title': 'What is LangGraph? - GeeksforGeeks',
   'content': 'LangGraph is an open-source framework built by LangChain that streamlines the creation and management of AI agent workflows. At its core, LangGraph combines large language models (LLMs) with graph-based architectures allowing developers to map, organize and optimize how AI agents interact and make decisions. By treating workflows as interconnected nodes and edges, LangGraph offers a scalable, transparent and developer-friendly way to design advanced AI systems ranging from simple chatbots to multi-agent system. The diagram below shows how LangGraph structures its agent-based workflow using distinct tools and stages. By designing workflows, users combine multiple nodes into powerful, dynamic AI processes. * ****langgraph:**** Framework for building graph-based AI workflows. ### Step 6: Build LangGraph Workflow. * Build the workflow graph using LangGraph, adding nodes for classification and response, connecting them with edges and compiling the app. * Send each input through the workflow graph and returns the bot‚Äôs response, either a greeting or an AI-powered answer. + Machine Learning Interview Questions and Answers15+ min read.',
   'score': 0.9393874,
   'raw_content': None},
  {'url': 'https://docs.langchain.com/oss/python/langgraph/overview',
   'title': 'LangGraph overview - Docs by LangChain',
   'content': 'Trusted by companies shaping the future of agents‚Äî including Klarna, Replit, Elastic, and more‚Äî LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain‚Äôs agents that provide pre-built architectures for common LLM and tool-calling loops. LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more. LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.## LangSmith Agent Server. Contains agent abstractions built on top of LangGraph.',
   'score': 0.92507464,
   'raw_content': None}],
 'request_id': '4d1b1ba4-3b2c-4355-a9d6-446cbddf26e0'}</code></pre>
</div>
</div>
</section>
<section id="create-an-agent" class="level2">
<h2 class="anchored" data-anchor-id="create-an-agent">Create an Agent</h2>
<p>Agents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.</p>
<p>An LLM Agent runs tools in a loop to achieve a goal. An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./assets/agent_loop.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Agent Loop"><img src="./assets/agent_loop.png" class="img-fluid figure-img" alt="Agent Loop"></a></p>
<figcaption>Agent Loop</figcaption>
</figure>
</div>
<p><code>create_agent</code> provides a production-ready agent implementation.</p>
<div id="065e6367" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.agents <span class="im">import</span> create_agent</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="co"># System prompt to steer the agent to be an expert researcher</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>AGENT_PROMPT <span class="op">=</span> <span class="st">"""You are an expert researcher. Your job is to conduct thorough research and then write a polished report.</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="st">You have access to an internet search tool as your primary means of gathering information.</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="st">Keep it short and concise.</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="st">## `internet_search`</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="st">Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> create_agent(</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model_nemotron3_nano,</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>    tools<span class="op">=</span>[internet_search],</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>    system_prompt<span class="op">=</span>AGENT_PROMPT</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="invoke-1" class="level3">
<h3 class="anchored" data-anchor-id="invoke-1">Invoke</h3>
<div id="e633e4c7" class="cell" data-execution_count="37">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> agent.invoke({<span class="st">"messages"</span>: [{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="st">"What is langgraph?"</span>}]})</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="be1de6bd" class="cell" data-execution_count="38">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>result</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>{'messages': [HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={}, id='29083314-fa84-4871-ba0d-f594b69ce1fe'),
  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 327, 'prompt_tokens': 447, 'total_tokens': 774, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 292, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771589000-vrLW5z3J6cADEb7ZfFeY', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c7aee-db92-70c2-9d0e-ef86169a1ec1-0', tool_calls=[{'name': 'internet_search', 'args': {'max_results': 5, 'topic': 'general', 'query': 'LangGraph', 'include_raw_content': True}, 'id': 'call_89580e7dd35048f5a3921112', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 447, 'output_tokens': 327, 'total_tokens': 774, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 292}}),
  ToolMessage(content='{"query": "LangGraph", "follow_up_questions": null, "answer": null, "images": [], "results": [{"url": "https://www.ibm.com/think/topics/langgraph", "title": "What is LangGraph? - IBM", "content": "LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent‚Äôs state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.", "score": 0.94251215, "raw_content": "# What is LangGraph?\\n\\n## Authors\\n\\n[Bryan Clark](https://www.ibm.com/think/author/bryan-clark) \\n\\nSenior Technology Advocate\\n\\n## LangGraph overview\\n\\nLangGraph, created by [LangChain](https://www.ibm.com/think/topics/langchain), is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize [large language models](https://www.ibm.com/think/topics/large-language-models) (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an [AI agent workflow](https://www.ibm.com/think/topics/ai-agents).\\n\\nWhat does all this information mean? The following example can offer a clearer understanding of LangGraph: Think about these graph-based architectures as a powerful configurable map, a ‚ÄúSuper-Map.‚Äù Users can envision the [AI workflow](https://www.ibm.com/think/topics/ai-workflow) as being ‚ÄúThe Navigator‚Äù of this ‚ÄúSuper-Map.‚Äù Finally, in this example, the user is ‚ÄúThe Cartographer.‚Äù In this sense, the navigator charts out the optimal routes between points on the ‚ÄúSuper-Map,‚Äù all of which are created by ‚ÄúThe Cartographer.‚Äù\\n\\nTo recap, optimal routes within the graph-based architectures (‚ÄúSuper-Map‚Äù) are charted and explored by using the AI workflow\xa0(‚ÄúThe Navigator‚Äù). This analogy is a great place to start understanding LangGraph‚Äîand if you like maps then you are welcome for the bonus opportunity to see someone use the word cartographer.\\n\\nLangGraph workflow\\n\\nLangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent‚Äôs state. Within LangGraph, the ‚Äústate‚Äù feature serves as a memory bank that records and tracks all the valuable information processed by the AI system. It‚Äôs similar to a digital notebook where the system captures and updates data as it moves through various stages of a workflow or graph analysis.\\n\\nFor example, if you were running agents to monitor the weather, this feature could track the number of times it snowed and make suggestions based on changing snowfall trends. This observability of how the system works to complete complex tasks is useful for beginners to understand more about state management. State management is helpful when it comes to debugging as it allows the application‚Äôs state to be centralized, thus often shortening the overall process.\\n\\nThis approach allows for more effective decision-making, improved scalability and enhanced overall performance. It also allows for more engagement with individuals who might be new to these processes or prefer a clearer picture of what is going on behind the scenes.\\n\\nLangGraph is also built on several key technologies, including [LangChain,](https://www.ibm.com/think/topics/langchain) a Python framework for building AI applications. LangChain includes a library for building and managing [LLMs](https://www.ibm.com/think/topics/large-language-models). LangGraph also uses the human-in-the-loop approach. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including [chatbots](https://www.ibm.com/think/topics/chatbots), state graphs and [other agent-based systems](https://www.ibm.com/think/topics/multiagent-system).\\n\\nDelve deeper into the world of LangGraph by exploring its key features, benefits and use cases. By the end of this article, you will have the knowledge and resources to take the next steps with LangGraph.\\n\\n## Key components of LangGraph\\n\\nLet‚Äôs begin by first understanding the key components that make up LangGraph. The framework is built around several key components that work together to enable users to create and manage complex AI workflows. These components include:\\n\\n#### Monitoring mechanism\\n\\n**Human-in-the-loop**: [Human-in-the-loop (HITL)](https://hdsr.mitpress.mit.edu/pub/812vijgg/release/3)\xa0refers to the requirement of human interaction at some point in the process. In the realm of [machine learning](https://www.ibm.com/think/topics/machine-learning) (ML), HITL refers to a collaborative process where humans augment the computational capabilities of machines to make informed decisions while building a model. By using the most critical data points, HITL enhances the accuracy of machine learning algorithms, surpassing random sampling methods.\\n\\n#### Graph architecture\\n\\n**Stateful graphs**: A concept where each node in the graph represents a step in the computation, essentially devising a state graph. This stateful approach allows the graph to retain information about the previous steps, enabling continuous and contextual processing of information as the computation unfolds. Users can manage all LangGraph‚Äôs stateful graphs with its APIs.\\n\\n**Cyclical graph**: A cyclical graph is any graph that contains at least one cycle and is essential for agent runtimes. This means that there exists a path that starts and ends at the same node, forming a loop within the graph. Complex workflows often involve cyclic dependencies, where the outcome of one step depends on previous steps in the loop.\\n\\n**Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. Nodes can be thought of as ‚Äúactors‚Äù that interact with each other in a specific way. For example,\xa0to add nodes for tool calling, one can use the ToolNode. Another example, the next node, refers to the node that will be executed following the current one.\\n\\n**Edges**: Edges are a function within Python that determines which node to execute next based on the current state. Edges can be conditional branches or fixed transitions.\\n\\n#### Tools\\n\\n**RAG**: [Retrieval-augmented generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) combines the power of LLMs with contextual information from external sources by retrieving relevant documents, which are then used as input for answer generation.\\n\\n**Workflows**: Workflows are the sequences of node interactions that define an AI workflow. By arranging nodes into a workflow, users can create more complex and dynamic workflows that use the strengths of individual components.\\n\\n**APIs**: LangGraph provides a set of [APIs](https://www.ibm.com/think/topics/api) that enable users to interact with its components in a programmatic way. Users can use an API key, add new nodes, modify existing workflows and retrieve data from an AI workflow.\\n\\n**LangSmith**: LangSmith is a specialized API for building and managing LLMs within LangGraph. It provides tools for initializing LLMs, adding conditional edges and optimizing performance. By combining these components in innovative ways, users can build more sophisticated AI workflows that use the strengths of individual components.\\n\\nIndustry newsletter\\n\\n### The latest AI trends, brought to you by experts\\n\\nGet curated insights on the most important‚Äîand intriguing‚ÄîAI news. Subscribe to our weekly Think newsletter. See the [IBM Privacy Statement](https://www.ibm.com/privacy).\\n\\n### Thank you! You are subscribed.\\n\\nYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe [here](https://www.ibm.com/account/reg/signup?formid=news-urx-51525). Refer to our [IBM Privacy Statement](https://www.ibm.com/us-en/privacy) for more information.\\n\\n## How LangGraph scales\\n\\nBy using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.\\n\\n**Enhanced decision-making**: By modeling complex relationships between nodes, LangGraph provides a framework for building more effective decision-making systems.\\n\\n**Increased flexibility**: An open source nature and modular design for developers to integrate new components and adapt existing workflows.\\n\\n**Multiagent workflows:** Complex tasks can be tackled through multiagent workflows. This approach involves creating dedicated LangChain agents for specific tasks or domains. Routing tasks to the appropriate LangChain agents allows for parallel execution and efficient handling of diverse workloads. Such a multiagent network architecture exemplifies the decentralized\xa0coordination of agent automation.\\n\\nA great example, created by Joao Moura, is using CrewAI with LangChain and LangGraph. Checking emails and creating drafts is automated with CrewAI orchestrating autonomous AI agents, enabling them to collaborate and run complex tasks efficiently.\\n\\nIBM watsonx.ai\\n\\n### Data Insights with LangGraph and watsonx.ai\\n\\nCan an AI agent take our natural language query and do the processing for us to give us that meaningful output? We use several pieces of open source technology and the power of watsonx.ai to\xa0put this to the test.\\n\\n[Explore watsonx.ai](https://www.ibm.com/products/watsonx-ai)\\n\\n## LangGraph use cases\\n\\n**Chatbots**: Users can build an agentic application for vacation planning, with node-based workflows and directed acyclic graphs (DAGs). The chatbot learns to respond to minimal user input and tailor recommendations. Currently, services such as Google‚Äôs Duplex are using LangGraph in a similar fashion to mimic human-like conversations.\\n\\n**Agent systems**: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\\n\\n**LLM applications**: By using LangGraph‚Äôs capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences.\\n\\n## LLM integration in LangGraph\\n\\nLangGraph‚Äôs agents are based on OpenAI‚Äôs series of GPT (generative pretrained transformer) models GPT-3.5 and GPT-4. However, LangGraph and its open source community have contributed to the addition of several other models that initialize through LLM API configuration, including Anthropic and AzureChatOpenAI models. The relatively small loop is similar to projects such as Auto-GPT.\\n\\nLangGraph offers a YouTube tutorial that facilitates the exploration of how to integrate with open source LLMs on its GitHub docs site. The first step to integrating an LLM is to set up an inference repository (repo) such as LLaMA-Factory, FastChat and Ollama. This repository enables deployment of the corresponding LLM model that is configured through its API credentials.\\n\\n## Other AI agent frameworks\\n\\nCrewAI, MetaGPT and AutoGen are just a few multiagent frameworks that can handle complex workflows. This operation allows for a more flexible and nuanced approach to tackling diverse computational challenges. By providing comprehensive debugging capabilities, these\xa0frameworks enable developers to quickly identify and resolve issues, leading to more efficient development and optimization processes.\\n\\n## LangGraph Studio: A visual interface for workflow development\\n\\nLangGraph has also introduced LangGraph Studio, a visual interface for workflow development. With LangGraph Studio, users can design and build workflows by using a graphical interface, without having to write code. The downloadable desktop application makes LangGraph Studio more usable for beginners. LangGraph Studio has also made these additional features available:\\n\\n**Shallow learning curve**: LangGraph Studio is not needed to access LangGraph. However, by using LangGraph Studio‚Äôs visual interface, users can focus on designing their workflows without getting bogged down in code.\\n\\n**Improved collaboration**: LangGraph Studio enables the sharing of workflows with others, whether that‚Äôs a team of developers or a client.\\n\\n**Debugging**: The capabilities do not end with building a graph, debugging features are included to ensure the graph is accurate and reliable. LangGraph Studio, with its cutting-edge integrated development environment (IDE), helps visualize and debug LangGraph applications.\\n\\n## Future developments\\n\\n**Enhanced natural language processing (NLP)**: LangGraph will have more advanced [NLP](https://www.ibm.com/think/topics/natural-language-processing) capabilities, allowing it to better understand natural language and provide more accurate responses.\\n\\n**Improved machine learning**: LangGraph will have improved machine learning capabilities, allowing it to learn and improve over time.\\n\\n**Support for new platforms**: LangGraph will support new platforms, such as mobile devices and edge computing to make its technology more accessible.\\n\\nLink copied\\n\\n[Ebook   Start realizing ROI: A practical guide to agentic AI \\n\\nDiscover ways to get ahead, successfully scaling AI across your business with real results.\\n\\n Read the ebook](https://www.ibm.com/account/reg/signup?formid=urx-54067)\\n\\n[Build, run and manage AI agents with watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)\\n\\n## Resources\\n\\n[IBV Report   The enterprise in 2030: Engineered for perpetual innovation \\n\\nDiscover our five predictions about what will define the most successful enterprises in 2030, and the steps leaders can take to gain an AI-first advantage.\\n\\n Read the report](https://www.ibm.com/thought-leadership/institute-business-value/report/enterprise-2030)\\n\\n[Report   AI governance imperative: evolving regulations and emergence of agentic AI \\n\\nLearn how evolving regulations and the emergence of AI agents are reshaping the need for robust AI governance frameworks.\\n\\n Read the report](https://www.ibm.com/forms/mkt-54070)\\n\\n[Techsplainers podcast   Agentic AI explained \\n\\nTechsplainers by IBM breaks down the essentials of agentic AI, from key concepts to real‚Äëworld use cases. Clear, quick episodes help you learn the fundamentals fast.\\n\\n Listen now](https://www.ibm.com/think/podcasts/techsplainers#tabs-fw-44e285b2cc-item-a3ea4f0927-tab)\\n\\n[Guidebook   Unlock AI ROI: A tactical guide to enterprise productivity \\n\\nLearn proven strategies to boost productivity and power enterprise transformation with AI and innovation at the core.\\n\\n Read the guidebook](https://www.ibm.com/account/reg/signup?formid=urx-54227)\\n\\n[Report   IDC MarketScape names IBM a leader in 2025 gen AI evaluation technology \\n\\nDownload the report to learn why IDC MarketScape named IBM a leader in 2025 gen AI evaluation technology, and how watsonx.governance¬Æ advances risk management, reporting and integration.\\n\\n Read the report](https://www.ibm.com/forms/mkt-54030)\\n\\n[Buyer guide   How AI agents and assistants can benefit your organization \\n\\nDive into this comprehensive guide that breaks down key use cases and core capabilities, providing step-by-step recommendations to help you choose the right solutions for your business.\\n\\n Read the guide](https://www.ibm.com/forms/mkt-53811)\\n\\n[Video   Reimagine business productivity with AI agents and assistants \\n\\nLearn how AI agents and AI assistants can work together to achieve new levels of productivity.\\n\\n Watch now](https://www.ibm.com/think/videos/ai-academy/reimagine-business-productivity-with-ai)\\n\\n[Demo   Try watsonx Orchestrate¬Æ \\n\\nExplore how generative AI assistants can lighten your workload and improve productivity.\\n\\n Start the demo](https://www.ibm.com/products/watsonx-orchestrate/demos)\\n\\n[Report   From AI projects to profits: How agentic AI can sustain financial returns \\n\\nLearn how organizations are shifting from launching AI in disparate pilots to using it to drive transformation at the core.\\n\\n Read the report](https://www.ibm.com/thought-leadership/institute-business-value/report/agentic-ai-profits)\\n\\n[Report   Omdia Report on empowered intelligence: The impact of AI agents \\n\\nDiscover how you can unlock the full potential of gen AI with AI agents.\\n\\n Read the report](https://www.ibm.com/forms/mkt-53501)\\n\\n[Podcast   How AI agents will reinvent productivity \\n\\nLearn ways to use AI to be more creative, efficient and start adapting to a future that involves working closely with AI agents.\\n\\n Listen now](https://www.ibm.com/think/podcasts/ai-in-action/how-ai-agents-will-reinvent-productivity)\\n\\n[News   Ushering in the agentic enterprise: Putting AI to work across your entire technology estate \\n\\nStay updated about the new emerging AI agents, a fundamental tipping point in the AI revolution.\\n\\n Read the news](https://www.ibm.com/new/announcements/productivity-revolution-with-ai-agents-that-work-across-stack)\\n\\n[Podcast   The future of agents, AI energy consumption, Anthropic computer use and Google watermarking AI-generated text \\n\\nStay ahead of the curve with our AI experts on this episode of Mixture of Experts as they dive deep into the future of AI agents and more.\\n\\n Listen now](https://www.ibm.com/think/podcasts/mixture-of-experts/future-of-ai-agents-ai-energy-consumption-anthropic-computer-use-google-watermarking-ai)\\n\\n[Case study   How Comparus is using a \\"banking assistant\\" \\n\\nComparus used solutions from IBM watsonx.ai¬Æ and impressively demonstrated the potential of conversational banking as a new interaction model.\\n\\n Read the case study](https://www.ibm.com/case-studies/comparus-gmbh)\\n\\nRelated solutions   \\n IBM¬Æ\xa0watsonx\xa0Orchestrate‚Ñ¢\xa0   \\n\\nEasily design scalable AI assistants and agents, automate repetitive tasks and simplify complex processes with\xa0IBM¬Æ\xa0watsonx\xa0Orchestrate‚Ñ¢.\\n\\n   [Explore watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)   \\n Artificial intelligence solutions   \\n\\nPut AI to work in your business with IBM\'s industry-leading AI expertise and portfolio of solutions at your side.\\n\\n   [Explore AI solutions](https://www.ibm.com/artificial-intelligence)   \\n AI consulting and services   \\n\\nReinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.\\n\\n   [Explore AI services](https://www.ibm.com/consulting/artificial-intelligence)\\n\\nTake the next step\\n\\n \\n\\nWhether you choose to customize pre-built apps and skills or build and deploy custom agentic services using an AI studio, the IBM watsonx platform has you covered.\\n\\n    [Explore watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)   [Explore watsonx.ai](https://www.ibm.com/products/watsonx-ai)"}, {"url": "https://docs.langchain.com/oss/python/langgraph/overview", "title": "LangGraph overview - Docs by LangChain", "content": "Trusted by companies shaping the future of agents‚Äî including Klarna, Replit, Elastic, and more‚Äî LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain‚Äôs agents that provide pre-built architectures for common LLM and tool-calling loops. LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more. LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.## LangSmith Agent Server. Contains agent abstractions built on top of LangGraph.", "score": 0.90039873, "raw_content": "[Docs by LangChain home page](/)\\n\\n[Deep Agents](/oss/python/deepagents/overview)[LangChain](/oss/python/langchain/overview)[LangGraph](/oss/python/langgraph/overview)[Integrations](/oss/python/integrations/providers/overview)[Learn](/oss/python/learn)[Reference](/oss/python/reference/overview)[Contribute](/oss/python/contributing/overview)\\n\\n* [Overview](/oss/python/langgraph/overview)\\n\\n##### Get started\\n\\n* [Install](/oss/python/langgraph/install)\\n* [Quickstart](/oss/python/langgraph/quickstart)\\n* [Local server](/oss/python/langgraph/local-server)\\n* [Changelog](https://docs.langchain.com/oss/python/releases/changelog)\\n* [Thinking in LangGraph](/oss/python/langgraph/thinking-in-langgraph)\\n* [Workflows + agents](/oss/python/langgraph/workflows-agents)\\n\\n##### Capabilities\\n\\n* [Persistence](/oss/python/langgraph/persistence)\\n* [Durable execution](/oss/python/langgraph/durable-execution)\\n* [Streaming](/oss/python/langgraph/streaming)\\n* [Interrupts](/oss/python/langgraph/interrupts)\\n* [Time travel](/oss/python/langgraph/use-time-travel)\\n* [Memory](/oss/python/langgraph/add-memory)\\n* [Subgraphs](/oss/python/langgraph/use-subgraphs)\\n\\n##### Production\\n\\n* [Application structure](/oss/python/langgraph/application-structure)\\n* [Test](/oss/python/langgraph/test)\\n* [LangSmith Studio](/oss/python/langgraph/studio)\\n* [Agent Chat UI](/oss/python/langgraph/ui)\\n* [LangSmith Deployment](/oss/python/langgraph/deploy)\\n* [LangSmith Observability](/oss/python/langgraph/observability)\\n\\n##### LangGraph APIs\\n\\n* [Runtime](/oss/python/langgraph/pregel)\\n\\n* [Install](#install)\\n* [Core benefits](#core-benefits)\\n* [LangGraph ecosystem](#langgraph-ecosystem)\\n* [Acknowledgements](#acknowledgements)\\n\\n# LangGraph overview\\n\\nGain control with LangGraph to design agents that reliably handle complex tasks\\n\\nTrusted by companies shaping the future of agents‚Äî including Klarna, Replit, Elastic, and more‚Äî LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/python/langchain/models) and [tools](/oss/python/langchain/tools). We will commonly use [LangChain](/oss/python/langchain/overview) components throughout the documentation to integrate models and tools, but you don‚Äôt need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain‚Äôs [agents](/oss/python/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops. LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\n## [\u200b](#install) Install\\n\\nCopy\\n\\n```\\npip install -U langgraph pip  install -U  langgraph\\n```\\n\\nThen, create a simple hello world example:\\n\\nCopy\\n\\n```\\nfrom langgraph.graph import StateGraph, MessagesState, START, END from langgraph.graph import StateGraph, MessagesState, START, END def mock_llm(state: MessagesState): def  mock_llm(state: MessagesState): return {\\"messages\\": [{\\"role\\": \\"ai\\", \\"content\\": \\"hello world\\"}]}  return {\\"messages\\": [{\\"role\\": \\"ai\\", \\"content\\": \\"hello world\\"}]} graph = StateGraph(MessagesState) graph = StateGraph(MessagesState)graph.add_node(mock_llm)graph.add_node(mock_llm)graph.add_edge(START, \\"mock_llm\\")graph.add_edge(START, \\"mock_llm\\")graph.add_edge(\\"mock_llm\\", END)graph.add_edge(\\"mock_llm\\", END)graph = graph.compile() graph = graph.compile() graph.invoke({\\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": \\"hi!\\"}]})graph.invoke({\\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": \\"hi!\\"}]})\\n```\\n\\n## [\u200b](#core-benefits) Core benefits\\n\\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\n* [Durable execution](/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\n* [Human-in-the-loop](/oss/python/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\\n* [Comprehensive memory](/oss/python/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n## [\u200b](#langgraph-ecosystem) LangGraph ecosystem\\n\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\n\\n[## LangSmith\\n\\nTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.](http://www.langchain.com/langsmith)[## LangSmith Agent Server\\n\\nDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äî and iterate quickly with visual prototyping in Studio.](/langsmith/agent-server)[## LangChain\\n\\nProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.](/oss/python/langchain/overview)\\n\\n## [\u200b](#acknowledgements) Acknowledgements\\n\\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain. \\n\\n---\\n\\n[Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\\n\\n[Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\\n\\nWas this page helpful?\\n\\n[Install LangGraph](/oss/python/langgraph/install)"}, {"url": "https://www.reddit.com/r/AI_Agents/comments/1l4uq7v/why_use_langgraph/", "title": "Why use LangGraph? : r/AI_Agents - Reddit", "content": "LangGraph emphasizes graph-based workflows and state management, making it ideal for complex applications with sophisticated logic and memory persistence.", "score": 0.78314424, "raw_content": null}, {"url": "https://www.langchain.com/langgraph", "title": "LangGraph: Agent Orchestration Framework for Reliable AI Agents", "content": "[![Image 1](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270df09334914882b88_Frame%209.svg) Observability Debug and monitor in-depth traces](https://www.langchain.com/langsmith/observability)[![Image 2](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270f9e8de1d368764a8_Frame%20206.svg) Evaluation Iterate on prompts and models](https://www.langchain.com/langsmith/evaluation)[![Image 3](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e2709eef27fc61465416_Frame%20100039.svg) Deployment Ship and scale agents in production](https://www.langchain.com/langsmith/deployment)[![Image 4](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/692e7522b3ef627cb0ec1155_Centralized%20management.svg) Agent Builder New Build no-code agents](https://www.langchain.com/langsmith/agent-builder). [![Image 5](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e27023585643370a6471_icons.svg) LangChain Quick start agents with any model provider](https://www.langchain.com/langchain)[![Image 6](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270417338c7f027082d_d035ce400e48f9bc4dd0578d0e3e3211_icons-1.svg) LangGraph Build custom agents with low-level control](https://www.langchain.com/langgraph)[![Image 7](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68f20863b71dbae1af829979_DeepAgents.svg) Deep Agents New Use planning, memory, and sub-agents for complex, long-running tasks](https://docs.langchain.com/oss/python/deepagents/overview#deep-agents-overview). [Start building](https://docs.langchain.com/oss/python/langgraph/overview). ![Image 9](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 10](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 11](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 13](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 14](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 16](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 18](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 19](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 20](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 21](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 23](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 24](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 26](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 28](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 29](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 30](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 31](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 33](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 34](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 36](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 38](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 39](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 40](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 41](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 43](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 44](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 46](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 48](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 49](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b080e4b3ca12dc5d5d439_Langgraph%20UI-2.webp). [Read a conceptual guide](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop). ![Image 50](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c93d559216bb904fe85a8_gif7%20(1).gif). [Learn how to add human-in-the-loop](https://docs.langchain.com/oss/python/langgraph/interrupts). ![Image 51](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68663ca715b9bd5d707bee71_Modified-Human-in-the-loop_white.gif). ![Image 52](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b895f061b6f892568ff6_Modified-Customizable-Agent-Architectures_white.gif). [See different agent architectures](https://docs.langchain.com/oss/python/langgraph/workflows-agents). [Learn about agent memory](https://docs.langchain.com/oss/python/langgraph/add-memory). ![Image 54](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b8b7df5aea00af3a4554_Modified-Streaming-intermediate-steps_white.gif). [See how to use streaming](https://docs.langchain.com/oss/python/langgraph/streaming). [![Image 57](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/678e35d6c553c4fb20f9b753_Frame%2099644.webp)![Image 58](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6787ae0bce5c99dd808545ce_card%202.webp)](https://academy.langchain.com/courses/intro-to-langgraph). ![Image 64](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png). ![Image 70](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png). ![Image 71](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c6a38f9c53ec71f5fc73de_langchain-word.svg).", "score": 0.7730283, "raw_content": "LangGraph: Agent Orchestration Framework for Reliable AI Agents\\n===============\\n\\n[](https://www.langchain.com/)\\n\\nProducts\\n\\nLangSmith\\n\\n[![Image 1](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270df09334914882b88_Frame%209.svg) Observability Debug and monitor in-depth traces](https://www.langchain.com/langsmith/observability)[![Image 2](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270f9e8de1d368764a8_Frame%20206.svg) Evaluation Iterate on prompts and models](https://www.langchain.com/langsmith/evaluation)[![Image 3](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e2709eef27fc61465416_Frame%20100039.svg) Deployment Ship and scale agents in production](https://www.langchain.com/langsmith/deployment)[![Image 4](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/692e7522b3ef627cb0ec1155_Centralized%20management.svg) Agent Builder New Build no-code agents](https://www.langchain.com/langsmith/agent-builder)\\n\\nOpen Source Frameworks\\n\\n[![Image 5](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e27023585643370a6471_icons.svg) LangChain Quick start agents with any model provider](https://www.langchain.com/langchain)[![Image 6](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270417338c7f027082d_d035ce400e48f9bc4dd0578d0e3e3211_icons-1.svg) LangGraph Build custom agents with low-level control](https://www.langchain.com/langgraph)[![Image 7](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68f20863b71dbae1af829979_DeepAgents.svg) Deep Agents New Use planning, memory, and sub-agents for complex, long-running tasks](https://docs.langchain.com/oss/python/deepagents/overview#deep-agents-overview)\\n\\nLearn\\n\\nResources\\n\\n[Blog](https://blog.langchain.com/)[2026 State of Agent Engineering](https://www.langchain.com/state-of-agent-engineering)[Customer Stories](https://www.langchain.com/customers)[Guides](https://www.langchain.com/resources)[Changelog](https://changelog.langchain.com/)[Trust Center](https://trust.langchain.com/)[Support](http://support.langchain.com/)\\n\\nHow-To\\n\\n[LangChain Academy](https://academy.langchain.com/)[YouTube](https://www.youtube.com/@LangChain)[Documentation](https://docs.langchain.com/)\\n\\nCommunity\\n\\n[LangSmith for Startups](https://www.langchain.com/startups)[Events](https://luma.com/langchain?k=c)[Community](https://www.langchain.com/community)[Community Forum](https://forum.langchain.com/)[Slack](https://www.langchain.com/join-community)\\n\\n[Docs](https://docs.langchain.com/)\\n\\nCompany\\n\\n[About](https://www.langchain.com/about)[Careers](https://www.langchain.com/careers)[Partners](https://www.langchain.com/langchain-partner-network)\\n\\n[Pricing](https://www.langchain.com/pricing)\\n\\n[Get a demo](https://www.langchain.com/contact-sales)\\n\\n[Try LangSmith](https://smith.langchain.com/)\\n\\nBalance agent control with agency\\n=================================\\n\\nGain control with LangGraph to design agents \\n\\nthat reliably handle complex tasks.\\n\\n[Start building](https://docs.langchain.com/oss/python/langgraph/overview)\\n\\n![Image 8](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66db8c2317fe5b9ad2b84ea0_lcacademylogo.png)\\nIntroduction to LangGraph\\n-------------------------\\n\\nLearn the basics of LangGraph in this LangChain Academy Course. You\'ll learn how to build agents that automate real-world tasks with LangGraph orchestration.\\n\\n[Enroll for free](https://academy.langchain.com/courses/intro-to-langgraph)[Book enterprise training](https://airtable.com/appGjCAN6126Jm7K8/pagNAp7niHQzRH8zk/form)\\n\\nTrusted by companies shaping the future of agents\\n-------------------------------------------------\\n\\n[See LangGraph use cases in production](https://www.langchain.com/built-with-langgraph)\\n\\n![Image 9](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\n\\n![Image 10](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\n\\n![Image 11](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\n\\n![Image 12](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\n\\n![Image 13](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\n\\n![Image 14](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\n\\n![Image 15](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\n\\n![Image 16](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\n\\n![Image 17](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\n\\n![Image 18](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\n\\n![Image 19](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\n\\n![Image 20](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\n\\n![Image 21](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\n\\n![Image 22](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\n\\n![Image 23](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\n\\n![Image 24](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\n\\n![Image 25](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\n\\n![Image 26](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\n\\n![Image 27](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\n\\n![Image 28](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\n\\n![Image 29](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\n\\n![Image 30](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\n\\n![Image 31](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\n\\n![Image 32](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\n\\n![Image 33](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\n\\n![Image 34](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\n\\n![Image 35](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\n\\n![Image 36](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\n\\n![Image 37](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\n\\n![Image 38](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\n\\n![Image 39](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\n\\n![Image 40](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\n\\n![Image 41](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\n\\n![Image 42](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\n\\n![Image 43](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\n\\n![Image 44](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\n\\n![Image 45](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\n\\n![Image 46](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\n\\n![Image 47](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\n\\n![Image 48](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\n\\n![Image 49](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b080e4b3ca12dc5d5d439_Langgraph%20UI-2.webp)\\n\\nControllable cognitive architecture for any task\\n------------------------------------------------\\n\\nLangGraph\'s flexible framework supports diverse control flows ‚Äì single agent, multi-agent, hierarchical, sequential ‚Äì and robustly handles realistic, complex scenarios. \\n\\nEnsure reliability with easy-to-add moderation and quality loops that prevent agents from veering off course.\\n\\nUse LangGraph Platform to templatize your cognitive architecture so that tools, prompts, and models are easily configurable with LangGraph Platform Assistants.\\n\\n[See the docs](https://langchain-ai.github.io/langgraph/)\\n\\nThousands of companies build AI apps better with LangChain products.\\n--------------------------------------------------------------------\\n\\nRead our select customer stories.\\n\\nDesigned for human-agent collaboration\\n--------------------------------------\\n\\nWith built-in statefulness, LangGraph agents seamlessly collaborate with humans by writing drafts for review and awaiting approval before acting. Easily inspect the agent‚Äôs actions and \\"time-travel\\" to roll back and take a different action to correct course.\\n\\n[Read a conceptual guide](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop)\\n\\n![Image 50](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c93d559216bb904fe85a8_gif7%20(1).gif)\\n\\nHow does LangGraph help?\\n------------------------\\n\\nGuide, moderate, and control your agent with human-in-the-loop\\n--------------------------------------------------------------\\n\\nPrevent agents from veering off course with easy-to-add moderation and quality controls. Add human-in-the-loop checks to steer and approve agent actions.\\n\\n[Learn how to add human-in-the-loop](https://docs.langchain.com/oss/python/langgraph/interrupts)\\n\\n![Image 51](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68663ca715b9bd5d707bee71_Modified-Human-in-the-loop_white.gif)\\n\\n![Image 52](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b895f061b6f892568ff6_Modified-Customizable-Agent-Architectures_white.gif)\\n\\nBuild expressive, customizable agent workflows\\n----------------------------------------------\\n\\nLangGraph‚Äôs low-level primitives provide the flexibility needed to create fully customizable agents. Design diverse control flows ‚Äî single, multi-agent, hierarchical ‚Äî all using one framework.\\n\\n[See different agent architectures](https://docs.langchain.com/oss/python/langgraph/workflows-agents)\\n\\nPersist context for long-term interactions\\n------------------------------------------\\n\\nLangGraph‚Äôs built-in memory stores conversation histories and maintains context over time, enabling rich, personalized interactions across sessions.\\n\\n[Learn about agent memory](https://docs.langchain.com/oss/python/langgraph/add-memory)\\n\\n![Image 53](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/684b39acf796422a803c3a03_Memory-GIF-edited.gif)\\n\\n![Image 54](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b8b7df5aea00af3a4554_Modified-Streaming-intermediate-steps_white.gif)\\n\\nFirst-class streaming for better UX design\\n------------------------------------------\\n\\nBridge user expectations and agent capabilities with native token-by-token streaming, showing agent reasoning and actions in real time.\\n\\n[See how to use streaming](https://docs.langchain.com/oss/python/langgraph/streaming)\\n\\n![Image 55](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c57f274b66a77e2a26b82_CleanShot2024-06-26at17.08.03-ezgif.com-video-to-gif-converter.gif)\\n\\nFirst class streaming support for better UX design\\n--------------------------------------------------\\n\\nBridge user expectations and agent capabilities with native token-by-token streaming and streaming of intermediate steps, helpful for showing agent reasoning and actions back to the user as they happen. Use LangGraph Platform\'s API to deliver dynamic and interactive user experiences.\\n\\n[Learn more](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/)\\n\\n![Image 56](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66db8c2317fe5b9ad2b84ea0_lcacademylogo.png)\\n\\nIntroduction to LangGraph\\n-------------------------\\n\\nLearn the basics of LangGraph in this LangChain Academy Course. You\'ll learn how to build agents that automate real-world tasks with LangGraph orchestration.\\n\\n[Enroll for free](https://academy.langchain.com/courses/intro-to-langgraph)\\n\\n[![Image 57](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/678e35d6c553c4fb20f9b753_Frame%2099644.webp)![Image 58](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6787ae0bce5c99dd808545ce_card%202.webp)](https://academy.langchain.com/courses/intro-to-langgraph)\\n\\nDeploy agents at scale, monitor carefully, iterate boldly\\n---------------------------------------------------------\\n\\nDesign agent-driven user experiences with LangGraph Platform\'s APIs. Quickly deploy and scale your application with infrastructure built for agents. Choose from multiple deployment options.\\n\\n### Fault-tolerant scalability\\n\\nHandle large workloads gracefully with horizontally-scaling servers, task queues, and built-in persistence. Enhance resilience with intelligent caching and automated retries.\\n\\n### Dynamic APIs for designing agent experience\\n\\nCraft personalized user experiences with APIs featuring long-term memory to recall information across conversation sessions. Track, update, and rewind your app\'s state for easy human steering and interaction. Kick off long-running background jobs for research-style or multi-step work.\\n\\n### Integrated developer experience\\n\\nSimplify prototyping, debugging, and sharing of agents in our visual LangGraph Studio. Deploy your application with 1-click deploy with our SaaS offering or within your own VPC. Then, monitor app performance with LangSmith.\\n\\n### Developers trust LangGraph to build reliable agents\\n\\nLangGraph helps teams of all sizes, across all industries, build reliable agents ready for production.\\n\\n[Hear how industry leaders use LangGraph](https://www.langchain.com/built-with-langgraph)\\n\\n![Image 59](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308aea1371b447cc4af9_elastic-ar21.png)\\n\\n‚ÄúLangChain is streets ahead with what they\'ve put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads ‚Äî from conversational agents, complex task automation, to custom LLM-backed experiences that \'just work\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.‚Äù\\n\\n![Image 60](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b26a1b4576291d6a9335b_garrett%20spong%201.webp)\\n\\nGarrett Spong\\n\\nPrincipal SWE \\n\\n![Image 61](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679de9dc4e7bee218d4b058_Norwegian-Cruise-Line-Logo%202-2.webp)\\n\\n‚ÄúLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent\'s thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.‚Äù\\n\\n![Image 62](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b265bed5f5a9d26d6b7d6_andres%20torres%201.webp)\\n\\nAndres Torres\\n\\nSr. Solutions Architect\\n\\n![Image 63](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e1baf7ea357d0763cde1_ally-bank%201-2.png)\\n\\n‚ÄúAs Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments with multi-actor agentic workflows. We are committed to deepening our partnership with LangChain.‚Äù\\n\\n‚ÄúAs Ally advances its exploration of Generative AI,\\n\\n![Image 64](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png)\\n\\nSathish Muthukrishnan\\n\\nChief Information, Data and Digital Officer\\n\\n![Image 65](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308aea1371b447cc4af9_elastic-ar21.png)\\n\\n‚ÄúLangChain is streets ahead with what they\'ve put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads ‚Äî from conversational agents, complex task automation, to custom LLM-backed experiences that \'just work\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.‚Äù\\n\\n![Image 66](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b26a1b4576291d6a9335b_garrett%20spong%201.webp)\\n\\nGarrett Spong\\n\\nPrincipal SWE \\n\\n![Image 67](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679de9dc4e7bee218d4b058_Norwegian-Cruise-Line-Logo%202-2.webp)\\n\\n‚ÄúLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent\'s thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.‚Äù\\n\\n![Image 68](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b265bed5f5a9d26d6b7d6_andres%20torres%201.webp)\\n\\nAndres Torres\\n\\nSr. Solutions Architect\\n\\n![Image 69](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e1baf7ea357d0763cde1_ally-bank%201-2.png)\\n\\n‚ÄúAs Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments with multi-actor agentic workflows. We are committed to deepening our partnership with LangChain.‚Äù\\n\\n‚ÄúAs Ally advances its exploration of Generative AI,\\n\\n![Image 70](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png)\\n\\nSathish Muthukrishnan\\n\\nChief Information, Data and Digital Officer\\n\\nLangGraph FAQs\\n--------------\\n\\nHow is LangGraph different from other agent frameworks?\\n\\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company‚Äôs needs. LangGraph provides a more expressive framework to handle companies‚Äô unique tasks without restricting users to a single black-box cognitive architecture.\\n\\nDoes LangGraph impact the performance of my app?\\n\\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\\n\\nIs LangGraph open source? Is it free?\\n\\nYes. LangGraph is an MIT-licensed open-source library and is free to use.\\n\\nReady to start shipping reliable agents faster?\\n-----------------------------------------------\\n\\nGet started with tools from the LangChain product suite for every step of the agent development lifecycle.\\n\\n[Talk to sales](https://www.langchain.com/contact-sales)[Sign Up](https://smith.langchain.com/)\\n\\nProducts\\n\\n[LangChain](https://www.langchain.com/langchain)[LangGraph](https://www.langchain.com/langgraph)[LangSmith Observability](https://www.langchain.com/langsmith/observability)[LangSmith Evaluation](https://www.langchain.com/langsmith/evaluation)[LangSmith Deployment](https://www.langchain.com/langsmith/deployment)\\n\\nResources\\n\\n[Guides](https://www.langchain.com/resources)[Blog](https://blog.langchain.com/)[Customer Stories](https://www.langchain.com/customers)[LangChain Academy](https://academy.langchain.com/)[Community](https://www.langchain.com/join-community)[Events](https://lu.ma/langchain)[Changelog](https://changelog.langchain.com/)[Docs](http://docs.langchain.com/)[Support](https://support.langchain.com/)\\n\\nCompany\\n\\n[About](https://www.langchain.com/about)[Careers](https://www.langchain.com/careers)[X](https://twitter.com/LangChain)[LinkedIn](https://www.linkedin.com/company/langchain/)[YouTube](https://www.youtube.com/@LangChain)[Marketing Assets](https://drive.google.com/drive/folders/17xybjzmVBdsQA-VxouuGLxF6bDsHDe80?usp=sharing)[Security](https://trust.langchain.com/)\\n\\nSign up for our newsletter to stay up to date\\n\\nThank you! Your submission has been received!\\n\\nOops! Something went wrong while submitting the form.\\n\\n![Image 71](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c6a38f9c53ec71f5fc73de_langchain-word.svg)\\n\\n[All systems operational](https://status.smith.langchain.com/)\\n\\n[Responsible Disclosure Policy](https://www.langchain.com/responsible-disclosure-policy)[Privacy Policy](https://www.langchain.com/privacy-policy)[Terms of Service](https://www.langchain.com/terms-of-service)"}, {"url": "https://github.com/langchain-ai/langgraph", "title": "langchain-ai/langgraph: Build resilient language agents as graphs.", "content": "[Skip to content](https://github.com/langchain-ai/langgraph#start-of-content). [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.Dismiss alert. [MIT license](https://github.com/langchain-ai/langgraph/blob/main/LICENSE). *   [Code](https://github.com/langchain-ai/langgraph). *   [Issues 179](https://github.com/langchain-ai/langgraph/issues). *   [Pull requests 126](https://github.com/langchain-ai/langgraph/pulls). *   [Actions](https://github.com/langchain-ai/langgraph/actions). *   [Projects 0](https://github.com/langchain-ai/langgraph/projects). *   [Security 3](https://github.com/langchain-ai/langgraph/security). *   [Insights](https://github.com/langchain-ai/langgraph/pulse). *   [Code](https://github.com/langchain-ai/langgraph). *   [Issues](https://github.com/langchain-ai/langgraph/issues). *   [Pull requests](https://github.com/langchain-ai/langgraph/pulls). *   [Actions](https://github.com/langchain-ai/langgraph/actions). *   [Projects](https://github.com/langchain-ai/langgraph/projects). *   [Security](https://github.com/langchain-ai/langgraph/security). *   [Insights](https://github.com/langchain-ai/langgraph/pulse). [](https://github.com/langchain-ai/langgraph/branches)[](https://github.com/langchain-ai/langgraph/tags). Please comment here if you encounter any issues\\")[#6720](https://github.com/langchain-ai/langgraph/pull/6720) | Jan 26, 2026 |. *   [README](https://github.com/langchain-ai/langgraph#). *   [Code of conduct](https://github.com/langchain-ai/langgraph#). *   [Contributing](https://github.com/langchain-ai/langgraph#). *   [MIT license](https://github.com/langchain-ai/langgraph#). *   [Security](https://github.com/langchain-ai/langgraph#). [](https://github.com/langchain-ai/langgraph#get-started). [](https://github.com/langchain-ai/langgraph#core-benefits). [](https://github.com/langchain-ai/langgraph#langgraphs-ecosystem). See the [JS repo](https://github.com/langchain-ai/langgraphjs) and the [JS docs](https://docs.langchain.com/oss/javascript/langgraph/overview). [](https://github.com/langchain-ai/langgraph#additional-resources). [](https://github.com/langchain-ai/langgraph#acknowledgements). [Readme](https://github.com/langchain-ai/langgraph#readme-ov-file). [MIT license](https://github.com/langchain-ai/langgraph#MIT-1-ov-file). [Code of conduct](https://github.com/langchain-ai/langgraph#coc-ov-file). [Contributing](https://github.com/langchain-ai/langgraph#contributing-ov-file). [Security policy](https://github.com/langchain-ai/langgraph#security-ov-file). [Please reload this page](https://github.com/langchain-ai/langgraph). [Activity](https://github.com/langchain-ai/langgraph/activity). [Custom properties](https://github.com/langchain-ai/langgraph/custom-properties). [**24.8k** stars](https://github.com/langchain-ai/langgraph/stargazers). [**142** watching](https://github.com/langchain-ai/langgraph/watchers). [**4.3k** forks](https://github.com/langchain-ai/langgraph/forks). [Releases 466](https://github.com/langchain-ai/langgraph/releases). [langgraph==1.0.9 Latest Feb 19, 2026](https://github.com/langchain-ai/langgraph/releases/tag/1.0.9). [+ 465 releases](https://github.com/langchain-ai/langgraph/releases). [Used by 36k](https://github.com/langchain-ai/langgraph/network/dependents). [Contributors 286](https://github.com/langchain-ai/langgraph/graphs/contributors). [+ 272 contributors](https://github.com/langchain-ai/langgraph/graphs/contributors). *   [Python 99.3%](https://github.com/langchain-ai/langgraph/search?l=python).", "score": 0.65146625, "raw_content": "GitHub - langchain-ai/langgraph: Build resilient language agents as graphs.\\n===============\\n\\n[Skip to content](https://github.com/langchain-ai/langgraph#start-of-content)\\nNavigation Menu\\n---------------\\n\\nToggle navigation\\n\\n[](https://github.com/)\\n\\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flanggraph)\\n\\nAppearance settings\\n\\n*   \\nPlatform\\n\\n    *   \\nAI CODE CREATION\\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\\n\\n    *   \\nDEVELOPER WORKFLOWS\\n        *   [Actions Automate any workflow](https://github.com/features/actions)\\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\\n        *   [Issues Plan and track work](https://github.com/features/issues)\\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\\n\\n    *   \\nAPPLICATION SECURITY\\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\\n\\n    *   \\nEXPLORE\\n        *   [Why GitHub](https://github.com/why-github)\\n        *   [Documentation](https://docs.github.com/)\\n        *   [Blog](https://github.blog/)\\n        *   [Changelog](https://github.blog/changelog)\\n        *   [Marketplace](https://github.com/marketplace)\\n\\n[View all features](https://github.com/features)\\n\\n*   \\nSolutions\\n\\n    *   \\nBY COMPANY SIZE\\n        *   [Enterprises](https://github.com/enterprise)\\n        *   [Small and medium teams](https://github.com/team)\\n        *   [Startups](https://github.com/enterprise/startups)\\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\\n\\n    *   \\nBY USE CASE\\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\\n        *   [DevOps](https://github.com/solutions/use-case/devops)\\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\\n        *   [View all use cases](https://github.com/solutions/use-case)\\n\\n    *   \\nBY INDUSTRY\\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\\n        *   [Government](https://github.com/solutions/industry/government)\\n        *   [View all industries](https://github.com/solutions/industry)\\n\\n[View all solutions](https://github.com/solutions)\\n\\n*   \\nResources\\n\\n    *   \\nEXPLORE BY TOPIC\\n        *   [AI](https://github.com/resources/articles?topic=ai)\\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\\n        *   [Security](https://github.com/resources/articles?topic=security)\\n        *   [View all topics](https://github.com/resources/articles)\\n\\n    *   \\nEXPLORE BY TYPE\\n        *   [Customer stories](https://github.com/customer-stories)\\n        *   [Events &amp; webinars](https://github.com/resources/events)\\n        *   [Ebooks &amp; reports](https://github.com/resources/whitepapers)\\n        *   [Business insights](https://github.com/solutions/executive-insights)\\n        *   [GitHub Skills](https://skills.github.com/)\\n\\n    *   \\nSUPPORT &amp; SERVICES\\n        *   [Documentation](https://docs.github.com/)\\n        *   [Customer support](https://support.github.com/)\\n        *   [Community forum](https://github.com/orgs/community/discussions)\\n        *   [Trust center](https://github.com/trust-center)\\n        *   [Partners](https://github.com/partners)\\n\\n*   \\nOpen Source\\n\\n    *   \\nCOMMUNITY\\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\\n\\n    *   \\nPROGRAMS\\n        *   [Security Lab](https://securitylab.github.com/)\\n        *   [Maintainer Community](https://maintainers.github.com/)\\n        *   [Accelerator](https://github.com/accelerator)\\n        *   [Archive Program](https://archiveprogram.github.com/)\\n\\n    *   \\nREPOSITORIES\\n        *   [Topics](https://github.com/topics)\\n        *   [Trending](https://github.com/trending)\\n        *   [Collections](https://github.com/collections)\\n\\n*   \\nEnterprise\\n\\n    *   \\nENTERPRISE SOLUTIONS\\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\\n\\n    *   \\nAVAILABLE ADD-ONS\\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\\n\\n*   [Pricing](https://github.com/pricing)\\n\\nSearch or jump to...\\n\\nSearch code, repositories, users, issues, pull requests...\\n==========================================================\\n\\n Search  \\n\\nClear\\n\\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\\n\\nProvide feedback\\n================\\n\\nWe read every piece of feedback, and take your input very seriously.\\n\\n- [x] Include my email address so I can be contacted \\n\\n Cancel  Submit feedback \\n\\nSaved searches\\n==============\\n\\nUse saved searches to filter your results more quickly\\n------------------------------------------------------\\n\\nName \\n\\nQuery \\n\\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\\n\\n Cancel  Create saved search \\n\\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flanggraph)\\n\\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=langchain-ai%2Flanggraph)\\n\\nAppearance settings\\n\\nResetting focus\\n\\nYou signed in with another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.Dismiss alert\\n\\n{{ message }}\\n\\n[langchain-ai](https://github.com/langchain-ai)/**[langgraph](https://github.com/langchain-ai/langgraph)**Public\\n\\n*   [Notifications](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)You must be signed in to change notification settings\\n*   [Fork 4.3k](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)\\n*   [Star 24.8k](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph) \\n\\nBuild resilient language agents as graphs.\\n\\n[docs.langchain.com/oss/python/langgraph/](https://docs.langchain.com/oss/python/langgraph/ \\"https://docs.langchain.com/oss/python/langgraph/\\")\\n\\n### License\\n\\n[MIT license](https://github.com/langchain-ai/langgraph/blob/main/LICENSE)\\n\\n[24.8k stars](https://github.com/langchain-ai/langgraph/stargazers)[4.3k forks](https://github.com/langchain-ai/langgraph/forks)[Branches](https://github.com/langchain-ai/langgraph/branches)[Tags](https://github.com/langchain-ai/langgraph/tags)[Activity](https://github.com/langchain-ai/langgraph/activity)\\n\\n[Star](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)\\n\\n[Notifications](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)You must be signed in to change notification settings\\n\\n*   [Code](https://github.com/langchain-ai/langgraph)\\n*   [Issues 179](https://github.com/langchain-ai/langgraph/issues)\\n*   [Pull requests 126](https://github.com/langchain-ai/langgraph/pulls)\\n*   [Actions](https://github.com/langchain-ai/langgraph/actions)\\n*   [Projects 0](https://github.com/langchain-ai/langgraph/projects)\\n*   [Security 3](https://github.com/langchain-ai/langgraph/security)\\n*   [Insights](https://github.com/langchain-ai/langgraph/pulse)\\n\\nAdditional navigation options\\n\\n*   [Code](https://github.com/langchain-ai/langgraph)\\n*   [Issues](https://github.com/langchain-ai/langgraph/issues)\\n*   [Pull requests](https://github.com/langchain-ai/langgraph/pulls)\\n*   [Actions](https://github.com/langchain-ai/langgraph/actions)\\n*   [Projects](https://github.com/langchain-ai/langgraph/projects)\\n*   [Security](https://github.com/langchain-ai/langgraph/security)\\n*   [Insights](https://github.com/langchain-ai/langgraph/pulse)\\n\\nlangchain-ai/langgraph\\n======================\\n\\nmain\\n\\n[**206**Branches](https://github.com/langchain-ai/langgraph/branches)[**477**Tags](https://github.com/langchain-ai/langgraph/tags)\\n\\n[](https://github.com/langchain-ai/langgraph/branches)[](https://github.com/langchain-ai/langgraph/tags)\\n\\nGo to file\\n\\nCode\\n\\nOpen more actions menu\\n\\nFolders and files\\n-----------------\\n\\n| Name | Name | Last commit message | Last commit date |\\n| --- | --- | --- | --- |\\n| Latest commit ------------- ![Image 1: jkennedyvz](https://avatars.githubusercontent.com/u/65985482?v=4&amp;size=40)![Image 2: claude](https://avatars.githubusercontent.com/u/81847?v=4&amp;size=40) [John Kennedy (jkennedyvz)](https://github.com/langchain-ai/langgraph/commits?author=jkennedyvz) and [Claude (claude)](https://github.com/langchain-ai/langgraph/commits?author=claude) [fix: bump js-yaml to 3.14.2 to resolve](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9)[CVE-2025-64718](https://github.com/advisories/GHSA-mh29-5h37-fv8m \\"CVE-2025-64718\\")[(](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9)[#6879](https://github.com/langchain-ai/langgraph/pull/6879)[)](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9) Open commit details success Feb 19, 2026 [ea20432](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9)¬∑Feb 19, 2026 History ------- [6,521 Commits](https://github.com/langchain-ai/langgraph/commits/main/) Open commit details [](https://github.com/langchain-ai/langgraph/commits/main/)6,521 Commits |\\n| [.github](https://github.com/langchain-ai/langgraph/tree/main/.github \\".github\\") | [.github](https://github.com/langchain-ai/langgraph/tree/main/.github \\".github\\") | [chore(docs): Add new redirects file(s) so we can update/add (](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\")[#6778](https://github.com/langchain-ai/langgraph/pull/6778)[)](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\") | Feb 18, 2026 |\\n| [docs](https://github.com/langchain-ai/langgraph/tree/main/docs \\"docs\\") | [docs](https://github.com/langchain-ai/langgraph/tree/main/docs \\"docs\\") | [chore(docs): Add new redirects file(s) so we can update/add (](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\")[#6778](https://github.com/langchain-ai/langgraph/pull/6778)[)](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\") | Feb 18, 2026 |\\n| [examples](https://github.com/langchain-ai/langgraph/tree/main/examples \\"examples\\") | [examples](https://github.com/langchain-ai/langgraph/tree/main/examples \\"examples\\") | [docs: update notebook links and add archival notices for examples (](https://github.com/langchain-ai/langgraph/commit/fbcb8a911b626b4373fd4aff9624124e9532987f \\"docs: update notebook links and add archival notices for examples (#6720) Addresses some comments in #6682 - Update links in notebooks to point to the new documentation location. - Add archival notices indicating that the examples are no longer updated. - Remove some obsolete notebooks that have been moved to the new documentation. Please comment here if you encounter any issues\\")[#6720](https://github.com/langchain-ai/langgraph/pull/6720) | Jan 26, 2026 |\\n| [libs](https://github.com/langchain-ai/langgraph/tree/main/libs \\"libs\\") | [libs](https://github.com/langchain-ai/langgraph/tree/main/libs \\"libs\\") | [fix: bump js-yaml to 3.14.2 to resolve](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9 \\"fix: bump js-yaml to 3.14.2 to resolve CVE-2025-64718 (#6879) ## Security Alert Patch Resolves 1 Dependabot security alert (medium severity). ### Package Updated | Package | Old Version | New Version | Strategy | CVE Resolved | |---------|-------------|-------------|----------|--------------| | `js-yaml` | 3.14.1 | 3.14.2 | Lockfile patch (within-range bump) | CVE-2025-64718 | ### CVE Details **CVE-2025-64718** / [GHSA-mh29-5h37-fv8m](https://github.com/advisories/GHSA-mh29-5h37-fv8m) ‚Äî `js-yaml` prototype pollution via YAML merge keys (`&lt;&lt;`). Affects versions &lt; 3.14.2. The vulnerable package is a transitive dev dependency pulled in by `@istanbuljs/load-nyc-config@1.1.0` (a Jest internal). No runtime impact. ### Fix Strategy Lockfile-only patch in `libs/cli/js-examples/yarn.lock`. The `^3.13.1` version range already allows 3.14.2, so no manifest changes were needed. The `js-yaml@^4.1.1` entry (used by `@eslint/eslintrc`) is untouched. ### Verification - [x] Lockfile updated ‚Äî `js-yaml@^3.13.1` now resolves to `3.14.2` - [x] `js-yaml@^4.1.1` entry unchanged (`4.1.1`) - [x] `yarn install --frozen-lockfile` passes ü§ñ Submitted by langster-patch Co-authored-by: Claude Sonnet 4.6 &lt;noreply@anthropic.com&gt;\\")[CVE-2025-64718](https://github.com/advisories/GHSA-mh29-5h37-fv8m \\"CVE-2025-64718\\")[(](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9 \\"fix: bump js-yaml to 3.14.2 to resolve CVE-2025-64718 (#6879) ## Security Alert Patch Resolves 1 Dependabot security alert (medium severity). ### Package Updated | Package | Old Version | New Version | Strategy | CVE Resolved | |---------|-------------|-------------|----------|--------------| | `js-yaml` | 3.14.1 | 3.14.2 | Lockfile patch (within-range bump) | CVE-2025-64718 | ### CVE Details **CVE-2025-64718** / [GHSA-mh29-5h37-fv8m](https://github.com/advisories/GHSA-mh29-5h37-fv8m) ‚Äî `js-yaml` prototype pollution via YAML merge keys (`&lt;&lt;`). Affects versions &lt; 3.14.2. The vulnerable package is a transitive dev dependency pulled in by `@istanbuljs/load-nyc-config@1.1.0` (a Jest internal). No runtime impact. ### Fix Strategy Lockfile-only patch in `libs/cli/js-examples/yarn.lock`. The `^3.13.1` version range already allows 3.14.2, so no manifest changes were needed. The `js-yaml@^4.1.1` entry (used by `@eslint/eslintrc`) is untouched. ### Verification - [x] Lockfile updated ‚Äî `js-yaml@^3.13.1` now resolves to `3.14.2` - [x] `js-yaml@^4.1.1` entry unchanged (`4.1.1`) - [x] `yarn install --frozen-lockfile` passes ü§ñ Submitted by langster-patch Co-authored-by: Claude Sonnet 4.6 &lt;noreply@anthropic.com&gt;\\")[#6879](https://github.com/langchain-ai/langgraph/pull/6879)[)](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9 \\"fix: bump js-yaml to 3.14.2 to resolve CVE-2025-64718 (#6879) ## Security Alert Patch Resolves 1 Dependabot security alert (medium severity). ### Package Updated | Package | Old Version | New Version | Strategy | CVE Resolved | |---------|-------------|-------------|----------|--------------| | `js-yaml` | 3.14.1 | 3.14.2 | Lockfile patch (within-range bump) | CVE-2025-64718 | ### CVE Details **CVE-2025-64718** / [GHSA-mh29-5h37-fv8m](https://github.com/advisories/GHSA-mh29-5h37-fv8m) ‚Äî `js-yaml` prototype pollution via YAML merge keys (`&lt;&lt;`). Affects versions &lt; 3.14.2. The vulnerable package is a transitive dev dependency pulled in by `@istanbuljs/load-nyc-config@1.1.0` (a Jest internal). No runtime impact. ### Fix Strategy Lockfile-only patch in `libs/cli/js-examples/yarn.lock`. The `^3.13.1` version range already allows 3.14.2, so no manifest changes were needed. The `js-yaml@^4.1.1` entry (used by `@eslint/eslintrc`) is untouched. ### Verification - [x] Lockfile updated ‚Äî `js-yaml@^3.13.1` now resolves to `3.14.2` - [x] `js-yaml@^4.1.1` entry unchanged (`4.1.1`) - [x] `yarn install --frozen-lockfile` passes ü§ñ Submitted by langster-patch Co-authored-by: Claude Sonnet 4.6 &lt;noreply@anthropic.com&gt;\\") | Feb 19, 2026 |\\n| [.gitignore](https://github.com/langchain-ai/langgraph/blob/main/.gitignore \\".gitignore\\") | [.gitignore](https://github.com/langchain-ai/langgraph/blob/main/.gitignore \\".gitignore\\") | [chore: more cleanup (](https://github.com/langchain-ai/langgraph/commit/d066a321bc5a6ca08c2532873a2759cdce1c4d0a \\"chore: more cleanup (#6667)\\")[#6667](https://github.com/langchain-ai/langgraph/pull/6667)[)](https://github.com/langchain-ai/langgraph/commit/d066a321bc5a6ca08c2532873a2759cdce1c4d0a \\"chore: more cleanup (#6667)\\") | Jan 9, 2026 |\\n| [AGENTS.md](https://github.com/langchain-ai/langgraph/blob/main/AGENTS.md \\"AGENTS.md\\") | [AGENTS.md](https://github.com/langchain-ai/langgraph/blob/main/AGENTS.md \\"AGENTS.md\\") | [chore(infra): update `AGENTS.md` for inline code formatting guidelines (](https://github.com/langchain-ai/langgraph/commit/f688b068e7f06111fc69ee70adc1b66c75fa5a93 \\"chore(infra): update `AGENTS.md` for inline code formatting guidelines (#6752)\\") | Feb 5, 2026 |\\n| [CLAUDE.md](https://github.com/langchain-ai/langgraph/blob/main/CLAUDE.md \\"CLAUDE.md\\") | [CLAUDE.md](https://github.com/langchain-ai/langgraph/blob/main/CLAUDE.md \\"CLAUDE.md\\") | [chore(infra): update `AGENTS.md` for inline code formatting guidelines (](https://github.com/langchain-ai/langgraph/commit/f688b068e7f06111fc69ee70adc1b66c75fa5a93 \\"chore(infra): update `AGENTS.md` for inline code formatting guidelines (#6752)\\") | Feb 5, 2026 |\\n| [LICENSE](https://github.com/langchain-ai/langgraph/blob/main/LICENSE \\"LICENSE\\") | [LICENSE](https://github.com/langchain-ai/langgraph/blob/main/LICENSE \\"LICENSE\\") | [libs: add cli, sdk-py, sdk-js and move core langgraph](https://github.com/langchain-ai/langgraph/commit/1a06d500d4282cfdb2ae9d7748bb570e8162acdf \\"libs: add cli, sdk-py, sdk-js and move core langgraph\\") | Jun 18, 2024 |\\n| [Makefile](https://github.com/langchain-ai/langgraph/blob/main/Makefile \\"Makefile\\") | [Makefile](https://github.com/langchain-ai/langgraph/blob/main/Makefile \\"Makefile\\") | [ci: add automated](https://github.com/langchain-ai/langgraph/commit/b3708bd7f66675f7a04bf9950c33d495c49cd843 \\"ci: add automated `uv lock --upgrade` workflow (#5307)\\")`uv lock --upgrade`[workflow (](https://github.com/langchain-ai/langgraph/commit/b3708bd7f66675f7a04bf9950c33d495c49cd843 \\"ci: add automated `uv lock --upgrade` workflow (#5307)\\")[#5307](https://github.com/langchain-ai/langgraph/pull/5307)[)](https://github.com/langchain-ai/langgraph/commit/b3708bd7f66675f7a04bf9950c33d495c49cd843 \\"ci: add automated `uv lock --upgrade` workflow (#5307)\\") | Jul 2, 2025 |\\n| [README.md](https://github.com/langchain-ai/langgraph/blob/main/README.md \\"README.md\\") | [README.md](https://github.com/langchain-ai/langgraph/blob/main/README.md \\"README.md\\") | [chore(docs): Update link for LangGraph guides in README (](https://github.com/langchain-ai/langgraph/commit/8cb87eaf7637ca591adc9715fc412e6661e4a77d \\"chore(docs): Update link for LangGraph guides in README (#6680) https://langchain.slack.com/archives/C04GWPE38LV/p1768251150861949\\")[#6680](https://github.com/langchain-ai/langgraph/pull/6680)[)](https://github.com/langchain-ai/langgraph/commit/8cb87eaf7637ca591adc9715fc412e6661e4a77d \\"chore(docs): Update link for LangGraph guides in README (#6680) https://langchain.slack.com/archives/C04GWPE38LV/p1768251150861949\\") | Jan 13, 2026 |\\n| View all files |\\n\\nRepository files navigation\\n---------------------------\\n\\n*   [README](https://github.com/langchain-ai/langgraph#)\\n*   [Code of conduct](https://github.com/langchain-ai/langgraph#)\\n*   [Contributing](https://github.com/langchain-ai/langgraph#)\\n*   [MIT license](https://github.com/langchain-ai/langgraph#)\\n*   [Security](https://github.com/langchain-ai/langgraph#)\\n\\n![Image 3: LangGraph Logo](https://camo.githubusercontent.com/35c8690644d21b455613e70f617eee193be16e02684824dc60be8ab1216eed6c/68747470733a2f2f6c616e67636861696e2d61692e6769746875622e696f2f6c616e6767726170682f7374617469632f776f72646d61726b5f6461726b2e737667)\\n\\n[![Image 4: Version](https://camo.githubusercontent.com/28715b4724dc05f3ffb3a0cda4069876d0e8ada606267c944bc83639b3ebe5ff/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c616e6767726170682e737667)](https://pypi.org/project/langgraph/)[![Image 5: Downloads](https://camo.githubusercontent.com/81ef74dbbdd86537708ac52c5757df06b12129ea96d7aa92254c8276fee2b3fa/68747470733a2f2f7374617469632e706570792e746563682f62616467652f6c616e6767726170682f6d6f6e7468)](https://pepy.tech/project/langgraph)[![Image 6: Open Issues](https://camo.githubusercontent.com/bcb848d635a6a73f5c1c2be97875ac1369539d5a025be181e6d1f6125933b2d1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6c616e67636861696e2d61692f6c616e676772617068)](https://github.com/langchain-ai/langgraph/issues)[![Image 7: Docs](https://camo.githubusercontent.com/b98c4ce4549448d09f2217965c7d6f2cf39ee6800b2b4c63dfd62080fb5533d8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c7565)](https://docs.langchain.com/oss/python/langgraph/overview)\\n\\nTrusted by companies shaping the future of agents ‚Äì including Klarna, Replit, Elastic, and more ‚Äì LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents.\\n\\nGet started\\n-----------\\n\\n[](https://github.com/langchain-ai/langgraph#get-started)\\n\\nInstall LangGraph:\\n\\n```\\npip install -U langgraph\\n```\\n\\nCreate a simple workflow:\\n\\nundefinedpython\\nfrom langgraph.graph import START, StateGraph\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    text: str\\n\\ndef node_a(state: State) -&gt; dict:\\n    return {\\"text\\": state[\\"text\\"] + \\"a\\"}\\n\\ndef node_b(state: State) -&gt; dict:\\n    return {\\"text\\": state[\\"text\\"] + \\"b\\"}\\n\\ngraph = StateGraph(State)\\ngraph.add_node(\\"node_a\\", node_a)\\ngraph.add_node(\\"node_b\\", node_b)\\ngraph.add_edge(START, \\"node_a\\")\\ngraph.add_edge(\\"node_a\\", \\"node_b\\")\\n\\nprint(graph.compile().invoke({\\"text\\": \\"\\"}))\\n# {\'text\': \'ab\'}\\nundefined\\n\\nGet started with the [LangGraph Quickstart](https://docs.langchain.com/oss/python/langgraph/quickstart).\\n\\nTo quickly build agents with LangChain\'s `create_agent` (built on LangGraph), see the [LangChain Agents documentation](https://docs.langchain.com/oss/python/langchain/agents).\\n\\nCore benefits\\n-------------\\n\\n[](https://github.com/langchain-ai/langgraph#core-benefits)\\n\\nLangGraph provides low-level supporting infrastructure for _any_ long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\n*   [Durable execution](https://docs.langchain.com/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, automatically resuming from exactly where they left off.\\n*   [Human-in-the-loop](https://docs.langchain.com/oss/python/langgraph/interrupts): Seamlessly incorporate human oversight by inspecting and modifying agent state at any point during execution.\\n*   [Comprehensive memory](https://docs.langchain.com/oss/python/langgraph/memory): Create truly stateful agents with both short-term working memory for ongoing reasoning and long-term persistent memory across sessions.\\n*   [Debugging with LangSmith](http://www.langchain.com/langsmith): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\n*   [Production-ready deployment](https://docs.langchain.com/langsmith/app-development): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\nLangGraph‚Äôs ecosystem\\n---------------------\\n\\n[](https://github.com/langchain-ai/langgraph#langgraphs-ecosystem)\\n\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\n\\n*   [LangSmith](http://www.langchain.com/langsmith) ‚Äî Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\n*   [LangSmith Deployment](https://docs.langchain.com/langsmith/deployments) ‚Äî Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äî and iterate quickly with visual prototyping in [LangGraph Studio](https://docs.langchain.com/oss/python/langgraph/studio).\\n*   [LangChain](https://docs.langchain.com/oss/python/langchain/overview) ‚Äì Provides integrations and composable components to streamline LLM application development.\\n\\nNote\\n\\nLooking for the JS version of LangGraph? See the [JS repo](https://github.com/langchain-ai/langgraphjs) and the [JS docs](https://docs.langchain.com/oss/javascript/langgraph/overview).\\n\\nAdditional resources\\n--------------------\\n\\n[](https://github.com/langchain-ai/langgraph#additional-resources)\\n\\n*   [Guides](https://docs.langchain.com/oss/python/langgraph/overview): Quick, actionable code snippets for topics such as streaming, adding memory &amp; persistence, and design patterns (e.g. branching, subgraphs, etc.).\\n*   [Reference](https://reference.langchain.com/python/langgraph/): Detailed reference on core classes, methods, how to use the graph and checkpointing APIs, and higher-level prebuilt components.\\n*   [Examples](https://docs.langchain.com/oss/python/langgraph/agentic-rag): Guided examples on getting started with LangGraph.\\n*   [LangChain Forum](https://forum.langchain.com/): Connect with the community and share all of your technical questions, ideas, and feedback.\\n*   [LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph): Learn the basics of LangGraph in our free, structured course.\\n*   [Case studies](https://www.langchain.com/built-with-langgraph): Hear how industry leaders use LangGraph to ship AI applications at scale.\\n\\nAcknowledgements\\n----------------\\n\\n[](https://github.com/langchain-ai/langgraph#acknowledgements)\\n\\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nAbout\\n-----\\n\\nBuild resilient language agents as graphs.\\n\\n[docs.langchain.com/oss/python/langgraph/](https://docs.langchain.com/oss/python/langgraph/ \\"https://docs.langchain.com/oss/python/langgraph/\\")\\n\\n### Topics\\n\\n[python](https://github.com/topics/python \\"Topic: python\\")[open-source](https://github.com/topics/open-source \\"Topic: open-source\\")[enterprise](https://github.com/topics/enterprise \\"Topic: enterprise\\")[framework](https://github.com/topics/framework \\"Topic: framework\\")[ai](https://github.com/topics/ai \\"Topic: ai\\")[gemini](https://github.com/topics/gemini \\"Topic: gemini\\")[openai](https://github.com/topics/openai \\"Topic: openai\\")[multiagent](https://github.com/topics/multiagent \\"Topic: multiagent\\")[agents](https://github.com/topics/agents \\"Topic: agents\\")[ai-agents](https://github.com/topics/ai-agents \\"Topic: ai-agents\\")[rag](https://github.com/topics/rag \\"Topic: rag\\")[pydantic](https://github.com/topics/pydantic \\"Topic: pydantic\\")[llm](https://github.com/topics/llm \\"Topic: llm\\")[generative-ai](https://github.com/topics/generative-ai \\"Topic: generative-ai\\")[chatgpt](https://github.com/topics/chatgpt \\"Topic: chatgpt\\")[langchain](https://github.com/topics/langchain \\"Topic: langchain\\")[langgraph](https://github.com/topics/langgraph \\"Topic: langgraph\\")[deepagents](https://github.com/topics/deepagents \\"Topic: deepagents\\")\\n\\n### Resources\\n\\n[Readme](https://github.com/langchain-ai/langgraph#readme-ov-file)\\n\\n### License\\n\\n[MIT license](https://github.com/langchain-ai/langgraph#MIT-1-ov-file)\\n\\n### Code of conduct\\n\\n[Code of conduct](https://github.com/langchain-ai/langgraph#coc-ov-file)\\n\\n### Contributing\\n\\n[Contributing](https://github.com/langchain-ai/langgraph#contributing-ov-file)\\n\\n### Security policy\\n\\n[Security policy](https://github.com/langchain-ai/langgraph#security-ov-file)\\n\\n### Uh oh!\\n\\nThere was an error while loading. [Please reload this page](https://github.com/langchain-ai/langgraph).\\n\\n[Activity](https://github.com/langchain-ai/langgraph/activity)\\n\\n[Custom properties](https://github.com/langchain-ai/langgraph/custom-properties)\\n\\n### Stars\\n\\n[**24.8k** stars](https://github.com/langchain-ai/langgraph/stargazers)\\n\\n### Watchers\\n\\n[**142** watching](https://github.com/langchain-ai/langgraph/watchers)\\n\\n### Forks\\n\\n[**4.3k** forks](https://github.com/langchain-ai/langgraph/forks)\\n\\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flanggraph&amp;report=langchain-ai+%28user%29)\\n\\n[Releases 466](https://github.com/langchain-ai/langgraph/releases)\\n------------------------------------------------------------------\\n\\n[langgraph==1.0.9 Latest Feb 19, 2026](https://github.com/langchain-ai/langgraph/releases/tag/1.0.9)\\n\\n[+ 465 releases](https://github.com/langchain-ai/langgraph/releases)\\n\\n[Used by 36k](https://github.com/langchain-ai/langgraph/network/dependents)\\n---------------------------------------------------------------------------\\n\\n[* ![Image 8: @ry86pkqf74-rgb](https://avatars.githubusercontent.com/u/245083510?s=64&amp;v=4) * ![Image 9: @SURESHBEEKHANI](https://avatars.githubusercontent.com/u/107859372?s=64&amp;v=4) * ![Image 10: @scaleapi](https://avatars.githubusercontent.com/u/21693938?s=64&amp;v=4) * ![Image 11: @rahulsharmavishwakarma](https://avatars.githubusercontent.com/u/93391253?s=64&amp;v=4) * ![Image 12: @Konscig](https://avatars.githubusercontent.com/u/71553656?s=64&amp;v=4) * ![Image 13: @asadullah48](https://avatars.githubusercontent.com/u/161634653?s=64&amp;v=4) * ![Image 14: @Ontotext-AD](https://avatars.githubusercontent.com/u/6817639?s=64&amp;v=4) * ![Image 15: @jason-jaemin-lee](https://avatars.githubusercontent.com/u/226609390?s=64&amp;v=4) + 36,021](https://github.com/langchain-ai/langgraph/network/dependents)\\n\\n[Contributors 286](https://github.com/langchain-ai/langgraph/graphs/contributors)\\n---------------------------------------------------------------------------------\\n\\n*   [![Image 16: @nfcampos](https://avatars.githubusercontent.com/u/56902?s=64&amp;v=4)](https://github.com/nfcampos)\\n*   [![Image 17: @hinthornw](https://avatars.githubusercontent.com/u/13333726?s=64&amp;v=4)](https://github.com/hinthornw)\\n*   [![Image 18: @dqbd](https://avatars.githubusercontent.com/u/1443449?s=64&amp;v=4)](https://github.com/dqbd)\\n*   [![Image 19: @eyurtsev](https://avatars.githubusercontent.com/u/3205522?s=64&amp;v=4)](https://github.com/eyurtsev)\\n*   [![Image 20: @sydney-runkle](https://avatars.githubusercontent.com/u/54324534?s=64&amp;v=4)](https://github.com/sydney-runkle)\\n*   [![Image 21: @andrewnguonly](https://avatars.githubusercontent.com/u/7654246?s=64&amp;v=4)](https://github.com/andrewnguonly)\\n*   [![Image 22: @hwchase17](https://avatars.githubusercontent.com/u/11986836?s=64&amp;v=4)](https://github.com/hwchase17)\\n*   [![Image 23: @isahers1](https://avatars.githubusercontent.com/u/78627776?s=64&amp;v=4)](https://github.com/isahers1)\\n*   [![Image 24: @lnhsingh](https://avatars.githubusercontent.com/u/15386648?s=64&amp;v=4)](https://github.com/lnhsingh)\\n*   [![Image 25: @rlancemartin](https://avatars.githubusercontent.com/u/122662504?s=64&amp;v=4)](https://github.com/rlancemartin)\\n*   [![Image 26: @ccurme](https://avatars.githubusercontent.com/u/26529506?s=64&amp;v=4)](https://github.com/ccurme)\\n*   [![Image 27: @dependabot[bot]](https://avatars.githubusercontent.com/in/29110?s=64&amp;v=4)](https://github.com/apps/dependabot)\\n*   [![Image 28: @bracesproul](https://avatars.githubusercontent.com/u/46789226?s=64&amp;v=4)](https://github.com/bracesproul)\\n*   [![Image 29: @lc-arjun](https://avatars.githubusercontent.com/u/185099244?s=64&amp;v=4)](https://github.com/lc-arjun)\\n\\n[+ 272 contributors](https://github.com/langchain-ai/langgraph/graphs/contributors)\\n\\nLanguages\\n---------\\n\\n*   [Python 99.3%](https://github.com/langchain-ai/langgraph/search?l=python)\\n*   Other 0.7%\\n\\nFooter\\n------\\n\\n[](https://github.com/) ¬© 2026 GitHub,Inc. \\n\\n### Footer navigation\\n\\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\\n*   [Security](https://github.com/security)\\n*   [Status](https://www.githubstatus.com/)\\n*   [Community](https://github.community/)\\n*   [Docs](https://docs.github.com/)\\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\\n*    Manage cookies \\n*    Do not share my personal information \\n\\n You can‚Äôt perform that action at this time."}], "response_time": 0.92, "request_id": "c638eed1-e8f2-49ee-8142-786043008a22"}', name='internet_search', id='92da397c-e083-4863-97ef-25bd2df6f56f', tool_call_id='call_89580e7dd35048f5a3921112'),
  AIMessage(content='**LangGraph ‚Äì A Polished Overview**\n\n---\n\n### 1. Executive Summary\nLangGraph is an **open‚Äësource orchestration framework** that lets developers build, deploy, and manage **stateful, long‚Äërunning AI agents** as graph‚Äëbased workflows. Created by the team behind **LangChain**, it provides low‚Äëlevel primitives for durable execution, human‚Äëin‚Äëthe‚Äëloop control, memory, streaming, and debugging. Because it models agent logic as a graph, LangGraph makes complex, multi‚Äëstep, and multi‚Äëagent processes transparent, scalable, and easy to reason about.\n\n&gt; *‚ÄúLangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L4„Äë  \n\n---\n\n### 2. Core Concepts\n\n| Concept | What It Is | Why It Matters |\n|---------|------------|----------------|\n| **Nodes** | Individual components or ‚Äúactors‚Äù (e.g., an LLM call, a tool, a data fetcher). | Represent the atomic steps of a workflow; can be reused, swapped, or extended. |\n| **Edges** | Directed connections that decide **which node runs next** (conditional or fixed). | Enable flexible routing, branching, and loops within the agent. |\n| **State (Graph Memory)** | A persistent key‚Äëvalue store that travels with the workflow, recording past actions, observations, and intermediate results. | Provides ‚Äúmemory‚Äù across steps, supports debugging, and enables reflection (agents reviewing their own outputs). |\n| **Graph Architecture** | The overall directed graph (often cyclic) that defines the execution topology. | Allows complex control flows (single‚Äëagent, multi‚Äëagent, hierarchical, sequential) while keeping the system **stateful and durable**. |\n| **Durable Execution** | Execution that survives crashes, restarts, or back‚Äëpressure; the runtime can resume exactly where it left off. | Guarantees reliability for long‚Äërunning tasks (e.g., multi‚Äëday data pipelines). |\n| **Human‚Äëin‚Äëthe‚ÄëLoop (HITL)** | Points in the graph where a human can inspect, approve, or modify the agent‚Äôs state. | Adds safety and oversight for high‚Äëstakes decisions. |\n| **Streaming** | Token‚Äëby‚Äëtoken or step‚Äëby‚Äëstep output that can be shown to users in real time. | Improves UX for interactive agents (chatbots, planning assistants). |\n| **Memory** | Built‚Äëin mechanisms for short‚Äëterm working memory and long‚Äëterm persistent memory. | Enables context‚Äëaware agents that remember past interactions across sessions. |\n\n&gt; *‚ÄúNodes represent individual components or agents within an AI workflow‚Ä¶ Edges are a function within Python that determines which node to execute next based on the current state.‚Äù* ‚Äì LangChain documentation„Äê2‚Ä†L1-L4„Äë  \n\n---\n\n### 3. Key Benefits\n\n| Benefit | Description |\n|---------|-------------|\n| **Transparency &amp; Observability** | The graph visualizes every step; state can be inspected at any point. |\n| **Scalability** | Graph‚Äëbased design allows horizontal scaling and parallel execution of independent branches. |\n| **Modularity** | Nodes and edges are interchangeable; developers can plug in new models, tools, or data sources without rewriting the whole workflow. |\n| **Production‚ÄëReady** | Integrates with **LangSmith** for tracing, evaluation, and deployment; supports durable execution and checkpointing. |\n| **Human‚ÄëCentric Control** | Built‚Äëin HITL checkpoints let humans intervene, approve, or redirect agents. |\n| **Open‚ÄëSource &amp; Free** | MIT‚Äëlicensed; no runtime overhead; can be used standalone or alongside LangChain. |\n| **Multi‚ÄëAgent Support** | Facilitates creation of networks of specialized agents that collaborate on complex problems. |\n\n&gt; *‚ÄúLangGraph provides low‚Äëlevel supporting infrastructure for any long‚Äërunning, stateful workflow or agent‚Ä¶ It does not abstract prompts or architecture, and provides the following central benefits: durable execution, human‚Äëin‚Äëthe‚Äëloop, comprehensive memory, debugging with LangSmith, production‚Äëready deployment.‚Äù* ‚Äì LangChain overview„Äê2‚Ä†L1-L5„Äë  \n\n---\n\n### 4. Typical Use Cases\n\n| Use Case | Example |\n|----------|---------|\n| **Chatbots &amp; Virtual Assistants** | Build a vacation‚Äëplanning bot that remembers user preferences, loops for clarification, and streams responses token‚Äëby‚Äëtoken. |\n| **Agent Systems** | Deploy autonomous agents for robotics, autonomous vehicles, or game NPCs that can pause for human review. |\n| **Retrieval‚ÄëAugmented Generation (RAG)** | Combine external document retrieval with LLM reasoning inside a graph where retrieval nodes feed into generation nodes. |\n| **Multi‚ÄëAgent Collaboration** | Orchestrate teams of specialized agents (e.g., ‚Äúresearcher‚Äù, ‚Äúwriter‚Äù, ‚Äúeditor‚Äù) that pass artifacts back and forth. |\n| **Enterprise Workflow Automation** | Automate multi‚Äëstep business processes (e.g., invoice approval, onboarding) with persistent state and audit trails. |\n| **Experimentation &amp; Prototyping** | Quickly prototype new agent topologies in a visual IDE (LangGraph Studio) before moving to production. |\n\n&gt; *‚ÄúLangGraph is also built on several key technologies, including LangChain‚Ä¶ It provides a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent‚Äëbased systems.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L3„Äë  \n\n---\n\n### 5. Relationship to LangChain &amp; the Broader Ecosystem\n\n| Component | Role |\n|-----------|------|\n| **LangChain** | High‚Äëlevel library that supplies ready‚Äëmade agents, tool‚Äëwrappers, and prompt templates. LangGraph sits **underneath** LangChain, offering the durable execution and state‚Äëmanagement backbone. |\n| **LangSmith** | Observability platform for tracing, evaluating, and monitoring LangGraph‚Äëbased agents in production. |\n| **LangGraph Studio** | Visual, drag‚Äëand‚Äëdrop IDE that lets users design graphs without writing code; useful for rapid prototyping and collaboration. |\n| **Community Integrations** | Projects like **CrewAI**, **AutoGen**, and **MetaGPT** can be built on top of LangGraph, leveraging its graph primitives for multi‚Äëagent orchestration. |\n| **Deployment Options** | Can be run locally, in a container, or as a managed service via LangSmith; supports scaling on Kubernetes, cloud VMs, or edge devices. |\n\n&gt; *‚ÄúLangGraph can be used standalone, but it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents.‚Äù* ‚Äì LangChain docs„Äê2‚Ä†L1-L2„Äë  \n\n---\n\n### 6. Getting Started (Practical Steps)\n\n1. **Install**  \n   ```bash\n   pip install -U langgraph\n   ```\n2. **Create a Simple Graph** (from LangChain docs)  \n   ```python\n   from langgraph.graph import StateGraph, MessagesState, START, END\n\n   class State(MessagesState):\n       pass\n\n   def mock_llm(state: State):\n       return {"messages": [{"role": "ai", "content": "hello world"}]}\n\n   graph = StateGraph(State)\n   graph.add_node(mock_llm)\n   graph.add_edge(START, "mock_llm")\n   graph.add_edge("mock_llm", END)\n   app = graph.compile()\n\n   print(app.invoke({"messages": [{"role": "user", "content": "hi!"}]))\n   ```\n3. **Add Memory &amp; Persistence** ‚Äì Use built‚Äëin checkpointing or integrate with a database (e.g., SQLite, Redis).  \n4. **Add Human‚Äëin‚Äëthe‚ÄëLoop** ‚Äì Insert `interrupt()` calls to pause and present the current state to a user for approval.  \n5. **Deploy** ‚Äì Wrap the compiled graph in a FastAPI/Flask endpoint, or deploy via **LangSmith** for managed scaling and observability.  \n\n&gt; *The official LangGraph Quickstart guide provides step‚Äëby‚Äëstep tutorials, example notebooks, and a ‚ÄúHello‚ÄëWorld‚Äù template.* ‚Äì LangChain documentation„Äê2‚Ä†L1-L3„Äë\n\n---\n\n### 7. Future Roadmap (as of early‚Äë2026)\n\n| Planned Enhancement | Expected Impact |\n|---------------------|-----------------|\n| **Advanced NLP** ‚Äì richer language understanding and generation capabilities. | More natural, context‚Äëaware interactions. |\n| **Improved ML Integration** ‚Äì auto‚Äëlearning loops that refine agent policies over time. | Continuous performance gains without manual retraining. |\n| **New Platform Support** ‚Äì mobile, edge, and IoT runtimes. | Bring agent workflows to constrained environments. |\n| **Enhanced Debugging &amp; Visualization** ‚Äì richer state‚Äëtravel and ‚Äútime‚Äëmachine‚Äù debugging. | Faster issue resolution and higher reliability. |\n| **Standardized Multi‚ÄëAgent Protocols** ‚Äì built‚Äëin patterns for coordination, negotiation, and conflict resolution. | Simplify creation of large‚Äëscale multi‚Äëagent systems. |\n\n&gt; *‚ÄúFuture developments include enhanced NLP, improved machine learning, and support for new platforms such as mobile and edge computing.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L5„Äë\n\n---\n\n### 8. Key Takeaways\n\n* **LangGraph = Graph‚Äëbased orchestration for AI agents** ‚Äì it turns complex, stateful workflows into visual, durable graphs.  \n* **Built for production** ‚Äì offers durable execution, human‚Äëin‚Äëthe‚Äëloop checkpoints, persistent memory, and seamless integration with LangSmith for observability.  \n* **Open‚Äësource &amp; flexible** ‚Äì MIT‚Äëlicensed, can be used alone or as the execution engine behind LangChain‚Äôs higher‚Äëlevel agents.  \n* **Wide adoption** ‚Äì Companies such as Klarna, Replit, Elastic, Norwegian Cruise Line, and IBM‚Äôs own watsonx.ai use LangGraph to power chatbots, multi‚Äëagent systems, and enterprise automation.  \n* **Getting started is easy** ‚Äì a few `pip install` commands and a short Python snippet get you a working graph; visual tools (LangGraph Studio) make design accessible to non‚Äëprogrammers.  \n\n---\n\n### 9. References\n\n1. **IBM Think ‚Äì ‚ÄúWhat is LangGraph?‚Äù** ‚Äì Overview of LangGraph‚Äôs purpose, architecture, and use cases.  \n2. **LangChain Documentation ‚Äì ‚ÄúLangGraph overview‚Äù** ‚Äì Core concepts, installation, and production‚Äëready features.  \n3. **Reddit ‚Äì r/AI_Agents ‚ÄúWhy use LangGraph?‚Äù** ‚Äì Community perspective on graph‚Äëbased workflow advantages.  \n4. **LangChain.com ‚Äì Product page for LangGraph** ‚Äì Marketing overview, customer stories, and FAQ.  \n5. **GitHub ‚Äì langchain-ai/langgraph** ‚Äì Source repository, release notes, and contribution activity.  \n\n--- \n\n*Prepared as a concise, structured briefing for anyone seeking a clear understanding of what LangGraph is, how it works, and why it matters in the modern AI‚Äëagent landscape.*', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2592, 'prompt_tokens': 31532, 'total_tokens': 34124, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 243, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771589005-ywGQGvGvLv8XbqFplapK', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c7aee-f0c1-7c50-927d-d827373b12a0-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 31532, 'output_tokens': 2592, 'total_tokens': 34124, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 243}})]}</code></pre>
</div>
</div>
<div id="eff95675" class="cell" data-execution_count="39">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the agent's response</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result[<span class="st">"messages"</span>][<span class="op">-</span><span class="dv">1</span>].content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>**LangGraph ‚Äì A Polished Overview**

---

### 1. Executive Summary
LangGraph is an **open‚Äësource orchestration framework** that lets developers build, deploy, and manage **stateful, long‚Äërunning AI agents** as graph‚Äëbased workflows. Created by the team behind **LangChain**, it provides low‚Äëlevel primitives for durable execution, human‚Äëin‚Äëthe‚Äëloop control, memory, streaming, and debugging. Because it models agent logic as a graph, LangGraph makes complex, multi‚Äëstep, and multi‚Äëagent processes transparent, scalable, and easy to reason about.

&gt; *‚ÄúLangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L4„Äë  

---

### 2. Core Concepts

| Concept | What It Is | Why It Matters |
|---------|------------|----------------|
| **Nodes** | Individual components or ‚Äúactors‚Äù (e.g., an LLM call, a tool, a data fetcher). | Represent the atomic steps of a workflow; can be reused, swapped, or extended. |
| **Edges** | Directed connections that decide **which node runs next** (conditional or fixed). | Enable flexible routing, branching, and loops within the agent. |
| **State (Graph Memory)** | A persistent key‚Äëvalue store that travels with the workflow, recording past actions, observations, and intermediate results. | Provides ‚Äúmemory‚Äù across steps, supports debugging, and enables reflection (agents reviewing their own outputs). |
| **Graph Architecture** | The overall directed graph (often cyclic) that defines the execution topology. | Allows complex control flows (single‚Äëagent, multi‚Äëagent, hierarchical, sequential) while keeping the system **stateful and durable**. |
| **Durable Execution** | Execution that survives crashes, restarts, or back‚Äëpressure; the runtime can resume exactly where it left off. | Guarantees reliability for long‚Äërunning tasks (e.g., multi‚Äëday data pipelines). |
| **Human‚Äëin‚Äëthe‚ÄëLoop (HITL)** | Points in the graph where a human can inspect, approve, or modify the agent‚Äôs state. | Adds safety and oversight for high‚Äëstakes decisions. |
| **Streaming** | Token‚Äëby‚Äëtoken or step‚Äëby‚Äëstep output that can be shown to users in real time. | Improves UX for interactive agents (chatbots, planning assistants). |
| **Memory** | Built‚Äëin mechanisms for short‚Äëterm working memory and long‚Äëterm persistent memory. | Enables context‚Äëaware agents that remember past interactions across sessions. |

&gt; *‚ÄúNodes represent individual components or agents within an AI workflow‚Ä¶ Edges are a function within Python that determines which node to execute next based on the current state.‚Äù* ‚Äì LangChain documentation„Äê2‚Ä†L1-L4„Äë  

---

### 3. Key Benefits

| Benefit | Description |
|---------|-------------|
| **Transparency &amp; Observability** | The graph visualizes every step; state can be inspected at any point. |
| **Scalability** | Graph‚Äëbased design allows horizontal scaling and parallel execution of independent branches. |
| **Modularity** | Nodes and edges are interchangeable; developers can plug in new models, tools, or data sources without rewriting the whole workflow. |
| **Production‚ÄëReady** | Integrates with **LangSmith** for tracing, evaluation, and deployment; supports durable execution and checkpointing. |
| **Human‚ÄëCentric Control** | Built‚Äëin HITL checkpoints let humans intervene, approve, or redirect agents. |
| **Open‚ÄëSource &amp; Free** | MIT‚Äëlicensed; no runtime overhead; can be used standalone or alongside LangChain. |
| **Multi‚ÄëAgent Support** | Facilitates creation of networks of specialized agents that collaborate on complex problems. |

&gt; *‚ÄúLangGraph provides low‚Äëlevel supporting infrastructure for any long‚Äërunning, stateful workflow or agent‚Ä¶ It does not abstract prompts or architecture, and provides the following central benefits: durable execution, human‚Äëin‚Äëthe‚Äëloop, comprehensive memory, debugging with LangSmith, production‚Äëready deployment.‚Äù* ‚Äì LangChain overview„Äê2‚Ä†L1-L5„Äë  

---

### 4. Typical Use Cases

| Use Case | Example |
|----------|---------|
| **Chatbots &amp; Virtual Assistants** | Build a vacation‚Äëplanning bot that remembers user preferences, loops for clarification, and streams responses token‚Äëby‚Äëtoken. |
| **Agent Systems** | Deploy autonomous agents for robotics, autonomous vehicles, or game NPCs that can pause for human review. |
| **Retrieval‚ÄëAugmented Generation (RAG)** | Combine external document retrieval with LLM reasoning inside a graph where retrieval nodes feed into generation nodes. |
| **Multi‚ÄëAgent Collaboration** | Orchestrate teams of specialized agents (e.g., ‚Äúresearcher‚Äù, ‚Äúwriter‚Äù, ‚Äúeditor‚Äù) that pass artifacts back and forth. |
| **Enterprise Workflow Automation** | Automate multi‚Äëstep business processes (e.g., invoice approval, onboarding) with persistent state and audit trails. |
| **Experimentation &amp; Prototyping** | Quickly prototype new agent topologies in a visual IDE (LangGraph Studio) before moving to production. |

&gt; *‚ÄúLangGraph is also built on several key technologies, including LangChain‚Ä¶ It provides a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent‚Äëbased systems.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L3„Äë  

---

### 5. Relationship to LangChain &amp; the Broader Ecosystem

| Component | Role |
|-----------|------|
| **LangChain** | High‚Äëlevel library that supplies ready‚Äëmade agents, tool‚Äëwrappers, and prompt templates. LangGraph sits **underneath** LangChain, offering the durable execution and state‚Äëmanagement backbone. |
| **LangSmith** | Observability platform for tracing, evaluating, and monitoring LangGraph‚Äëbased agents in production. |
| **LangGraph Studio** | Visual, drag‚Äëand‚Äëdrop IDE that lets users design graphs without writing code; useful for rapid prototyping and collaboration. |
| **Community Integrations** | Projects like **CrewAI**, **AutoGen**, and **MetaGPT** can be built on top of LangGraph, leveraging its graph primitives for multi‚Äëagent orchestration. |
| **Deployment Options** | Can be run locally, in a container, or as a managed service via LangSmith; supports scaling on Kubernetes, cloud VMs, or edge devices. |

&gt; *‚ÄúLangGraph can be used standalone, but it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents.‚Äù* ‚Äì LangChain docs„Äê2‚Ä†L1-L2„Äë  

---

### 6. Getting Started (Practical Steps)

1. **Install**  
   ```bash
   pip install -U langgraph
   ```
2. **Create a Simple Graph** (from LangChain docs)  
   ```python
   from langgraph.graph import StateGraph, MessagesState, START, END

   class State(MessagesState):
       pass

   def mock_llm(state: State):
       return {"messages": [{"role": "ai", "content": "hello world"}]}

   graph = StateGraph(State)
   graph.add_node(mock_llm)
   graph.add_edge(START, "mock_llm")
   graph.add_edge("mock_llm", END)
   app = graph.compile()

   print(app.invoke({"messages": [{"role": "user", "content": "hi!"}]))
   ```
3. **Add Memory &amp; Persistence** ‚Äì Use built‚Äëin checkpointing or integrate with a database (e.g., SQLite, Redis).  
4. **Add Human‚Äëin‚Äëthe‚ÄëLoop** ‚Äì Insert `interrupt()` calls to pause and present the current state to a user for approval.  
5. **Deploy** ‚Äì Wrap the compiled graph in a FastAPI/Flask endpoint, or deploy via **LangSmith** for managed scaling and observability.  

&gt; *The official LangGraph Quickstart guide provides step‚Äëby‚Äëstep tutorials, example notebooks, and a ‚ÄúHello‚ÄëWorld‚Äù template.* ‚Äì LangChain documentation„Äê2‚Ä†L1-L3„Äë

---

### 7. Future Roadmap (as of early‚Äë2026)

| Planned Enhancement | Expected Impact |
|---------------------|-----------------|
| **Advanced NLP** ‚Äì richer language understanding and generation capabilities. | More natural, context‚Äëaware interactions. |
| **Improved ML Integration** ‚Äì auto‚Äëlearning loops that refine agent policies over time. | Continuous performance gains without manual retraining. |
| **New Platform Support** ‚Äì mobile, edge, and IoT runtimes. | Bring agent workflows to constrained environments. |
| **Enhanced Debugging &amp; Visualization** ‚Äì richer state‚Äëtravel and ‚Äútime‚Äëmachine‚Äù debugging. | Faster issue resolution and higher reliability. |
| **Standardized Multi‚ÄëAgent Protocols** ‚Äì built‚Äëin patterns for coordination, negotiation, and conflict resolution. | Simplify creation of large‚Äëscale multi‚Äëagent systems. |

&gt; *‚ÄúFuture developments include enhanced NLP, improved machine learning, and support for new platforms such as mobile and edge computing.‚Äù* ‚Äì IBM Think article„Äê1‚Ä†L1-L5„Äë

---

### 8. Key Takeaways

* **LangGraph = Graph‚Äëbased orchestration for AI agents** ‚Äì it turns complex, stateful workflows into visual, durable graphs.  
* **Built for production** ‚Äì offers durable execution, human‚Äëin‚Äëthe‚Äëloop checkpoints, persistent memory, and seamless integration with LangSmith for observability.  
* **Open‚Äësource &amp; flexible** ‚Äì MIT‚Äëlicensed, can be used alone or as the execution engine behind LangChain‚Äôs higher‚Äëlevel agents.  
* **Wide adoption** ‚Äì Companies such as Klarna, Replit, Elastic, Norwegian Cruise Line, and IBM‚Äôs own watsonx.ai use LangGraph to power chatbots, multi‚Äëagent systems, and enterprise automation.  
* **Getting started is easy** ‚Äì a few `pip install` commands and a short Python snippet get you a working graph; visual tools (LangGraph Studio) make design accessible to non‚Äëprogrammers.  

---

### 9. References

1. **IBM Think ‚Äì ‚ÄúWhat is LangGraph?‚Äù** ‚Äì Overview of LangGraph‚Äôs purpose, architecture, and use cases.  
2. **LangChain Documentation ‚Äì ‚ÄúLangGraph overview‚Äù** ‚Äì Core concepts, installation, and production‚Äëready features.  
3. **Reddit ‚Äì r/AI_Agents ‚ÄúWhy use LangGraph?‚Äù** ‚Äì Community perspective on graph‚Äëbased workflow advantages.  
4. **LangChain.com ‚Äì Product page for LangGraph** ‚Äì Marketing overview, customer stories, and FAQ.  
5. **GitHub ‚Äì langchain-ai/langgraph** ‚Äì Source repository, release notes, and contribution activity.  

--- 

*Prepared as a concise, structured briefing for anyone seeking a clear understanding of what LangGraph is, how it works, and why it matters in the modern AI‚Äëagent landscape.*</code></pre>
</div>
</div>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ul>
<li>Three key methods for models: invoke, stream, and batch.</li>
<li>LLMs can be configured to responsd in a structured format</li>
<li>Agent = Model + Tools</li>
<li>Models (LLMs) are the brain-power of agents</li>
<li>Tools are simply names and agruments of defined Python functions</li>
</ul>
</section>
<section id="activity" class="level2">
<h2 class="anchored" data-anchor-id="activity">Activity</h2>
<p><strong>Over to you:</strong> create an Agent that is able to answer questions, with an added internet search capability.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>