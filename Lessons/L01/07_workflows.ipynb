{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c541c74a",
   "metadata": {},
   "source": [
    "# Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d875d",
   "metadata": {},
   "source": [
    "## Understanding and Using Functional API\n",
    "\n",
    "The **Functional API** allows you to add LangGraph's key features — [persistence](/oss/python/langgraph/persistence), [memory](/oss/python/langgraph/add-memory), [human-in-the-loop](/oss/python/langgraph/interrupts), and [streaming](/oss/python/langgraph/streaming) — to your applications with minimal changes to your existing code.\n",
    "\n",
    "It is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as `if` statements, `for` loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\n",
    "\n",
    "The Functional API uses two key building blocks:\n",
    "\n",
    "* **`@entrypoint`** – Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\n",
    "* **[`@task`](https://reference.langchain.com/python/langgraph/func/task)** – Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\n",
    "\n",
    "This provides a minimal abstraction for building workflows with state management and streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec717b",
   "metadata": {},
   "source": [
    "## Entrypoint\n",
    "\n",
    "The [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/entrypoint) decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling *long-running tasks* and [interrupts](/oss/python/langgraph/interrupts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a618d89",
   "metadata": {},
   "source": [
    "\n",
    "### Injectable parameters\n",
    "\n",
    "When declaring an `entrypoint`, you can request access to additional parameters that will be injected automatically at runtime. These parameters include:\n",
    "\n",
    "| Parameter    | Description                                                                                                                                                                 |\n",
    "| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **previous** | Access the state associated with the previous `checkpoint` for the given thread. See [short-term-memory](#short-term-memory).                                               |\n",
    "| **store**    | An instance of \\[BaseStore]\\[langgraph.store.base.BaseStore]. Useful for [long-term memory](/oss/python/langgraph/use-functional-api#long-term-memory).                     |\n",
    "| **writer**   | Use to access the StreamWriter when working with Async Python \\< 3.11. See [streaming with functional API for details](/oss/python/langgraph/use-functional-api#streaming). |\n",
    "| **config**   | For accessing run time configuration. See [RunnableConfig](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) for information.                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a495c",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.func import entrypoint\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import StreamWriter\n",
    "\n",
    "in_memory_checkpointer = InMemorySaver(...)\n",
    "in_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory\n",
    "\n",
    "@entrypoint(\n",
    "    checkpointer=in_memory_checkpointer,  # Specify the checkpointer\n",
    "    store=in_memory_store  # Specify the store\n",
    ")\n",
    "def my_workflow(\n",
    "    some_input: dict,  # The input (e.g., passed via `invoke`)\n",
    "    *,\n",
    "    previous: Any = None, # For short-term memory\n",
    "    store: BaseStore,  # For long-term memory\n",
    "    writer: StreamWriter,  # For streaming custom data\n",
    "    config: RunnableConfig  # For accessing the configuration passed to the entrypoint\n",
    ") -> ...:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b004b2",
   "metadata": {},
   "source": [
    "\n",
    "### Short-term memory\n",
    "\n",
    "When an `entrypoint` is defined with a `checkpointer`, it stores information between successive invocations on the same **thread id** in [checkpoints](/oss/python/langgraph/persistence#checkpoints).\n",
    "\n",
    "This allows accessing the state from the previous invocation using the `previous` parameter.\n",
    "\n",
    "By default, the `previous` parameter is the return value of the previous invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3466c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langgraph.func import entrypoint\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Create a checkpointer\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "def my_workflow(number: int, *, previous: Any = None) -> int:\n",
    "    previous = previous or 0\n",
    "    return number + previous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ecae45",
   "metadata": {},
   "source": [
    "Note: the `*` in the function signature marks the end of positional arguments and the start of keyword-only arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb65f68",
   "metadata": {},
   "source": [
    "Invocation with `config.configurable.thread_id` which allows for consecutive calls to be grouped under the same \"conversation\" or \"session\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3372d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"some_thread_id\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea9e553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_workflow.invoke(1, config=config)  # 1 (previous was None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a262d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_workflow.invoke(2, config=config)  # 3 (previous was 1 from the previous "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6966643",
   "metadata": {},
   "source": [
    "The checkpointer saves the state of the workflow between invocations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52b1e2",
   "metadata": {},
   "source": [
    "### Long-term memory\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed396d8",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b75bcfa",
   "metadata": {},
   "source": [
    "\n",
    "## Task\n",
    "\n",
    "A **task** represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\n",
    "\n",
    "* **Asynchronous Execution**: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\n",
    "* **Checkpointing**: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See [persistence](/oss/python/langgraph/persistence) for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d25fd",
   "metadata": {},
   "source": [
    "```python\n",
    "from langgraph.func import task\n",
    "\n",
    "@task()\n",
    "def slow_computation(input_value):\n",
    "    # Simulate a long-running operation\n",
    "    ...\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4ef83",
   "metadata": {},
   "source": [
    "::: {.callout-warning}\n",
    "\n",
    "**Serialization**: The **outputs** of tasks must be JSON-serializable to support checkpointing.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65401863",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "**Tasks** can only be called from within an **entrypoint**, another **task**, or a [state graph node](/oss/python/langgraph/graph-api#nodes).\n",
    "\n",
    "Tasks *cannot* be called directly from the main application code.\n",
    "\n",
    "When you call a **task**, it returns *immediately* with a future object. A future is a placeholder for a result that will be available later.\n",
    "\n",
    "To obtain the result of a **task**, you can either wait for it synchronously (using `result()`) or await it asynchronously (using `await`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cefda",
   "metadata": {},
   "source": [
    "Wait for the result synchronously:\n",
    "\n",
    "```python\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "def my_workflow(some_input: int) -> int:\n",
    "    future = slow_computation(some_input)\n",
    "    result = future.result()\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd94381",
   "metadata": {},
   "source": [
    "Await result asynchronously:\n",
    "\n",
    "```python\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "async def my_workflow(some_input: int) -> int:\n",
    "    future = slow_computation(some_input)\n",
    "    result = await future\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cf05b0",
   "metadata": {},
   "source": [
    "\n",
    "## When to use a task\n",
    "\n",
    "**Tasks** are useful in the following scenarios:\n",
    "\n",
    "**Checkpointing**: When you need to save the result of a long-running operation to a checkpoint, so you don't need to recompute it when resuming the workflow.\n",
    "\n",
    "**Human-in-the-loop**: If you're building a workflow that requires human intervention, you MUST use **tasks** to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the [determinism](#determinism) section for more details.\n",
    "\n",
    "**Parallel Execution**: For I/O-bound tasks, **tasks** enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\n",
    "\n",
    "**Observability**: Wrapping operations in **tasks** provides a way to track the progress of the workflow and monitor the execution of individual operations using [LangSmith](https://docs.langchain.com/langsmith/home).\n",
    "\n",
    "**Retryable Work**: When work needs to be retried to handle failures or inconsistencies, **tasks** provide a way to encapsulate and manage the retry logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e05a44",
   "metadata": {},
   "source": [
    "# Using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dfa41a",
   "metadata": {},
   "source": [
    "## Creating a simple workflow\n",
    "\n",
    "When defining an `entrypoint`, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f80fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.func import entrypoint\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "def my_workflow(inputs: dict) -> int:\n",
    "    value = inputs[\"value\"]\n",
    "    another_value = inputs[\"another_value\"]\n",
    "    \n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"some_thread_id\",\n",
    "    }\n",
    "}\n",
    "\n",
    "my_workflow.invoke({\"value\": 1, \"another_value\": 2}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925f19c",
   "metadata": {},
   "source": [
    "Extended example: simple workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b272b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.func import entrypoint, task\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Task that checks if a number is even\n",
    "@task\n",
    "def is_even(number: int) -> bool:\n",
    "    return number % 2 == 0\n",
    "\n",
    "# Task that formats a message\n",
    "@task\n",
    "def format_message(is_even: bool) -> str:\n",
    "    return \"The number is even.\" if is_even else \"The number is odd.\"\n",
    "\n",
    "# Create a checkpointer for persistence\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "def workflow(inputs: dict) -> str:\n",
    "    \"\"\"Simple workflow to classify a number.\"\"\"\n",
    "    even = is_even(inputs[\"number\"]).result()\n",
    "    return format_message(even).result()\n",
    "\n",
    "# Run the workflow with a unique thread ID\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "result = workflow.invoke({\"number\": 7}, config=config)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc469264",
   "metadata": {},
   "source": [
    "This example demonstrates how to use the` @task` and `@entrypoint` decorators syntactically. Given that a checkpointer is provided, the workflow results will be persisted in the checkpointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.func import entrypoint, task\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "model = init_chat_model('gpt-3.5-turbo')\n",
    "\n",
    "# Task: generate essay using an LLM\n",
    "@task\n",
    "def compose_essay(topic: str) -> str:\n",
    "    \"\"\"Generate an essay about the given topic.\"\"\"\n",
    "    return model.invoke([\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes essays.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Write an essay about {topic}.\"}\n",
    "    ]).content\n",
    "\n",
    "# Create a checkpointer for persistence\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "def workflow(topic: str) -> str:\n",
    "    \"\"\"Simple workflow that generates an essay with an LLM.\"\"\"\n",
    "    result = compose_essay(topic).result()\n",
    "    return result\n",
    "\n",
    "# Execute the workflow\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "result = workflow.invoke(\"the history of flight\", config=config)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc5fbf",
   "metadata": {},
   "source": [
    "## Parallel execution\n",
    "\n",
    "Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5faafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def add_one(number: int) -> int:\n",
    "    return number + 1\n",
    "\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "def graph(numbers: list[int]) -> list[str]:\n",
    "    futures = [add_one(i) for i in numbers]\n",
    "    return [f.result() for f in futures]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf4542",
   "metadata": {},
   "source": [
    "This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.func import entrypoint, task\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Initialize the LLM model\n",
    "model = init_chat_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Task that generates a paragraph about a given topic\n",
    "@task\n",
    "def generate_paragraph(topic: str) -> str:\n",
    "    response = model.invoke([\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes educational paragraphs.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Write a paragraph about {topic}.\"}\n",
    "    ])\n",
    "    return response.content\n",
    "\n",
    "# Create a checkpointer for persistence\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "def workflow(topics: list[str]) -> str:\n",
    "    \"\"\"Generates multiple paragraphs in parallel and combines them.\"\"\"\n",
    "    futures = [generate_paragraph(topic) for topic in topics]\n",
    "    paragraphs = [f.result() for f in futures]\n",
    "    return \"\\n\\n\".join(paragraphs)\n",
    "\n",
    "# Run the workflow\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "result = workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config=config)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
