{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b90b342c",
      "metadata": {},
      "source": [
        "# Agents in LangChain\n",
        "\n",
        "This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "331015e2",
      "metadata": {},
      "source": [
        "## Questions\n",
        "\n",
        "- What is an Agent in LangChain and how to make one?\n",
        "- What's the relationship between an LLM and an Agent?\n",
        "- What can agents do?\n",
        "- Can I run an agent locally without a provider?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eda9316e",
      "metadata": {},
      "source": [
        "## Setup Virtual Environment\n",
        "\n",
        "```sh\n",
        "uv init\n",
        "uv venv -p 3.12\n",
        "```\n",
        "\n",
        "### Activate the virtual environment\n",
        "\n",
        "::: {.panel-tabset}\n",
        "#### Windows\n",
        "\n",
        "```sh\n",
        ".venv\\Scripts\\activate.bat\n",
        "```\n",
        "\n",
        "#### MacOS / Linux\n",
        "\n",
        "```sh\n",
        "source .venv/bin/activate\n",
        "```\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32145515",
      "metadata": {},
      "source": [
        "## What is an **API Key**?\n",
        "\n",
        "Think of an API Key as a **hotel key card**.\n",
        "\n",
        "* **The Hotel (Server):** Has resources (rooms) but keeps them locked.\n",
        "* **The Guest (Client):** Wants access.\n",
        "* **The Key Card (API Key):** Identifies you and proves you are allowed to enter specific rooms.\n",
        "\n",
        "---\n",
        "\n",
        "### What & Why\n",
        "\n",
        "An API key is a unique string of characters used to identify the calling program.\n",
        "\n",
        "* **Identification:** Keys \"authenticate the calling project,\" allowing the server to recognize who is asking for data.\n",
        "* **Control:** This lets the server track usage for billing and enforce limits (quotas) so one user doesn't crash the system.\n",
        "\n",
        "---\n",
        "\n",
        "### Security Risks\n",
        "\n",
        "If you lose your key, it is like dropping your credit card.\n",
        "\n",
        "* **Theft:** Attackers can use your key to make requests on your behalf.\n",
        "* **Consequences:** You suffer **financial loss** (paying for their usage) or **service denial** (they use up your available quota).\n",
        "\n",
        "> **Rule:** Never post keys on public sites like GitHub.\n",
        "\n",
        "### How to Set Your API Key?\n",
        "\n",
        "This project uses OpenRouter (**The Unified Interface For LLMs**), via LiteLLM to access the DeepSeek model, which requires an API key. If you don't already have an OpenRouter API key, you can create one for free at: [OpenRouter](https://openrouter.ai/keys).\n",
        "\n",
        "Write your API key into an `.env` file as an environment variable, as follows:\n",
        "\n",
        "```sh\n",
        "OPENROUTER_API_KEY=...\n",
        "```\n",
        "\n",
        "> Note: make sure to add it to `.gitignore` to avoid committing it to the repository.\n",
        "> \n",
        "> Note: this is different than the `.venv` file used for the virtual environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb93d08",
      "metadata": {},
      "source": [
        "If we use the OpenAI API, we'll have to add:\n",
        "\n",
        "```sh\n",
        "OPENAI_API_BASE=\"https://openrouter.ai/api/v1\"\n",
        "```\n",
        "\n",
        ".. such that the model uses OpenRouter instead of the default OpenAI API."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e48d9532",
      "metadata": {},
      "source": [
        "### Sign up and Set LangSmith API (Free)\n",
        "\n",
        "* Cost: \n",
        "* Sign up for LangSmith [here](https://docs.langchain.com/langsmith/create-account-api-key#create-an-account-and-api-key), find out more about LangSmith and how to use it within your workflow [here](https://www.langchain.com/langsmith). \n",
        "*  Set `LANGSMITH_API_KEY`, `LANGSMITH_TRACING_V2=\"true\"` `LANGSMITH_PROJECT=\"langchain-academy\"`in your environment \n",
        "*  If you are on the EU instance also set `LANGSMITH_ENDPOINT`=\"https://eu.api.smith.langchain.com\" as well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1870e009",
      "metadata": {},
      "source": [
        "### Set up Tavily API for web search (Free)\n",
        "\n",
        "* Tavily Search API is a search engine optimized for LLMs and RAG, aimed at efficient, \n",
        "quick, and persistent search results. \n",
        "* You can sign up for an API key [here](https://tavily.com/). \n",
        "It's easy to sign up and offers a very generous free tier. Some lessons (in Module 4) will use Tavily. \n",
        "\n",
        "* Set `TAVILY_API_KEY` in your environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7b717ae",
      "metadata": {},
      "source": [
        "### Install dependencies\n",
        "\n",
        "```sh\n",
        "uv add langchain tavily-python langchain_openai langchain_community \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56162cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# We use OpenRouter for the agent â€” set OPENROUTER_API_KEY in .env\n",
        "# Get your key at https://openrouter.ai/keys\n",
        "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
        "    raise RuntimeError(\n",
        "        \"OPENROUTER_API_KEY is not set. Add it to your .env file, e.g.:\\n\"\n",
        "        \"OPENROUTER_API_KEY=your-openrouter-api-key\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d61903c",
      "metadata": {},
      "source": [
        "# Models\n",
        "\n",
        "[LLMs](https://docs.langchain.com/oss/python/langchain/models) are powerful AI tools that can interpret and generate text like humans. They're versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.\n",
        "\n",
        "The quality and capabilities of the model you choose directly impact your agent's baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28cc8bce",
      "metadata": {},
      "source": [
        "### Choosing between models\n",
        "\n",
        "- [Models | OpenRouter.ai](https://openrouter.ai/models)\n",
        "- [LLM Stats](https://llm-stats.com/)\n",
        "- [Model Recommendation | Artficial Analysis](https://artificialanalysis.ai/models/recommend)\n",
        "  - [TTS | Artficial Analysis](https://artificialanalysis.ai/text-to-speech/leaderboard)\n",
        "- [Arena.ai](https://arena.ai/leaderboard/text-to-image)\n",
        "- [MTEB: Embedding Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n",
        "- [Open ASR](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ffab830",
      "metadata": {},
      "source": [
        "Note: **Agents** require [**a model that supports tool calling**](https://openrouter.ai/models?fmt=cards&supported_parameters=tools)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe91a055",
      "metadata": {},
      "source": [
        "## Basic usage\n",
        "\n",
        "Models can be utilized in two ways:\n",
        "\n",
        "1. **With agents** - Models can be dynamically specified when creating an [agent](/oss/python/langchain/agents#model).\n",
        "2. **Standalone** - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0c543c",
      "metadata": {},
      "source": [
        "[Here](https://docs.langchain.com/oss/python/langchain/models) is a useful how-to for all the things that you can do with chat models, but we'll show a few highlights below.\n",
        "\n",
        "There are [a few standard parameters](https://docs.langchain.com/oss/python/langchain/models#parameters) that we can set with chat models. Two of the most common are:\n",
        "\n",
        "* `model`: the name of the model\n",
        "* `temperature`: the sampling temperature\n",
        "* `max_tokens`: the maximum number of tokens to generate\n",
        "\n",
        "`Temperature` controls the randomness or creativity of the model's output where:\n",
        "\n",
        "- **Low temperature** (close to 0) is more deterministic and focused outputs. This is good for tasks requiring accuracy or factual responses.\n",
        "- **High temperature** (close to 1) is good for creative tasks or generating varied responses. \n",
        "\n",
        "`max_tokens` limits the total number of tokens in the response, effectively controlling how long the output can be."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b21b0e5d",
      "metadata": {},
      "source": [
        "LangChain supports many models via [third-party integrations](https://docs.langchain.com/oss/python/integrations/chat). By default, the course will use  [ChatOpenAI](https://docs.langchain.com/oss/python/integrations/chat/openai) because it is both popular and performant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44cc8280",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# https://openrouter.ai/openai/gpt-5-nano\n",
        "model_gpt5_nano = ChatOpenAI(\n",
        "    model=\"openai/gpt-5-nano\",\n",
        "    temperature=0,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
        ")\n",
        "\n",
        "# https://openrouter.ai/nvidia/nemotron-3-nano-30b-a3b:free\n",
        "model_nemotron3_nano = ChatOpenAI(\n",
        "    model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n",
        "    temperature=0,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8e23bc9",
      "metadata": {},
      "source": [
        "### Running a model locally\n",
        "\n",
        "LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.\n",
        "\n",
        "[Ollama](https://docs.langchain.com/oss/python/integrations/chat/ollama) is one of the easiest ways to run chat and embedding models locally."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438cfb9e",
      "metadata": {},
      "source": [
        "## Key Methods\n",
        "\n",
        "1. Invoke\n",
        "3. Stream\n",
        "2. Batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc95fea",
      "metadata": {},
      "source": [
        "### 1. Invoke"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7877273",
      "metadata": {},
      "source": [
        "The most straightforward way to call a model is to use `invoke()` with a single message or a list of messages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b52c4ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "message = model_nemotron3_nano.invoke(\"why do parrots talk?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5310787e",
      "metadata": {},
      "source": [
        ".. this returns an `AIMessage` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b3b68f58",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Parrots â€œtalkâ€ because theyâ€™re one of the few animals that can **learn and reproduce complex vocalizations**â€”a skill called vocal learning. Hereâ€™s a quick rundown of why and how they do it:\\n\\n| Reason | What it means for parrots |\\n|--------|--------------------------|\\n| **Social bonding** | In the wild, parrots use calls to stay in contact with flock members, coordinate movement, and reinforce pair bonds. Mimicking the sounds of their companions (including human speech) helps them stay socially connected. |\\n| **Territory & status** | Some species use distinctive vocalizations to claim space or signal dominance. A parrot that can produce a clear, attentionâ€‘grabbing â€œspeechâ€ may gain more social leverage. |\\n| **Mental stimulation** | Parrots are highly intelligent; they need cognitive challenges. Learning new sounds is a form of problemâ€‘solving that keeps their brains active and reduces boredomâ€‘related behaviors (like featherâ€‘plucking). |\\n| **Mimicry as a survival tool** | In the wild, many parrots imitate the calls of other species (e.g., hawks, other birds) to deter predators or to blend into mixedâ€‘species flocks. This ability extends to human speech when they live with people. |\\n| **Neural specialization** | Parrots possess a brain region called the **song system** that is analogous to the vocalâ€‘learning circuits of songbirds and humans. This system allows them to store, modify, and reproduce complex sound patterns with high fidelity. |\\n\\n### How they actually â€œtalkâ€\\n\\n1. **Listening & Encoding** â€“ A parrot constantly hears sounds in its environment and stores a mental â€œtemplateâ€ of each one.\\n2. **Practice & Refinement** â€“ Using its syrinx (the bird equivalent of vocal cords), it tries out different variations, gradually shaping the output to match the stored template.\\n3. **Feedback Loop** â€“ The bird hears its own attempt, compares it to the template, and tweaks pitch, rhythm, or timing until itâ€™s close enough.\\n4. **Reinforcement** â€“ Positive attention from humans (treats, praise, interaction) reinforces the behavior, encouraging the parrot to repeat the sound.\\n\\n### Why some parrots sound more â€œhumanâ€‘likeâ€\\n\\n- **Species differences** â€“ African Grey, Amazon, and some Cockatoos have especially flexible vocal repertoires.\\n- **Individual exposure** â€“ The more varied and frequent the human speech they hear, the richer their mimicry can become.\\n- **Training & interaction** â€“ Repetition, consistent cues, and reward-based training help solidify specific words or phrases.\\n\\n### Bottom line\\n\\nParrots â€œtalkâ€ not because they understand language the way we do, but because theyâ€™re **social, intelligent mimics** equipped with a unique vocalâ€‘learning brain circuit. By copying the sounds that matter most to themâ€”whether itâ€™s a flock mateâ€™s call, a predatorâ€™s alarm, or a humanâ€™s voiceâ€”they enhance their social world and keep their minds sharp. \\n\\nIf youâ€™re curious about getting a talking parrot, remember that the ability to mimic speech varies widely among individuals and species, and it thrives best with plenty of social interaction, mental enrichment, and consistent, positive training.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 731, 'prompt_tokens': 22, 'total_tokens': 753, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 58, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771586508-ixswTWdhOMhqmt9bxk5a', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c7ac8-d0ab-75d1-aeeb-5bcfce7f31bb-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 22, 'output_tokens': 731, 'total_tokens': 753, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 58}})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d02242",
      "metadata": {},
      "source": [
        ".. which has a `content` property, which includes the generated response text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4efc494d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parrots â€œtalkâ€ because theyâ€™re one of the few animals that can **learn and reproduce complex vocalizations**â€”a skill called vocal learning. Hereâ€™s a quick rundown of why and how they do it:\n",
            "\n",
            "| Reason | What it means for parrots |\n",
            "|--------|--------------------------|\n",
            "| **Social bonding** | In the wild, parrots use calls to stay in contact with flock members, coordinate movement, and reinforce pair bonds. Mimicking the sounds of their companions (including human speech) helps them stay socially connected. |\n",
            "| **Territory & status** | Some species use distinctive vocalizations to claim space or signal dominance. A parrot that can produce a clear, attentionâ€‘grabbing â€œspeechâ€ may gain more social leverage. |\n",
            "| **Mental stimulation** | Parrots are highly intelligent; they need cognitive challenges. Learning new sounds is a form of problemâ€‘solving that keeps their brains active and reduces boredomâ€‘related behaviors (like featherâ€‘plucking). |\n",
            "| **Mimicry as a survival tool** | In the wild, many parrots imitate the calls of other species (e.g., hawks, other birds) to deter predators or to blend into mixedâ€‘species flocks. This ability extends to human speech when they live with people. |\n",
            "| **Neural specialization** | Parrots possess a brain region called the **song system** that is analogous to the vocalâ€‘learning circuits of songbirds and humans. This system allows them to store, modify, and reproduce complex sound patterns with high fidelity. |\n",
            "\n",
            "### How they actually â€œtalkâ€\n",
            "\n",
            "1. **Listening & Encoding** â€“ A parrot constantly hears sounds in its environment and stores a mental â€œtemplateâ€ of each one.\n",
            "2. **Practice & Refinement** â€“ Using its syrinx (the bird equivalent of vocal cords), it tries out different variations, gradually shaping the output to match the stored template.\n",
            "3. **Feedback Loop** â€“ The bird hears its own attempt, compares it to the template, and tweaks pitch, rhythm, or timing until itâ€™s close enough.\n",
            "4. **Reinforcement** â€“ Positive attention from humans (treats, praise, interaction) reinforces the behavior, encouraging the parrot to repeat the sound.\n",
            "\n",
            "### Why some parrots sound more â€œhumanâ€‘likeâ€\n",
            "\n",
            "- **Species differences** â€“ African Grey, Amazon, and some Cockatoos have especially flexible vocal repertoires.\n",
            "- **Individual exposure** â€“ The more varied and frequent the human speech they hear, the richer their mimicry can become.\n",
            "- **Training & interaction** â€“ Repetition, consistent cues, and reward-based training help solidify specific words or phrases.\n",
            "\n",
            "### Bottom line\n",
            "\n",
            "Parrots â€œtalkâ€ not because they understand language the way we do, but because theyâ€™re **social, intelligent mimics** equipped with a unique vocalâ€‘learning brain circuit. By copying the sounds that matter most to themâ€”whether itâ€™s a flock mateâ€™s call, a predatorâ€™s alarm, or a humanâ€™s voiceâ€”they enhance their social world and keep their minds sharp. \n",
            "\n",
            "If youâ€™re curious about getting a talking parrot, remember that the ability to mimic speech varies widely among individuals and species, and it thrives best with plenty of social interaction, mental enrichment, and consistent, positive training.\n"
          ]
        }
      ],
      "source": [
        "print(message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45931846",
      "metadata": {},
      "source": [
        "A list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f4c8fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ø£Ø­Ø¨ ØªØ·ÙˆÙŠØ± Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª.\n"
          ]
        }
      ],
      "source": [
        "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "conversation = [    \n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to Arabic.\"),\n",
        "    HumanMessage(content=\"Translate: I love programming.\"),\n",
        "    AIMessage(content=\"Ø£Ø­Ø¨ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©.\"),\n",
        "    HumanMessage(content=\"I love building applications.\")\n",
        "]\n",
        "\n",
        "message = model_nemotron3_nano.invoke(conversation)\n",
        "print(message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81367072",
      "metadata": {},
      "source": [
        "### 2. Stream\n",
        "\n",
        "Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.\n",
        "\n",
        "Calling `stream()` returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3c795917",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Par|ro|ts| are| famous| for| their| vivid| plum|age|,| and| that| color|ation| isn|â€™t| just| for| show| â€”| it| serves| several| important| functions| that| have| been| shaped| by| evolution|.| Here|â€™s| a| quick| rund|own| of| the| main| reasons|:\n",
            "\n",
            "||| Reason| || How| it| works| || Why| it| matters| for| parro|ts| |\n",
            "|||--------|||------------|--|||----------------|------------||\n",
            "||| **|Sex|ual| selection|**| || Bright|,| contrasting| colors| signal| health|,| good| genetics|,| and| strong| immune| systems|.| Males| and| females| often| use| plum|age| to| attract| mates| or| to| assess| rivals|.| || In| many| par|rot| species|,| brighter| males| are| preferred| by| females|,| leading| to| stronger| reproductive| success| for| those| with| more| vivid| feathers|.| |\n",
            "||| **|Species| and| individual| recognition|**| || Dist|inct| color| patterns| help| individuals| identify| members| of| their| own| species| (|and| sometimes| specific| mates| or| offspring|).| || In| dense|,| multi|â€‘|species| fl|ocks|,| a| unique| â€œ|signature|â€| color|ation| reduces| the| chance| of mistaken| identity and helps| keep| the| group| cohesive|.| |\n",
            "||| **|Social| signaling|**| || Color| can| convey| status|,| dominance|,| or| readiness| to| breed|.| Some| parro|ts| change| the| intensity| or| hue| of| their| feathers| during| courts|hip| or| when| threatened|.| || A| sudden| flash| of| bright| color| can| start|le| a| predator| or| signal| aggression| to| a| rival|,| while| a| muted| palette| may| indicate| submission| or| non|â€‘|th|reat|.| |\n",
            "||| **|Cam|oufl|age| in| specific| habitats|**| || While| many| parro|ts| are| conspicuous|,| some| (|e|.g|.,| forest|â€‘|dw|elling| species|)| have| gre|ens| and| brown|s| that| blend| with| foli|age|.| || In| these| cases|,| color|ation| is| an| adaptation| to| the| visual| background| of| their| environment|,| helping| them| avoid| predators| while| still| being| visible| to| cons|pecific|s|.| |\n",
            "||| **|D|iet|â€‘|derived| pigments|**| || Parro|ts| cannot| synthesize| many| bright| pigments| themselves|;| they| obtain| red|s|,| or|anges|,| and| yellow|s| from| carot|eno|ids| in| fruits|,| flowers|,| and| seeds|.| || A| diet| rich| in| these| pigments| not| only| fuels| health| but| also| directly| influences| how| vivid| a| bird|â€™s| feathers| appear|.| |\n",
            "||| **|Struct|ural| color|ation|**| || Mic|roscopic| structures| in| feather| barb|ules| scatter| light|,| producing| ir|ides|cent| blues| and| gre|ens| that| are| not| pigment|â€‘|based|.| || This| creates| sh|immer|ing| effects| that| can| be| more| eye|â€‘|catch|ing| than| pigment| alone|,| especially| under| different| lighting| conditions|.| |\n",
            "\n",
            "|###| Put|ting| it| together|\n",
            "|-| **|E|volution|ary| pressure|**:| Bright| colors| increase| an| individual|â€™s| chances| of| finding| a| mate| and| defending| territory|,| which| boost|s| reproductive| success|.\n",
            "|-| **|Ec|ological| context|**:| The| same| colors| that| attract| mates| can| also| serve| as| warning| signals| or| camoufl|age|,| depending| on| the| species|â€™| habitat| and| behavior|.\n",
            "|-| **|Phys|iological| limits|**:| Because| parro|ts| rely| on| dietary| pigments| and| structural| feather| features|,| their| color| palette| is| tightly| linked| to| what| they| eat| and| how| their| feathers| are| built.\n",
            "\n",
            "|So|,| the| spectacular| rainbow| of| par|rot| feathers| is| a| multif|unctional| adaptation| â€”| primarily| a| tool| for| communication| and| reproduction|,| fine|â€‘|t|uned| by| diet|,| habitat|,| and| the| physics| of| light|.||| ðŸŒˆ||||ðŸ¦œ||||"
          ]
        }
      ],
      "source": [
        "for chunk in model_nemotron3_nano.stream(\"Why do parrots have colorful feathers?\"):\n",
        "    print(chunk.text, end=\"|\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6647e9b",
      "metadata": {},
      "source": [
        "### 3. Batch\n",
        "\n",
        "Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b1cd4f85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='The capital of Saudi Arabia is **Riyadh**.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 24, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 26, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587230-rws1sKd4yd2Ir45jBE92', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-dab8-7670-aebf-be1a19d73902-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 24, 'output_tokens': 38, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 26}}\n",
            "content='2\\u202f+\\u202f8\\u202f=\\u202f10.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 23, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 22, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587230-FEcwZ8VMp5p1Qgvgnrqr', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-daba-76d3-856b-d4204b17db9a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 23, 'output_tokens': 40, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 22}}\n",
            "content='The sky appears blue because molecules in the atmosphere scatter shortâ€‘wavelength (blue) sunlightâ€”an objective physical effect that our visual system interprets as the color blue.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 32, 'total_tokens': 206, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 159, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771587231-VBn1J8V9CcCv10J2gqBT', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7ad3-dabc-7871-bdd8-f89b2bed8661-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 32, 'output_tokens': 174, 'total_tokens': 206, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 159}}\n"
          ]
        }
      ],
      "source": [
        "responses = model_nemotron3_nano.batch([\n",
        "    \"What is the capital of Saudi Arabia?\",\n",
        "    \"What is 2 + 8\",\n",
        "    \"Is the sky blue or is it our perception? give a short and concise answer\"\n",
        "])\n",
        "\n",
        "for response in responses:\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2264be1a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The capital of Saudi Arabia is **Riyadh**.\n",
            "====================================================================================================\n",
            "2â€¯+â€¯8â€¯=â€¯10.\n",
            "====================================================================================================\n",
            "The sky appears blue because molecules in the atmosphere scatter shortâ€‘wavelength (blue) sunlightâ€”an objective physical effect that our visual system interprets as the color blue.\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "for i, response in enumerate(responses):\n",
        "    print(response.content)\n",
        "    print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c83b28",
      "metadata": {},
      "source": [
        "## Structured output\n",
        "\n",
        "Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.\n",
        "\n",
        "[Pydantic models](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage) provide the richest feature set with field validation, descriptions, and nested structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4765e1b9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "title='Inception' year=2010 director='Christopher Nolan' rating=8.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/halgoz/work/ai-agents/content/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=Movie(title='Inception', ...pher Nolan', rating=8.8), input_type=Movie])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Movie(BaseModel):\n",
        "    \"\"\"A movie with details.\"\"\"\n",
        "    title: str = Field(..., description=\"The title of the movie\")\n",
        "    year: int = Field(..., description=\"The year the movie was released\")\n",
        "    director: str = Field(..., description=\"The director of the movie\")\n",
        "    rating: float = Field(..., description=\"The movie's rating out of 10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f79423",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_with_structure = model_nemotron3_nano.with_structured_output(Movie)\n",
        "response = model_with_structure.invoke(\"Provide details about the movie Inception\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "634b07db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Inception\n",
            "Year: 2010\n",
            "Director: Christopher Nolan\n",
            "Rating: 8.8\n"
          ]
        }
      ],
      "source": [
        "print(\"Title:\", response.title)\n",
        "print(\"Year:\", response.year)\n",
        "print(\"Director:\", response.director)\n",
        "print(\"Rating:\", response.rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7695fe5",
      "metadata": {},
      "source": [
        "## Tool calling\n",
        "\n",
        "Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\n",
        "\n",
        "1. A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\n",
        "2. A function or coroutine to execute.\n",
        "\n",
        "Note: A *coroutine* is a method that can suspend execution and resume at a later time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d53c5b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool: get_weather\n",
            "Args: {'location': 'Boston'}\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Get the weather at a location.\"\"\"\n",
        "    return f\"It's always sunny in {location}.\"\n",
        "\n",
        "\n",
        "model_with_tools = model_nemotron3_nano.bind_tools([get_weather])\n",
        "\n",
        "response = model_with_tools.invoke(\"What's the weather like in the Moon?\")\n",
        "for tool_call in response.tool_calls:\n",
        "    # View tool calls made by the model\n",
        "    print(f\"Tool: {tool_call['name']}\")\n",
        "    print(f\"Args: {tool_call['args']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f7441f4",
      "metadata": {},
      "source": [
        "### Tool Input Schemas\n",
        "\n",
        "Define complex inputs with Pydantic models or JSON schemas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "530c429c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "class WeatherInput(BaseModel):\n",
        "    \"\"\"Input for weather queries.\"\"\"\n",
        "    location: str = Field(description=\"City name or coordinates\")\n",
        "    units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n",
        "        default=\"celsius\",\n",
        "        description=\"Temperature unit preference\"\n",
        "    )\n",
        "    include_forecast: bool = Field(\n",
        "        default=False,\n",
        "        description=\"Include 5-day forecast\"\n",
        "    )\n",
        "\n",
        "@tool(args_schema=WeatherInput)\n",
        "def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n",
        "    \"\"\"Get current weather and optional forecast.\"\"\"\n",
        "    temp = 22 if units == \"celsius\" else 72\n",
        "    result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n",
        "    if include_forecast:\n",
        "        result += \"\\nNext 5 days: Sunny\"\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9ec84860",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_with_tools = model_nemotron3_nano.bind_tools([get_weather])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "afe66a0d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool: get_weather\n",
            "Args: {'location': 'the Moon', 'units': 'fahrenheit', 'include_forecast': True}\n"
          ]
        }
      ],
      "source": [
        "response = model_with_tools.invoke(\n",
        "    \"What's the weather like in the Moon? \"\n",
        "    \"in fahrenheit and include the forecast please.\"\n",
        ")\n",
        "for tool_call in response.tool_calls:\n",
        "    # View tool calls made by the model\n",
        "    print(f\"Tool: {tool_call['name']}\")\n",
        "    print(f\"Args: {tool_call['args']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10e241c1",
      "metadata": {},
      "source": [
        "## Search Tools\n",
        "\n",
        "Tavily is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3fcc980e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tavily import TavilyClient\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "539beacd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def internet_search(\n",
        "    query: str,\n",
        "    max_results: int = 5,\n",
        "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
        "    include_raw_content: bool = False,\n",
        "):\n",
        "    \"\"\"Run a web search\"\"\"\n",
        "    return tavily_client.search(\n",
        "        query,\n",
        "        max_results=max_results,\n",
        "        include_raw_content=include_raw_content,\n",
        "        topic=topic,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "45c13907",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'What is LangGraph?',\n",
              " 'response_time': 0.61,\n",
              " 'follow_up_questions': None,\n",
              " 'answer': None,\n",
              " 'images': [],\n",
              " 'results': [{'url': 'https://www.ibm.com/think/topics/langgraph',\n",
              "   'title': 'What is LangGraph? - IBM',\n",
              "   'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agentâ€™s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.',\n",
              "   'score': 0.95539,\n",
              "   'raw_content': None},\n",
              "  {'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/',\n",
              "   'title': 'What is LangGraph? - GeeksforGeeks',\n",
              "   'content': 'LangGraph is an open-source framework built by LangChain that streamlines the creation and management of AI agent workflows. At its core, LangGraph combines large language models (LLMs) with graph-based architectures allowing developers to map, organize and optimize how AI agents interact and make decisions. By treating workflows as interconnected nodes and edges, LangGraph offers a scalable, transparent and developer-friendly way to design advanced AI systems ranging from simple chatbots to multi-agent system. The diagram below shows how LangGraph structures its agent-based workflow using distinct tools and stages. By designing workflows, users combine multiple nodes into powerful, dynamic AI processes. * ****langgraph:**** Framework for building graph-based AI workflows. ### Step 6: Build LangGraph Workflow. * Build the workflow graph using LangGraph, adding nodes for classification and response, connecting them with edges and compiling the app. * Send each input through the workflow graph and returns the botâ€™s response, either a greeting or an AI-powered answer. + Machine Learning Interview Questions and Answers15+ min read.',\n",
              "   'score': 0.9393874,\n",
              "   'raw_content': None},\n",
              "  {'url': 'https://docs.langchain.com/oss/python/langgraph/overview',\n",
              "   'title': 'LangGraph overview - Docs by LangChain',\n",
              "   'content': 'Trusted by companies shaping the future of agentsâ€” including Klarna, Replit, Elastic, and moreâ€” LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChainâ€™s agents that provide pre-built architectures for common LLM and tool-calling loops. LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more. LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.## LangSmith Agent Server. Contains agent abstractions built on top of LangGraph.',\n",
              "   'score': 0.92507464,\n",
              "   'raw_content': None}],\n",
              " 'request_id': '4d1b1ba4-3b2c-4355-a9d6-446cbddf26e0'}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = internet_search(\"What is LangGraph?\", max_results=3)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "212f3dd1",
      "metadata": {},
      "source": [
        "## Create an Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9515b993",
      "metadata": {},
      "source": [
        "Agents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n",
        "\n",
        "An LLM Agent runs tools in a loop to achieve a goal. An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e58e9774",
      "metadata": {},
      "source": [
        "![Agent Loop](./assets/agent_loop.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b80204a1",
      "metadata": {},
      "source": [
        "`create_agent` provides a production-ready agent implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065e6367",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "\n",
        "# System prompt to steer the agent to be an expert researcher\n",
        "AGENT_PROMPT = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n",
        "\n",
        "You have access to an internet search tool as your primary means of gathering information.\n",
        "\n",
        "Keep it short and concise.\n",
        "\n",
        "## `internet_search`\n",
        "\n",
        "Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n",
        "\"\"\"\n",
        "\n",
        "agent = create_agent(\n",
        "    model=model_nemotron3_nano,\n",
        "    tools=[internet_search],\n",
        "    system_prompt=AGENT_PROMPT\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cd0c585",
      "metadata": {},
      "source": [
        "### Invoke"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "e633e4c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "be1de6bd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={}, id='29083314-fa84-4871-ba0d-f594b69ce1fe'),\n",
              "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 327, 'prompt_tokens': 447, 'total_tokens': 774, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 292, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771589000-vrLW5z3J6cADEb7ZfFeY', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c7aee-db92-70c2-9d0e-ef86169a1ec1-0', tool_calls=[{'name': 'internet_search', 'args': {'max_results': 5, 'topic': 'general', 'query': 'LangGraph', 'include_raw_content': True}, 'id': 'call_89580e7dd35048f5a3921112', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 447, 'output_tokens': 327, 'total_tokens': 774, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 292}}),\n",
              "  ToolMessage(content='{\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph? - IBM\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agentâ€™s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\", \"score\": 0.94251215, \"raw_content\": \"# What is LangGraph?\\\\n\\\\n## Authors\\\\n\\\\n[Bryan Clark](https://www.ibm.com/think/author/bryan-clark) \\\\n\\\\nSenior Technology Advocate\\\\n\\\\n## LangGraph overview\\\\n\\\\nLangGraph, created by [LangChain](https://www.ibm.com/think/topics/langchain), is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize [large language models](https://www.ibm.com/think/topics/large-language-models) (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an [AI agent workflow](https://www.ibm.com/think/topics/ai-agents).\\\\n\\\\nWhat does all this information mean? The following example can offer a clearer understanding of LangGraph: Think about these graph-based architectures as a powerful configurable map, a â€œSuper-Map.â€ Users can envision the [AI workflow](https://www.ibm.com/think/topics/ai-workflow) as being â€œThe Navigatorâ€ of this â€œSuper-Map.â€ Finally, in this example, the user is â€œThe Cartographer.â€ In this sense, the navigator charts out the optimal routes between points on the â€œSuper-Map,â€ all of which are created by â€œThe Cartographer.â€\\\\n\\\\nTo recap, optimal routes within the graph-based architectures (â€œSuper-Mapâ€) are charted and explored by using the AI workflow\\xa0(â€œThe Navigatorâ€). This analogy is a great place to start understanding LangGraphâ€”and if you like maps then you are welcome for the bonus opportunity to see someone use the word cartographer.\\\\n\\\\nLangGraph workflow\\\\n\\\\nLangGraph illuminates the processes within an AI workflow, allowing full transparency of the agentâ€™s state. Within LangGraph, the â€œstateâ€ feature serves as a memory bank that records and tracks all the valuable information processed by the AI system. Itâ€™s similar to a digital notebook where the system captures and updates data as it moves through various stages of a workflow or graph analysis.\\\\n\\\\nFor example, if you were running agents to monitor the weather, this feature could track the number of times it snowed and make suggestions based on changing snowfall trends. This observability of how the system works to complete complex tasks is useful for beginners to understand more about state management. State management is helpful when it comes to debugging as it allows the applicationâ€™s state to be centralized, thus often shortening the overall process.\\\\n\\\\nThis approach allows for more effective decision-making, improved scalability and enhanced overall performance. It also allows for more engagement with individuals who might be new to these processes or prefer a clearer picture of what is going on behind the scenes.\\\\n\\\\nLangGraph is also built on several key technologies, including [LangChain,](https://www.ibm.com/think/topics/langchain) a Python framework for building AI applications. LangChain includes a library for building and managing [LLMs](https://www.ibm.com/think/topics/large-language-models). LangGraph also uses the human-in-the-loop approach. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including [chatbots](https://www.ibm.com/think/topics/chatbots), state graphs and [other agent-based systems](https://www.ibm.com/think/topics/multiagent-system).\\\\n\\\\nDelve deeper into the world of LangGraph by exploring its key features, benefits and use cases. By the end of this article, you will have the knowledge and resources to take the next steps with LangGraph.\\\\n\\\\n## Key components of LangGraph\\\\n\\\\nLetâ€™s begin by first understanding the key components that make up LangGraph. The framework is built around several key components that work together to enable users to create and manage complex AI workflows. These components include:\\\\n\\\\n#### Monitoring mechanism\\\\n\\\\n**Human-in-the-loop**: [Human-in-the-loop (HITL)](https://hdsr.mitpress.mit.edu/pub/812vijgg/release/3)\\xa0refers to the requirement of human interaction at some point in the process. In the realm of [machine learning](https://www.ibm.com/think/topics/machine-learning) (ML), HITL refers to a collaborative process where humans augment the computational capabilities of machines to make informed decisions while building a model. By using the most critical data points, HITL enhances the accuracy of machine learning algorithms, surpassing random sampling methods.\\\\n\\\\n#### Graph architecture\\\\n\\\\n**Stateful graphs**: A concept where each node in the graph represents a step in the computation, essentially devising a state graph. This stateful approach allows the graph to retain information about the previous steps, enabling continuous and contextual processing of information as the computation unfolds. Users can manage all LangGraphâ€™s stateful graphs with its APIs.\\\\n\\\\n**Cyclical graph**: A cyclical graph is any graph that contains at least one cycle and is essential for agent runtimes. This means that there exists a path that starts and ends at the same node, forming a loop within the graph. Complex workflows often involve cyclic dependencies, where the outcome of one step depends on previous steps in the loop.\\\\n\\\\n**Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. Nodes can be thought of as â€œactorsâ€ that interact with each other in a specific way. For example,\\xa0to add nodes for tool calling, one can use the ToolNode. Another example, the next node, refers to the node that will be executed following the current one.\\\\n\\\\n**Edges**: Edges are a function within Python that determines which node to execute next based on the current state. Edges can be conditional branches or fixed transitions.\\\\n\\\\n#### Tools\\\\n\\\\n**RAG**: [Retrieval-augmented generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) combines the power of LLMs with contextual information from external sources by retrieving relevant documents, which are then used as input for answer generation.\\\\n\\\\n**Workflows**: Workflows are the sequences of node interactions that define an AI workflow. By arranging nodes into a workflow, users can create more complex and dynamic workflows that use the strengths of individual components.\\\\n\\\\n**APIs**: LangGraph provides a set of [APIs](https://www.ibm.com/think/topics/api) that enable users to interact with its components in a programmatic way. Users can use an API key, add new nodes, modify existing workflows and retrieve data from an AI workflow.\\\\n\\\\n**LangSmith**: LangSmith is a specialized API for building and managing LLMs within LangGraph. It provides tools for initializing LLMs, adding conditional edges and optimizing performance. By combining these components in innovative ways, users can build more sophisticated AI workflows that use the strengths of individual components.\\\\n\\\\nIndustry newsletter\\\\n\\\\n### The latest AI trends, brought to you by experts\\\\n\\\\nGet curated insights on the most importantâ€”and intriguingâ€”AI news. Subscribe to our weekly Think newsletter. See the [IBM Privacy Statement](https://www.ibm.com/privacy).\\\\n\\\\n### Thank you! You are subscribed.\\\\n\\\\nYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe [here](https://www.ibm.com/account/reg/signup?formid=news-urx-51525). Refer to our [IBM Privacy Statement](https://www.ibm.com/us-en/privacy) for more information.\\\\n\\\\n## How LangGraph scales\\\\n\\\\nBy using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.\\\\n\\\\n**Enhanced decision-making**: By modeling complex relationships between nodes, LangGraph provides a framework for building more effective decision-making systems.\\\\n\\\\n**Increased flexibility**: An open source nature and modular design for developers to integrate new components and adapt existing workflows.\\\\n\\\\n**Multiagent workflows:** Complex tasks can be tackled through multiagent workflows. This approach involves creating dedicated LangChain agents for specific tasks or domains. Routing tasks to the appropriate LangChain agents allows for parallel execution and efficient handling of diverse workloads. Such a multiagent network architecture exemplifies the decentralized\\xa0coordination of agent automation.\\\\n\\\\nA great example, created by Joao Moura, is using CrewAI with LangChain and LangGraph. Checking emails and creating drafts is automated with CrewAI orchestrating autonomous AI agents, enabling them to collaborate and run complex tasks efficiently.\\\\n\\\\nIBM watsonx.ai\\\\n\\\\n### Data Insights with LangGraph and watsonx.ai\\\\n\\\\nCan an AI agent take our natural language query and do the processing for us to give us that meaningful output? We use several pieces of open source technology and the power of watsonx.ai to\\xa0put this to the test.\\\\n\\\\n[Explore watsonx.ai](https://www.ibm.com/products/watsonx-ai)\\\\n\\\\n## LangGraph use cases\\\\n\\\\n**Chatbots**: Users can build an agentic application for vacation planning, with node-based workflows and directed acyclic graphs (DAGs). The chatbot learns to respond to minimal user input and tailor recommendations. Currently, services such as Googleâ€™s Duplex are using LangGraph in a similar fashion to mimic human-like conversations.\\\\n\\\\n**Agent systems**: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\\\\n\\\\n**LLM applications**: By using LangGraphâ€™s capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences.\\\\n\\\\n## LLM integration in LangGraph\\\\n\\\\nLangGraphâ€™s agents are based on OpenAIâ€™s series of GPT (generative pretrained transformer) models GPT-3.5 and GPT-4. However, LangGraph and its open source community have contributed to the addition of several other models that initialize through LLM API configuration, including Anthropic and AzureChatOpenAI models. The relatively small loop is similar to projects such as Auto-GPT.\\\\n\\\\nLangGraph offers a YouTube tutorial that facilitates the exploration of how to integrate with open source LLMs on its GitHub docs site. The first step to integrating an LLM is to set up an inference repository (repo) such as LLaMA-Factory, FastChat and Ollama. This repository enables deployment of the corresponding LLM model that is configured through its API credentials.\\\\n\\\\n## Other AI agent frameworks\\\\n\\\\nCrewAI, MetaGPT and AutoGen are just a few multiagent frameworks that can handle complex workflows. This operation allows for a more flexible and nuanced approach to tackling diverse computational challenges. By providing comprehensive debugging capabilities, these\\xa0frameworks enable developers to quickly identify and resolve issues, leading to more efficient development and optimization processes.\\\\n\\\\n## LangGraph Studio: A visual interface for workflow development\\\\n\\\\nLangGraph has also introduced LangGraph Studio, a visual interface for workflow development. With LangGraph Studio, users can design and build workflows by using a graphical interface, without having to write code. The downloadable desktop application makes LangGraph Studio more usable for beginners. LangGraph Studio has also made these additional features available:\\\\n\\\\n**Shallow learning curve**: LangGraph Studio is not needed to access LangGraph. However, by using LangGraph Studioâ€™s visual interface, users can focus on designing their workflows without getting bogged down in code.\\\\n\\\\n**Improved collaboration**: LangGraph Studio enables the sharing of workflows with others, whether thatâ€™s a team of developers or a client.\\\\n\\\\n**Debugging**: The capabilities do not end with building a graph, debugging features are included to ensure the graph is accurate and reliable. LangGraph Studio, with its cutting-edge integrated development environment (IDE), helps visualize and debug LangGraph applications.\\\\n\\\\n## Future developments\\\\n\\\\n**Enhanced natural language processing (NLP)**: LangGraph will have more advanced [NLP](https://www.ibm.com/think/topics/natural-language-processing) capabilities, allowing it to better understand natural language and provide more accurate responses.\\\\n\\\\n**Improved machine learning**: LangGraph will have improved machine learning capabilities, allowing it to learn and improve over time.\\\\n\\\\n**Support for new platforms**: LangGraph will support new platforms, such as mobile devices and edge computing to make its technology more accessible.\\\\n\\\\nLink copied\\\\n\\\\n[Ebook   Start realizing ROI: A practical guide to agentic AI \\\\n\\\\nDiscover ways to get ahead, successfully scaling AI across your business with real results.\\\\n\\\\n Read the ebook](https://www.ibm.com/account/reg/signup?formid=urx-54067)\\\\n\\\\n[Build, run and manage AI agents with watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)\\\\n\\\\n## Resources\\\\n\\\\n[IBV Report   The enterprise in 2030: Engineered for perpetual innovation \\\\n\\\\nDiscover our five predictions about what will define the most successful enterprises in 2030, and the steps leaders can take to gain an AI-first advantage.\\\\n\\\\n Read the report](https://www.ibm.com/thought-leadership/institute-business-value/report/enterprise-2030)\\\\n\\\\n[Report   AI governance imperative: evolving regulations and emergence of agentic AI \\\\n\\\\nLearn how evolving regulations and the emergence of AI agents are reshaping the need for robust AI governance frameworks.\\\\n\\\\n Read the report](https://www.ibm.com/forms/mkt-54070)\\\\n\\\\n[Techsplainers podcast   Agentic AI explained \\\\n\\\\nTechsplainers by IBM breaks down the essentials of agentic AI, from key concepts to realâ€‘world use cases. Clear, quick episodes help you learn the fundamentals fast.\\\\n\\\\n Listen now](https://www.ibm.com/think/podcasts/techsplainers#tabs-fw-44e285b2cc-item-a3ea4f0927-tab)\\\\n\\\\n[Guidebook   Unlock AI ROI: A tactical guide to enterprise productivity \\\\n\\\\nLearn proven strategies to boost productivity and power enterprise transformation with AI and innovation at the core.\\\\n\\\\n Read the guidebook](https://www.ibm.com/account/reg/signup?formid=urx-54227)\\\\n\\\\n[Report   IDC MarketScape names IBM a leader in 2025 gen AI evaluation technology \\\\n\\\\nDownload the report to learn why IDC MarketScape named IBM a leader in 2025 gen AI evaluation technology, and how watsonx.governanceÂ® advances risk management, reporting and integration.\\\\n\\\\n Read the report](https://www.ibm.com/forms/mkt-54030)\\\\n\\\\n[Buyer guide   How AI agents and assistants can benefit your organization \\\\n\\\\nDive into this comprehensive guide that breaks down key use cases and core capabilities, providing step-by-step recommendations to help you choose the right solutions for your business.\\\\n\\\\n Read the guide](https://www.ibm.com/forms/mkt-53811)\\\\n\\\\n[Video   Reimagine business productivity with AI agents and assistants \\\\n\\\\nLearn how AI agents and AI assistants can work together to achieve new levels of productivity.\\\\n\\\\n Watch now](https://www.ibm.com/think/videos/ai-academy/reimagine-business-productivity-with-ai)\\\\n\\\\n[Demo   Try watsonx OrchestrateÂ® \\\\n\\\\nExplore how generative AI assistants can lighten your workload and improve productivity.\\\\n\\\\n Start the demo](https://www.ibm.com/products/watsonx-orchestrate/demos)\\\\n\\\\n[Report   From AI projects to profits: How agentic AI can sustain financial returns \\\\n\\\\nLearn how organizations are shifting from launching AI in disparate pilots to using it to drive transformation at the core.\\\\n\\\\n Read the report](https://www.ibm.com/thought-leadership/institute-business-value/report/agentic-ai-profits)\\\\n\\\\n[Report   Omdia Report on empowered intelligence: The impact of AI agents \\\\n\\\\nDiscover how you can unlock the full potential of gen AI with AI agents.\\\\n\\\\n Read the report](https://www.ibm.com/forms/mkt-53501)\\\\n\\\\n[Podcast   How AI agents will reinvent productivity \\\\n\\\\nLearn ways to use AI to be more creative, efficient and start adapting to a future that involves working closely with AI agents.\\\\n\\\\n Listen now](https://www.ibm.com/think/podcasts/ai-in-action/how-ai-agents-will-reinvent-productivity)\\\\n\\\\n[News   Ushering in the agentic enterprise: Putting AI to work across your entire technology estate \\\\n\\\\nStay updated about the new emerging AI agents, a fundamental tipping point in the AI revolution.\\\\n\\\\n Read the news](https://www.ibm.com/new/announcements/productivity-revolution-with-ai-agents-that-work-across-stack)\\\\n\\\\n[Podcast   The future of agents, AI energy consumption, Anthropic computer use and Google watermarking AI-generated text \\\\n\\\\nStay ahead of the curve with our AI experts on this episode of Mixture of Experts as they dive deep into the future of AI agents and more.\\\\n\\\\n Listen now](https://www.ibm.com/think/podcasts/mixture-of-experts/future-of-ai-agents-ai-energy-consumption-anthropic-computer-use-google-watermarking-ai)\\\\n\\\\n[Case study   How Comparus is using a \\\\\"banking assistant\\\\\" \\\\n\\\\nComparus used solutions from IBM watsonx.aiÂ® and impressively demonstrated the potential of conversational banking as a new interaction model.\\\\n\\\\n Read the case study](https://www.ibm.com/case-studies/comparus-gmbh)\\\\n\\\\nRelated solutions   \\\\n IBMÂ®\\xa0watsonx\\xa0Orchestrateâ„¢\\xa0   \\\\n\\\\nEasily design scalable AI assistants and agents, automate repetitive tasks and simplify complex processes with\\xa0IBMÂ®\\xa0watsonx\\xa0Orchestrateâ„¢.\\\\n\\\\n   [Explore watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)   \\\\n Artificial intelligence solutions   \\\\n\\\\nPut AI to work in your business with IBM\\'s industry-leading AI expertise and portfolio of solutions at your side.\\\\n\\\\n   [Explore AI solutions](https://www.ibm.com/artificial-intelligence)   \\\\n AI consulting and services   \\\\n\\\\nReinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.\\\\n\\\\n   [Explore AI services](https://www.ibm.com/consulting/artificial-intelligence)\\\\n\\\\nTake the next step\\\\n\\\\n \\\\n\\\\nWhether you choose to customize pre-built apps and skills or build and deploy custom agentic services using an AI studio, the IBM watsonx platform has you covered.\\\\n\\\\n    [Explore watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)   [Explore watsonx.ai](https://www.ibm.com/products/watsonx-ai)\"}, {\"url\": \"https://docs.langchain.com/oss/python/langgraph/overview\", \"title\": \"LangGraph overview - Docs by LangChain\", \"content\": \"Trusted by companies shaping the future of agentsâ€” including Klarna, Replit, Elastic, and moreâ€” LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChainâ€™s agents that provide pre-built architectures for common LLM and tool-calling loops. LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more. LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.## LangSmith Agent Server. Contains agent abstractions built on top of LangGraph.\", \"score\": 0.90039873, \"raw_content\": \"[Docs by LangChain home page](/)\\\\n\\\\n[Deep Agents](/oss/python/deepagents/overview)[LangChain](/oss/python/langchain/overview)[LangGraph](/oss/python/langgraph/overview)[Integrations](/oss/python/integrations/providers/overview)[Learn](/oss/python/learn)[Reference](/oss/python/reference/overview)[Contribute](/oss/python/contributing/overview)\\\\n\\\\n* [Overview](/oss/python/langgraph/overview)\\\\n\\\\n##### Get started\\\\n\\\\n* [Install](/oss/python/langgraph/install)\\\\n* [Quickstart](/oss/python/langgraph/quickstart)\\\\n* [Local server](/oss/python/langgraph/local-server)\\\\n* [Changelog](https://docs.langchain.com/oss/python/releases/changelog)\\\\n* [Thinking in LangGraph](/oss/python/langgraph/thinking-in-langgraph)\\\\n* [Workflows + agents](/oss/python/langgraph/workflows-agents)\\\\n\\\\n##### Capabilities\\\\n\\\\n* [Persistence](/oss/python/langgraph/persistence)\\\\n* [Durable execution](/oss/python/langgraph/durable-execution)\\\\n* [Streaming](/oss/python/langgraph/streaming)\\\\n* [Interrupts](/oss/python/langgraph/interrupts)\\\\n* [Time travel](/oss/python/langgraph/use-time-travel)\\\\n* [Memory](/oss/python/langgraph/add-memory)\\\\n* [Subgraphs](/oss/python/langgraph/use-subgraphs)\\\\n\\\\n##### Production\\\\n\\\\n* [Application structure](/oss/python/langgraph/application-structure)\\\\n* [Test](/oss/python/langgraph/test)\\\\n* [LangSmith Studio](/oss/python/langgraph/studio)\\\\n* [Agent Chat UI](/oss/python/langgraph/ui)\\\\n* [LangSmith Deployment](/oss/python/langgraph/deploy)\\\\n* [LangSmith Observability](/oss/python/langgraph/observability)\\\\n\\\\n##### LangGraph APIs\\\\n\\\\n* [Runtime](/oss/python/langgraph/pregel)\\\\n\\\\n* [Install](#install)\\\\n* [Core benefits](#core-benefits)\\\\n* [LangGraph ecosystem](#langgraph-ecosystem)\\\\n* [Acknowledgements](#acknowledgements)\\\\n\\\\n# LangGraph overview\\\\n\\\\nGain control with LangGraph to design agents that reliably handle complex tasks\\\\n\\\\nTrusted by companies shaping the future of agentsâ€” including Klarna, Replit, Elastic, and moreâ€” LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/python/langchain/models) and [tools](/oss/python/langchain/tools). We will commonly use [LangChain](/oss/python/langchain/overview) components throughout the documentation to integrate models and tools, but you donâ€™t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChainâ€™s [agents](/oss/python/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops. LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\\\n\\\\n## [\\u200b](#install) Install\\\\n\\\\nCopy\\\\n\\\\n```\\\\npip install -U langgraph pip  install -U  langgraph\\\\n```\\\\n\\\\nThen, create a simple hello world example:\\\\n\\\\nCopy\\\\n\\\\n```\\\\nfrom langgraph.graph import StateGraph, MessagesState, START, END from langgraph.graph import StateGraph, MessagesState, START, END def mock_llm(state: MessagesState): def  mock_llm(state: MessagesState): return {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"ai\\\\\", \\\\\"content\\\\\": \\\\\"hello world\\\\\"}]}  return {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"ai\\\\\", \\\\\"content\\\\\": \\\\\"hello world\\\\\"}]} graph = StateGraph(MessagesState) graph = StateGraph(MessagesState)graph.add_node(mock_llm)graph.add_node(mock_llm)graph.add_edge(START, \\\\\"mock_llm\\\\\")graph.add_edge(START, \\\\\"mock_llm\\\\\")graph.add_edge(\\\\\"mock_llm\\\\\", END)graph.add_edge(\\\\\"mock_llm\\\\\", END)graph = graph.compile() graph = graph.compile() graph.invoke({\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"hi!\\\\\"}]})graph.invoke({\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"hi!\\\\\"}]})\\\\n```\\\\n\\\\n## [\\u200b](#core-benefits) Core benefits\\\\n\\\\nLangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\\\n\\\\n* [Durable execution](/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\\\n* [Human-in-the-loop](/oss/python/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.\\\\n* [Comprehensive memory](/oss/python/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\\\n* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\\\n* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\\\n\\\\n## [\\u200b](#langgraph-ecosystem) LangGraph ecosystem\\\\n\\\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\\\n\\\\n[## LangSmith\\\\n\\\\nTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.](http://www.langchain.com/langsmith)[## LangSmith Agent Server\\\\n\\\\nDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in Studio.](/langsmith/agent-server)[## LangChain\\\\n\\\\nProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.](/oss/python/langchain/overview)\\\\n\\\\n## [\\u200b](#acknowledgements) Acknowledgements\\\\n\\\\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain. \\\\n\\\\n---\\\\n\\\\n[Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\\\\n\\\\n[Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\\\\n\\\\nWas this page helpful?\\\\n\\\\n[Install LangGraph](/oss/python/langgraph/install)\"}, {\"url\": \"https://www.reddit.com/r/AI_Agents/comments/1l4uq7v/why_use_langgraph/\", \"title\": \"Why use LangGraph? : r/AI_Agents - Reddit\", \"content\": \"LangGraph emphasizes graph-based workflows and state management, making it ideal for complex applications with sophisticated logic and memory persistence.\", \"score\": 0.78314424, \"raw_content\": null}, {\"url\": \"https://www.langchain.com/langgraph\", \"title\": \"LangGraph: Agent Orchestration Framework for Reliable AI Agents\", \"content\": \"[![Image 1](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270df09334914882b88_Frame%209.svg) Observability Debug and monitor in-depth traces](https://www.langchain.com/langsmith/observability)[![Image 2](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270f9e8de1d368764a8_Frame%20206.svg) Evaluation Iterate on prompts and models](https://www.langchain.com/langsmith/evaluation)[![Image 3](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e2709eef27fc61465416_Frame%20100039.svg) Deployment Ship and scale agents in production](https://www.langchain.com/langsmith/deployment)[![Image 4](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/692e7522b3ef627cb0ec1155_Centralized%20management.svg) Agent Builder New Build no-code agents](https://www.langchain.com/langsmith/agent-builder). [![Image 5](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e27023585643370a6471_icons.svg) LangChain Quick start agents with any model provider](https://www.langchain.com/langchain)[![Image 6](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270417338c7f027082d_d035ce400e48f9bc4dd0578d0e3e3211_icons-1.svg) LangGraph Build custom agents with low-level control](https://www.langchain.com/langgraph)[![Image 7](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68f20863b71dbae1af829979_DeepAgents.svg) Deep Agents New Use planning, memory, and sub-agents for complex, long-running tasks](https://docs.langchain.com/oss/python/deepagents/overview#deep-agents-overview). [Start building](https://docs.langchain.com/oss/python/langgraph/overview). ![Image 9](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 10](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 11](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 13](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 14](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 16](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 18](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 19](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 20](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 21](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 23](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 24](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 26](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 28](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 29](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 30](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 31](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 33](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 34](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 36](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 38](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 39](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg). ![Image 40](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg). ![Image 41](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg). ![Image 43](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg). ![Image 44](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg). ![Image 46](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg). ![Image 48](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg). ![Image 49](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b080e4b3ca12dc5d5d439_Langgraph%20UI-2.webp). [Read a conceptual guide](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop). ![Image 50](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c93d559216bb904fe85a8_gif7%20(1).gif). [Learn how to add human-in-the-loop](https://docs.langchain.com/oss/python/langgraph/interrupts). ![Image 51](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68663ca715b9bd5d707bee71_Modified-Human-in-the-loop_white.gif). ![Image 52](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b895f061b6f892568ff6_Modified-Customizable-Agent-Architectures_white.gif). [See different agent architectures](https://docs.langchain.com/oss/python/langgraph/workflows-agents). [Learn about agent memory](https://docs.langchain.com/oss/python/langgraph/add-memory). ![Image 54](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b8b7df5aea00af3a4554_Modified-Streaming-intermediate-steps_white.gif). [See how to use streaming](https://docs.langchain.com/oss/python/langgraph/streaming). [![Image 57](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/678e35d6c553c4fb20f9b753_Frame%2099644.webp)![Image 58](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6787ae0bce5c99dd808545ce_card%202.webp)](https://academy.langchain.com/courses/intro-to-langgraph). ![Image 64](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png). ![Image 70](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png). ![Image 71](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c6a38f9c53ec71f5fc73de_langchain-word.svg).\", \"score\": 0.7730283, \"raw_content\": \"LangGraph: Agent Orchestration Framework for Reliable AI Agents\\\\n===============\\\\n\\\\n[](https://www.langchain.com/)\\\\n\\\\nProducts\\\\n\\\\nLangSmith\\\\n\\\\n[![Image 1](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270df09334914882b88_Frame%209.svg) Observability Debug and monitor in-depth traces](https://www.langchain.com/langsmith/observability)[![Image 2](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270f9e8de1d368764a8_Frame%20206.svg) Evaluation Iterate on prompts and models](https://www.langchain.com/langsmith/evaluation)[![Image 3](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e2709eef27fc61465416_Frame%20100039.svg) Deployment Ship and scale agents in production](https://www.langchain.com/langsmith/deployment)[![Image 4](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/692e7522b3ef627cb0ec1155_Centralized%20management.svg) Agent Builder New Build no-code agents](https://www.langchain.com/langsmith/agent-builder)\\\\n\\\\nOpen Source Frameworks\\\\n\\\\n[![Image 5](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e27023585643370a6471_icons.svg) LangChain Quick start agents with any model provider](https://www.langchain.com/langchain)[![Image 6](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68e8e270417338c7f027082d_d035ce400e48f9bc4dd0578d0e3e3211_icons-1.svg) LangGraph Build custom agents with low-level control](https://www.langchain.com/langgraph)[![Image 7](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68f20863b71dbae1af829979_DeepAgents.svg) Deep Agents New Use planning, memory, and sub-agents for complex, long-running tasks](https://docs.langchain.com/oss/python/deepagents/overview#deep-agents-overview)\\\\n\\\\nLearn\\\\n\\\\nResources\\\\n\\\\n[Blog](https://blog.langchain.com/)[2026 State of Agent Engineering](https://www.langchain.com/state-of-agent-engineering)[Customer Stories](https://www.langchain.com/customers)[Guides](https://www.langchain.com/resources)[Changelog](https://changelog.langchain.com/)[Trust Center](https://trust.langchain.com/)[Support](http://support.langchain.com/)\\\\n\\\\nHow-To\\\\n\\\\n[LangChain Academy](https://academy.langchain.com/)[YouTube](https://www.youtube.com/@LangChain)[Documentation](https://docs.langchain.com/)\\\\n\\\\nCommunity\\\\n\\\\n[LangSmith for Startups](https://www.langchain.com/startups)[Events](https://luma.com/langchain?k=c)[Community](https://www.langchain.com/community)[Community Forum](https://forum.langchain.com/)[Slack](https://www.langchain.com/join-community)\\\\n\\\\n[Docs](https://docs.langchain.com/)\\\\n\\\\nCompany\\\\n\\\\n[About](https://www.langchain.com/about)[Careers](https://www.langchain.com/careers)[Partners](https://www.langchain.com/langchain-partner-network)\\\\n\\\\n[Pricing](https://www.langchain.com/pricing)\\\\n\\\\n[Get a demo](https://www.langchain.com/contact-sales)\\\\n\\\\n[Try LangSmith](https://smith.langchain.com/)\\\\n\\\\nBalance agent control with agency\\\\n=================================\\\\n\\\\nGain control with LangGraph to design agents \\\\n\\\\nthat reliably handle complex tasks.\\\\n\\\\n[Start building](https://docs.langchain.com/oss/python/langgraph/overview)\\\\n\\\\n![Image 8](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66db8c2317fe5b9ad2b84ea0_lcacademylogo.png)\\\\nIntroduction to LangGraph\\\\n-------------------------\\\\n\\\\nLearn the basics of LangGraph in this LangChain Academy Course. You\\'ll learn how to build agents that automate real-world tasks with LangGraph orchestration.\\\\n\\\\n[Enroll for free](https://academy.langchain.com/courses/intro-to-langgraph)[Book enterprise training](https://airtable.com/appGjCAN6126Jm7K8/pagNAp7niHQzRH8zk/form)\\\\n\\\\nTrusted by companies shaping the future of agents\\\\n-------------------------------------------------\\\\n\\\\n[See LangGraph use cases in production](https://www.langchain.com/built-with-langgraph)\\\\n\\\\n![Image 9](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\\\n\\\\n![Image 10](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\\\n\\\\n![Image 11](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\\\n\\\\n![Image 12](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\\\n\\\\n![Image 13](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\\\n\\\\n![Image 14](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\\\n\\\\n![Image 15](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\\\n\\\\n![Image 16](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\\\n\\\\n![Image 17](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\\\n\\\\n![Image 18](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\\\n\\\\n![Image 19](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\\\n\\\\n![Image 20](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\\\n\\\\n![Image 21](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\\\n\\\\n![Image 22](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\\\n\\\\n![Image 23](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\\\n\\\\n![Image 24](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\\\n\\\\n![Image 25](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\\\n\\\\n![Image 26](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\\\n\\\\n![Image 27](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\\\n\\\\n![Image 28](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\\\n\\\\n![Image 29](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\\\n\\\\n![Image 30](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\\\n\\\\n![Image 31](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\\\n\\\\n![Image 32](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\\\n\\\\n![Image 33](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\\\n\\\\n![Image 34](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\\\n\\\\n![Image 35](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\\\n\\\\n![Image 36](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\\\n\\\\n![Image 37](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\\\n\\\\n![Image 38](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\\\n\\\\n![Image 39](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1aa251143166667aec3_logo_Rakuten.svg)\\\\n\\\\n![Image 40](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1d39b2c6c806093f171_GitLab_logo_(2)%201.svg)\\\\n\\\\n![Image 41](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1c1c55df212370b53fd_logo_Elastic.svg)\\\\n\\\\n![Image 42](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ddfdc1291f495b3697_AppFolio%2C_Inc._Wordmark%2C_2021%201.svg)\\\\n\\\\n![Image 43](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/680b77461846a7cf254d8391_Klarna_Logo_black%201.svg)\\\\n\\\\n![Image 44](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/681b568070e9f341ed73b877_logo_Cisco.svg)\\\\n\\\\n![Image 45](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d206eefdb37789451855_The_Home_Depot-Logo.wine%201.svg)\\\\n\\\\n![Image 46](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1e509525a5b39a81208_unify%20logo%20-%20black%201.svg)\\\\n\\\\n![Image 47](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d18a7cef47c38c0eeb48_C._H._Robinson_logo%201.svg)\\\\n\\\\n![Image 48](https://cdn.prod.website-files.com/65c81e88c254bb0f97633a71/6811d1ee0c9249f8c32e645d_logo_Komodo.svg)\\\\n\\\\n![Image 49](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b080e4b3ca12dc5d5d439_Langgraph%20UI-2.webp)\\\\n\\\\nControllable cognitive architecture for any task\\\\n------------------------------------------------\\\\n\\\\nLangGraph\\'s flexible framework supports diverse control flows â€“ single agent, multi-agent, hierarchical, sequential â€“ and robustly handles realistic, complex scenarios. \\\\n\\\\nEnsure reliability with easy-to-add moderation and quality loops that prevent agents from veering off course.\\\\n\\\\nUse LangGraph Platform to templatize your cognitive architecture so that tools, prompts, and models are easily configurable with LangGraph Platform Assistants.\\\\n\\\\n[See the docs](https://langchain-ai.github.io/langgraph/)\\\\n\\\\nThousands of companies build AI apps better with LangChain products.\\\\n--------------------------------------------------------------------\\\\n\\\\nRead our select customer stories.\\\\n\\\\nDesigned for human-agent collaboration\\\\n--------------------------------------\\\\n\\\\nWith built-in statefulness, LangGraph agents seamlessly collaborate with humans by writing drafts for review and awaiting approval before acting. Easily inspect the agentâ€™s actions and \\\\\"time-travel\\\\\" to roll back and take a different action to correct course.\\\\n\\\\n[Read a conceptual guide](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop)\\\\n\\\\n![Image 50](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c93d559216bb904fe85a8_gif7%20(1).gif)\\\\n\\\\nHow does LangGraph help?\\\\n------------------------\\\\n\\\\nGuide, moderate, and control your agent with human-in-the-loop\\\\n--------------------------------------------------------------\\\\n\\\\nPrevent agents from veering off course with easy-to-add moderation and quality controls. Add human-in-the-loop checks to steer and approve agent actions.\\\\n\\\\n[Learn how to add human-in-the-loop](https://docs.langchain.com/oss/python/langgraph/interrupts)\\\\n\\\\n![Image 51](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/68663ca715b9bd5d707bee71_Modified-Human-in-the-loop_white.gif)\\\\n\\\\n![Image 52](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b895f061b6f892568ff6_Modified-Customizable-Agent-Architectures_white.gif)\\\\n\\\\nBuild expressive, customizable agent workflows\\\\n----------------------------------------------\\\\n\\\\nLangGraphâ€™s low-level primitives provide the flexibility needed to create fully customizable agents. Design diverse control flows â€” single, multi-agent, hierarchical â€” all using one framework.\\\\n\\\\n[See different agent architectures](https://docs.langchain.com/oss/python/langgraph/workflows-agents)\\\\n\\\\nPersist context for long-term interactions\\\\n------------------------------------------\\\\n\\\\nLangGraphâ€™s built-in memory stores conversation histories and maintains context over time, enabling rich, personalized interactions across sessions.\\\\n\\\\n[Learn about agent memory](https://docs.langchain.com/oss/python/langgraph/add-memory)\\\\n\\\\n![Image 53](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/684b39acf796422a803c3a03_Memory-GIF-edited.gif)\\\\n\\\\n![Image 54](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6865b8b7df5aea00af3a4554_Modified-Streaming-intermediate-steps_white.gif)\\\\n\\\\nFirst-class streaming for better UX design\\\\n------------------------------------------\\\\n\\\\nBridge user expectations and agent capabilities with native token-by-token streaming, showing agent reasoning and actions in real time.\\\\n\\\\n[See how to use streaming](https://docs.langchain.com/oss/python/langgraph/streaming)\\\\n\\\\n![Image 55](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c57f274b66a77e2a26b82_CleanShot2024-06-26at17.08.03-ezgif.com-video-to-gif-converter.gif)\\\\n\\\\nFirst class streaming support for better UX design\\\\n--------------------------------------------------\\\\n\\\\nBridge user expectations and agent capabilities with native token-by-token streaming and streaming of intermediate steps, helpful for showing agent reasoning and actions back to the user as they happen. Use LangGraph Platform\\'s API to deliver dynamic and interactive user experiences.\\\\n\\\\n[Learn more](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/)\\\\n\\\\n![Image 56](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66db8c2317fe5b9ad2b84ea0_lcacademylogo.png)\\\\n\\\\nIntroduction to LangGraph\\\\n-------------------------\\\\n\\\\nLearn the basics of LangGraph in this LangChain Academy Course. You\\'ll learn how to build agents that automate real-world tasks with LangGraph orchestration.\\\\n\\\\n[Enroll for free](https://academy.langchain.com/courses/intro-to-langgraph)\\\\n\\\\n[![Image 57](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/678e35d6c553c4fb20f9b753_Frame%2099644.webp)![Image 58](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6787ae0bce5c99dd808545ce_card%202.webp)](https://academy.langchain.com/courses/intro-to-langgraph)\\\\n\\\\nDeploy agents at scale, monitor carefully, iterate boldly\\\\n---------------------------------------------------------\\\\n\\\\nDesign agent-driven user experiences with LangGraph Platform\\'s APIs. Quickly deploy and scale your application with infrastructure built for agents. Choose from multiple deployment options.\\\\n\\\\n### Fault-tolerant scalability\\\\n\\\\nHandle large workloads gracefully with horizontally-scaling servers, task queues, and built-in persistence. Enhance resilience with intelligent caching and automated retries.\\\\n\\\\n### Dynamic APIs for designing agent experience\\\\n\\\\nCraft personalized user experiences with APIs featuring long-term memory to recall information across conversation sessions. Track, update, and rewind your app\\'s state for easy human steering and interaction. Kick off long-running background jobs for research-style or multi-step work.\\\\n\\\\n### Integrated developer experience\\\\n\\\\nSimplify prototyping, debugging, and sharing of agents in our visual LangGraph Studio. Deploy your application with 1-click deploy with our SaaS offering or within your own VPC. Then, monitor app performance with LangSmith.\\\\n\\\\n### Developers trust LangGraph to build reliable agents\\\\n\\\\nLangGraph helps teams of all sizes, across all industries, build reliable agents ready for production.\\\\n\\\\n[Hear how industry leaders use LangGraph](https://www.langchain.com/built-with-langgraph)\\\\n\\\\n![Image 59](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308aea1371b447cc4af9_elastic-ar21.png)\\\\n\\\\nâ€œLangChain is streets ahead with what they\\'ve put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.â€\\\\n\\\\n![Image 60](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b26a1b4576291d6a9335b_garrett%20spong%201.webp)\\\\n\\\\nGarrett Spong\\\\n\\\\nPrincipal SWE \\\\n\\\\n![Image 61](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679de9dc4e7bee218d4b058_Norwegian-Cruise-Line-Logo%202-2.webp)\\\\n\\\\nâ€œLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent\\'s thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.â€\\\\n\\\\n![Image 62](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b265bed5f5a9d26d6b7d6_andres%20torres%201.webp)\\\\n\\\\nAndres Torres\\\\n\\\\nSr. Solutions Architect\\\\n\\\\n![Image 63](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e1baf7ea357d0763cde1_ally-bank%201-2.png)\\\\n\\\\nâ€œAs Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments with multi-actor agentic workflows. We are committed to deepening our partnership with LangChain.â€\\\\n\\\\nâ€œAs Ally advances its exploration of Generative AI,\\\\n\\\\n![Image 64](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png)\\\\n\\\\nSathish Muthukrishnan\\\\n\\\\nChief Information, Data and Digital Officer\\\\n\\\\n![Image 65](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308aea1371b447cc4af9_elastic-ar21.png)\\\\n\\\\nâ€œLangChain is streets ahead with what they\\'ve put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.â€\\\\n\\\\n![Image 66](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b26a1b4576291d6a9335b_garrett%20spong%201.webp)\\\\n\\\\nGarrett Spong\\\\n\\\\nPrincipal SWE \\\\n\\\\n![Image 67](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679de9dc4e7bee218d4b058_Norwegian-Cruise-Line-Logo%202-2.webp)\\\\n\\\\nâ€œLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent\\'s thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.â€\\\\n\\\\n![Image 68](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667b265bed5f5a9d26d6b7d6_andres%20torres%201.webp)\\\\n\\\\nAndres Torres\\\\n\\\\nSr. Solutions Architect\\\\n\\\\n![Image 69](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e1baf7ea357d0763cde1_ally-bank%201-2.png)\\\\n\\\\nâ€œAs Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments with multi-actor agentic workflows. We are committed to deepening our partnership with LangChain.â€\\\\n\\\\nâ€œAs Ally advances its exploration of Generative AI,\\\\n\\\\n![Image 70](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6679e2d31352c6bd56c84280_ally.png)\\\\n\\\\nSathish Muthukrishnan\\\\n\\\\nChief Information, Data and Digital Officer\\\\n\\\\nLangGraph FAQs\\\\n--------------\\\\n\\\\nHow is LangGraph different from other agent frameworks?\\\\n\\\\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a companyâ€™s needs. LangGraph provides a more expressive framework to handle companiesâ€™ unique tasks without restricting users to a single black-box cognitive architecture.\\\\n\\\\nDoes LangGraph impact the performance of my app?\\\\n\\\\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\\\\n\\\\nIs LangGraph open source? Is it free?\\\\n\\\\nYes. LangGraph is an MIT-licensed open-source library and is free to use.\\\\n\\\\nReady to start shipping reliable agents faster?\\\\n-----------------------------------------------\\\\n\\\\nGet started with tools from the LangChain product suite for every step of the agent development lifecycle.\\\\n\\\\n[Talk to sales](https://www.langchain.com/contact-sales)[Sign Up](https://smith.langchain.com/)\\\\n\\\\nProducts\\\\n\\\\n[LangChain](https://www.langchain.com/langchain)[LangGraph](https://www.langchain.com/langgraph)[LangSmith Observability](https://www.langchain.com/langsmith/observability)[LangSmith Evaluation](https://www.langchain.com/langsmith/evaluation)[LangSmith Deployment](https://www.langchain.com/langsmith/deployment)\\\\n\\\\nResources\\\\n\\\\n[Guides](https://www.langchain.com/resources)[Blog](https://blog.langchain.com/)[Customer Stories](https://www.langchain.com/customers)[LangChain Academy](https://academy.langchain.com/)[Community](https://www.langchain.com/join-community)[Events](https://lu.ma/langchain)[Changelog](https://changelog.langchain.com/)[Docs](http://docs.langchain.com/)[Support](https://support.langchain.com/)\\\\n\\\\nCompany\\\\n\\\\n[About](https://www.langchain.com/about)[Careers](https://www.langchain.com/careers)[X](https://twitter.com/LangChain)[LinkedIn](https://www.linkedin.com/company/langchain/)[YouTube](https://www.youtube.com/@LangChain)[Marketing Assets](https://drive.google.com/drive/folders/17xybjzmVBdsQA-VxouuGLxF6bDsHDe80?usp=sharing)[Security](https://trust.langchain.com/)\\\\n\\\\nSign up for our newsletter to stay up to date\\\\n\\\\nThank you! Your submission has been received!\\\\n\\\\nOops! Something went wrong while submitting the form.\\\\n\\\\n![Image 71](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c6a38f9c53ec71f5fc73de_langchain-word.svg)\\\\n\\\\n[All systems operational](https://status.smith.langchain.com/)\\\\n\\\\n[Responsible Disclosure Policy](https://www.langchain.com/responsible-disclosure-policy)[Privacy Policy](https://www.langchain.com/privacy-policy)[Terms of Service](https://www.langchain.com/terms-of-service)\"}, {\"url\": \"https://github.com/langchain-ai/langgraph\", \"title\": \"langchain-ai/langgraph: Build resilient language agents as graphs.\", \"content\": \"[Skip to content](https://github.com/langchain-ai/langgraph#start-of-content). [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.Dismiss alert. [MIT license](https://github.com/langchain-ai/langgraph/blob/main/LICENSE). *   [Code](https://github.com/langchain-ai/langgraph). *   [Issues 179](https://github.com/langchain-ai/langgraph/issues). *   [Pull requests 126](https://github.com/langchain-ai/langgraph/pulls). *   [Actions](https://github.com/langchain-ai/langgraph/actions). *   [Projects 0](https://github.com/langchain-ai/langgraph/projects). *   [Security 3](https://github.com/langchain-ai/langgraph/security). *   [Insights](https://github.com/langchain-ai/langgraph/pulse). *   [Code](https://github.com/langchain-ai/langgraph). *   [Issues](https://github.com/langchain-ai/langgraph/issues). *   [Pull requests](https://github.com/langchain-ai/langgraph/pulls). *   [Actions](https://github.com/langchain-ai/langgraph/actions). *   [Projects](https://github.com/langchain-ai/langgraph/projects). *   [Security](https://github.com/langchain-ai/langgraph/security). *   [Insights](https://github.com/langchain-ai/langgraph/pulse). [](https://github.com/langchain-ai/langgraph/branches)[](https://github.com/langchain-ai/langgraph/tags). Please comment here if you encounter any issues\\\\\")[#6720](https://github.com/langchain-ai/langgraph/pull/6720) | Jan 26, 2026 |. *   [README](https://github.com/langchain-ai/langgraph#). *   [Code of conduct](https://github.com/langchain-ai/langgraph#). *   [Contributing](https://github.com/langchain-ai/langgraph#). *   [MIT license](https://github.com/langchain-ai/langgraph#). *   [Security](https://github.com/langchain-ai/langgraph#). [](https://github.com/langchain-ai/langgraph#get-started). [](https://github.com/langchain-ai/langgraph#core-benefits). [](https://github.com/langchain-ai/langgraph#langgraphs-ecosystem). See the [JS repo](https://github.com/langchain-ai/langgraphjs) and the [JS docs](https://docs.langchain.com/oss/javascript/langgraph/overview). [](https://github.com/langchain-ai/langgraph#additional-resources). [](https://github.com/langchain-ai/langgraph#acknowledgements). [Readme](https://github.com/langchain-ai/langgraph#readme-ov-file). [MIT license](https://github.com/langchain-ai/langgraph#MIT-1-ov-file). [Code of conduct](https://github.com/langchain-ai/langgraph#coc-ov-file). [Contributing](https://github.com/langchain-ai/langgraph#contributing-ov-file). [Security policy](https://github.com/langchain-ai/langgraph#security-ov-file). [Please reload this page](https://github.com/langchain-ai/langgraph). [Activity](https://github.com/langchain-ai/langgraph/activity). [Custom properties](https://github.com/langchain-ai/langgraph/custom-properties). [**24.8k** stars](https://github.com/langchain-ai/langgraph/stargazers). [**142** watching](https://github.com/langchain-ai/langgraph/watchers). [**4.3k** forks](https://github.com/langchain-ai/langgraph/forks). [Releases 466](https://github.com/langchain-ai/langgraph/releases). [langgraph==1.0.9 Latest Feb 19, 2026](https://github.com/langchain-ai/langgraph/releases/tag/1.0.9). [+ 465 releases](https://github.com/langchain-ai/langgraph/releases). [Used by 36k](https://github.com/langchain-ai/langgraph/network/dependents). [Contributors 286](https://github.com/langchain-ai/langgraph/graphs/contributors). [+ 272 contributors](https://github.com/langchain-ai/langgraph/graphs/contributors). *   [Python 99.3%](https://github.com/langchain-ai/langgraph/search?l=python).\", \"score\": 0.65146625, \"raw_content\": \"GitHub - langchain-ai/langgraph: Build resilient language agents as graphs.\\\\n===============\\\\n\\\\n[Skip to content](https://github.com/langchain-ai/langgraph#start-of-content)\\\\nNavigation Menu\\\\n---------------\\\\n\\\\nToggle navigation\\\\n\\\\n[](https://github.com/)\\\\n\\\\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flanggraph)\\\\n\\\\nAppearance settings\\\\n\\\\n*   \\\\nPlatform\\\\n\\\\n    *   \\\\nAI CODE CREATION\\\\n        *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)\\\\n        *   [GitHub Spark Build and deploy intelligent apps](https://github.com/features/spark)\\\\n        *   [GitHub Models Manage and compare prompts](https://github.com/features/models)\\\\n        *   [MCP Registry New Integrate external tools](https://github.com/mcp)\\\\n\\\\n    *   \\\\nDEVELOPER WORKFLOWS\\\\n        *   [Actions Automate any workflow](https://github.com/features/actions)\\\\n        *   [Codespaces Instant dev environments](https://github.com/features/codespaces)\\\\n        *   [Issues Plan and track work](https://github.com/features/issues)\\\\n        *   [Code Review Manage code changes](https://github.com/features/code-review)\\\\n\\\\n    *   \\\\nAPPLICATION SECURITY\\\\n        *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)\\\\n        *   [Code security Secure your code as you build](https://github.com/security/advanced-security/code-security)\\\\n        *   [Secret protection Stop leaks before they start](https://github.com/security/advanced-security/secret-protection)\\\\n\\\\n    *   \\\\nEXPLORE\\\\n        *   [Why GitHub](https://github.com/why-github)\\\\n        *   [Documentation](https://docs.github.com/)\\\\n        *   [Blog](https://github.blog/)\\\\n        *   [Changelog](https://github.blog/changelog)\\\\n        *   [Marketplace](https://github.com/marketplace)\\\\n\\\\n[View all features](https://github.com/features)\\\\n\\\\n*   \\\\nSolutions\\\\n\\\\n    *   \\\\nBY COMPANY SIZE\\\\n        *   [Enterprises](https://github.com/enterprise)\\\\n        *   [Small and medium teams](https://github.com/team)\\\\n        *   [Startups](https://github.com/enterprise/startups)\\\\n        *   [Nonprofits](https://github.com/solutions/industry/nonprofits)\\\\n\\\\n    *   \\\\nBY USE CASE\\\\n        *   [App Modernization](https://github.com/solutions/use-case/app-modernization)\\\\n        *   [DevSecOps](https://github.com/solutions/use-case/devsecops)\\\\n        *   [DevOps](https://github.com/solutions/use-case/devops)\\\\n        *   [CI/CD](https://github.com/solutions/use-case/ci-cd)\\\\n        *   [View all use cases](https://github.com/solutions/use-case)\\\\n\\\\n    *   \\\\nBY INDUSTRY\\\\n        *   [Healthcare](https://github.com/solutions/industry/healthcare)\\\\n        *   [Financial services](https://github.com/solutions/industry/financial-services)\\\\n        *   [Manufacturing](https://github.com/solutions/industry/manufacturing)\\\\n        *   [Government](https://github.com/solutions/industry/government)\\\\n        *   [View all industries](https://github.com/solutions/industry)\\\\n\\\\n[View all solutions](https://github.com/solutions)\\\\n\\\\n*   \\\\nResources\\\\n\\\\n    *   \\\\nEXPLORE BY TOPIC\\\\n        *   [AI](https://github.com/resources/articles?topic=ai)\\\\n        *   [Software Development](https://github.com/resources/articles?topic=software-development)\\\\n        *   [DevOps](https://github.com/resources/articles?topic=devops)\\\\n        *   [Security](https://github.com/resources/articles?topic=security)\\\\n        *   [View all topics](https://github.com/resources/articles)\\\\n\\\\n    *   \\\\nEXPLORE BY TYPE\\\\n        *   [Customer stories](https://github.com/customer-stories)\\\\n        *   [Events & webinars](https://github.com/resources/events)\\\\n        *   [Ebooks & reports](https://github.com/resources/whitepapers)\\\\n        *   [Business insights](https://github.com/solutions/executive-insights)\\\\n        *   [GitHub Skills](https://skills.github.com/)\\\\n\\\\n    *   \\\\nSUPPORT & SERVICES\\\\n        *   [Documentation](https://docs.github.com/)\\\\n        *   [Customer support](https://support.github.com/)\\\\n        *   [Community forum](https://github.com/orgs/community/discussions)\\\\n        *   [Trust center](https://github.com/trust-center)\\\\n        *   [Partners](https://github.com/partners)\\\\n\\\\n*   \\\\nOpen Source\\\\n\\\\n    *   \\\\nCOMMUNITY\\\\n        *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)\\\\n\\\\n    *   \\\\nPROGRAMS\\\\n        *   [Security Lab](https://securitylab.github.com/)\\\\n        *   [Maintainer Community](https://maintainers.github.com/)\\\\n        *   [Accelerator](https://github.com/accelerator)\\\\n        *   [Archive Program](https://archiveprogram.github.com/)\\\\n\\\\n    *   \\\\nREPOSITORIES\\\\n        *   [Topics](https://github.com/topics)\\\\n        *   [Trending](https://github.com/trending)\\\\n        *   [Collections](https://github.com/collections)\\\\n\\\\n*   \\\\nEnterprise\\\\n\\\\n    *   \\\\nENTERPRISE SOLUTIONS\\\\n        *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)\\\\n\\\\n    *   \\\\nAVAILABLE ADD-ONS\\\\n        *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)\\\\n        *   [Copilot for Business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)\\\\n        *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)\\\\n\\\\n*   [Pricing](https://github.com/pricing)\\\\n\\\\nSearch or jump to...\\\\n\\\\nSearch code, repositories, users, issues, pull requests...\\\\n==========================================================\\\\n\\\\n Search  \\\\n\\\\nClear\\\\n\\\\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\\\\n\\\\nProvide feedback\\\\n================\\\\n\\\\nWe read every piece of feedback, and take your input very seriously.\\\\n\\\\n- [x] Include my email address so I can be contacted \\\\n\\\\n Cancel  Submit feedback \\\\n\\\\nSaved searches\\\\n==============\\\\n\\\\nUse saved searches to filter your results more quickly\\\\n------------------------------------------------------\\\\n\\\\nName \\\\n\\\\nQuery \\\\n\\\\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\\\\n\\\\n Cancel  Create saved search \\\\n\\\\n[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flanggraph)\\\\n\\\\n[Sign up](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=langchain-ai%2Flanggraph)\\\\n\\\\nAppearance settings\\\\n\\\\nResetting focus\\\\n\\\\nYou signed in with another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/langchain-ai/langgraph) to refresh your session.Dismiss alert\\\\n\\\\n{{ message }}\\\\n\\\\n[langchain-ai](https://github.com/langchain-ai)/**[langgraph](https://github.com/langchain-ai/langgraph)**Public\\\\n\\\\n*   [Notifications](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)You must be signed in to change notification settings\\\\n*   [Fork 4.3k](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)\\\\n*   [Star 24.8k](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph) \\\\n\\\\nBuild resilient language agents as graphs.\\\\n\\\\n[docs.langchain.com/oss/python/langgraph/](https://docs.langchain.com/oss/python/langgraph/ \\\\\"https://docs.langchain.com/oss/python/langgraph/\\\\\")\\\\n\\\\n### License\\\\n\\\\n[MIT license](https://github.com/langchain-ai/langgraph/blob/main/LICENSE)\\\\n\\\\n[24.8k stars](https://github.com/langchain-ai/langgraph/stargazers)[4.3k forks](https://github.com/langchain-ai/langgraph/forks)[Branches](https://github.com/langchain-ai/langgraph/branches)[Tags](https://github.com/langchain-ai/langgraph/tags)[Activity](https://github.com/langchain-ai/langgraph/activity)\\\\n\\\\n[Star](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)\\\\n\\\\n[Notifications](https://github.com/login?return_to=%2Flangchain-ai%2Flanggraph)You must be signed in to change notification settings\\\\n\\\\n*   [Code](https://github.com/langchain-ai/langgraph)\\\\n*   [Issues 179](https://github.com/langchain-ai/langgraph/issues)\\\\n*   [Pull requests 126](https://github.com/langchain-ai/langgraph/pulls)\\\\n*   [Actions](https://github.com/langchain-ai/langgraph/actions)\\\\n*   [Projects 0](https://github.com/langchain-ai/langgraph/projects)\\\\n*   [Security 3](https://github.com/langchain-ai/langgraph/security)\\\\n*   [Insights](https://github.com/langchain-ai/langgraph/pulse)\\\\n\\\\nAdditional navigation options\\\\n\\\\n*   [Code](https://github.com/langchain-ai/langgraph)\\\\n*   [Issues](https://github.com/langchain-ai/langgraph/issues)\\\\n*   [Pull requests](https://github.com/langchain-ai/langgraph/pulls)\\\\n*   [Actions](https://github.com/langchain-ai/langgraph/actions)\\\\n*   [Projects](https://github.com/langchain-ai/langgraph/projects)\\\\n*   [Security](https://github.com/langchain-ai/langgraph/security)\\\\n*   [Insights](https://github.com/langchain-ai/langgraph/pulse)\\\\n\\\\nlangchain-ai/langgraph\\\\n======================\\\\n\\\\nmain\\\\n\\\\n[**206**Branches](https://github.com/langchain-ai/langgraph/branches)[**477**Tags](https://github.com/langchain-ai/langgraph/tags)\\\\n\\\\n[](https://github.com/langchain-ai/langgraph/branches)[](https://github.com/langchain-ai/langgraph/tags)\\\\n\\\\nGo to file\\\\n\\\\nCode\\\\n\\\\nOpen more actions menu\\\\n\\\\nFolders and files\\\\n-----------------\\\\n\\\\n| Name | Name | Last commit message | Last commit date |\\\\n| --- | --- | --- | --- |\\\\n| Latest commit ------------- ![Image 1: jkennedyvz](https://avatars.githubusercontent.com/u/65985482?v=4&size=40)![Image 2: claude](https://avatars.githubusercontent.com/u/81847?v=4&size=40) [John Kennedy (jkennedyvz)](https://github.com/langchain-ai/langgraph/commits?author=jkennedyvz) and [Claude (claude)](https://github.com/langchain-ai/langgraph/commits?author=claude) [fix: bump js-yaml to 3.14.2 to resolve](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9)[CVE-2025-64718](https://github.com/advisories/GHSA-mh29-5h37-fv8m \\\\\"CVE-2025-64718\\\\\")[(](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9)[#6879](https://github.com/langchain-ai/langgraph/pull/6879)[)](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9) Open commit details success Feb 19, 2026 [ea20432](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9)Â·Feb 19, 2026 History ------- [6,521 Commits](https://github.com/langchain-ai/langgraph/commits/main/) Open commit details [](https://github.com/langchain-ai/langgraph/commits/main/)6,521 Commits |\\\\n| [.github](https://github.com/langchain-ai/langgraph/tree/main/.github \\\\\".github\\\\\") | [.github](https://github.com/langchain-ai/langgraph/tree/main/.github \\\\\".github\\\\\") | [chore(docs): Add new redirects file(s) so we can update/add (](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\\\\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\\\\")[#6778](https://github.com/langchain-ai/langgraph/pull/6778)[)](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\\\\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\\\\") | Feb 18, 2026 |\\\\n| [docs](https://github.com/langchain-ai/langgraph/tree/main/docs \\\\\"docs\\\\\") | [docs](https://github.com/langchain-ai/langgraph/tree/main/docs \\\\\"docs\\\\\") | [chore(docs): Add new redirects file(s) so we can update/add (](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\\\\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\\\\")[#6778](https://github.com/langchain-ai/langgraph/pull/6778)[)](https://github.com/langchain-ai/langgraph/commit/c4f58611660543bf9341232eb971551654dcf5d0 \\\\\"chore(docs): Add new redirects file(s) so we can update/add (#6778) Adds redirects for all old LangGraph docs URLs to docs.langchain.com using meta refresh and GitHub Pages\\\\\") | Feb 18, 2026 |\\\\n| [examples](https://github.com/langchain-ai/langgraph/tree/main/examples \\\\\"examples\\\\\") | [examples](https://github.com/langchain-ai/langgraph/tree/main/examples \\\\\"examples\\\\\") | [docs: update notebook links and add archival notices for examples (](https://github.com/langchain-ai/langgraph/commit/fbcb8a911b626b4373fd4aff9624124e9532987f \\\\\"docs: update notebook links and add archival notices for examples (#6720) Addresses some comments in #6682 - Update links in notebooks to point to the new documentation location. - Add archival notices indicating that the examples are no longer updated. - Remove some obsolete notebooks that have been moved to the new documentation. Please comment here if you encounter any issues\\\\\")[#6720](https://github.com/langchain-ai/langgraph/pull/6720) | Jan 26, 2026 |\\\\n| [libs](https://github.com/langchain-ai/langgraph/tree/main/libs \\\\\"libs\\\\\") | [libs](https://github.com/langchain-ai/langgraph/tree/main/libs \\\\\"libs\\\\\") | [fix: bump js-yaml to 3.14.2 to resolve](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9 \\\\\"fix: bump js-yaml to 3.14.2 to resolve CVE-2025-64718 (#6879) ## Security Alert Patch Resolves 1 Dependabot security alert (medium severity). ### Package Updated | Package | Old Version | New Version | Strategy | CVE Resolved | |---------|-------------|-------------|----------|--------------| | `js-yaml` | 3.14.1 | 3.14.2 | Lockfile patch (within-range bump) | CVE-2025-64718 | ### CVE Details **CVE-2025-64718** / [GHSA-mh29-5h37-fv8m](https://github.com/advisories/GHSA-mh29-5h37-fv8m) â€” `js-yaml` prototype pollution via YAML merge keys (`<<`). Affects versions < 3.14.2. The vulnerable package is a transitive dev dependency pulled in by `@istanbuljs/load-nyc-config@1.1.0` (a Jest internal). No runtime impact. ### Fix Strategy Lockfile-only patch in `libs/cli/js-examples/yarn.lock`. The `^3.13.1` version range already allows 3.14.2, so no manifest changes were needed. The `js-yaml@^4.1.1` entry (used by `@eslint/eslintrc`) is untouched. ### Verification - [x] Lockfile updated â€” `js-yaml@^3.13.1` now resolves to `3.14.2` - [x] `js-yaml@^4.1.1` entry unchanged (`4.1.1`) - [x] `yarn install --frozen-lockfile` passes ðŸ¤– Submitted by langster-patch Co-authored-by: Claude Sonnet 4.6 <noreply@anthropic.com>\\\\\")[CVE-2025-64718](https://github.com/advisories/GHSA-mh29-5h37-fv8m \\\\\"CVE-2025-64718\\\\\")[(](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9 \\\\\"fix: bump js-yaml to 3.14.2 to resolve CVE-2025-64718 (#6879) ## Security Alert Patch Resolves 1 Dependabot security alert (medium severity). ### Package Updated | Package | Old Version | New Version | Strategy | CVE Resolved | |---------|-------------|-------------|----------|--------------| | `js-yaml` | 3.14.1 | 3.14.2 | Lockfile patch (within-range bump) | CVE-2025-64718 | ### CVE Details **CVE-2025-64718** / [GHSA-mh29-5h37-fv8m](https://github.com/advisories/GHSA-mh29-5h37-fv8m) â€” `js-yaml` prototype pollution via YAML merge keys (`<<`). Affects versions < 3.14.2. The vulnerable package is a transitive dev dependency pulled in by `@istanbuljs/load-nyc-config@1.1.0` (a Jest internal). No runtime impact. ### Fix Strategy Lockfile-only patch in `libs/cli/js-examples/yarn.lock`. The `^3.13.1` version range already allows 3.14.2, so no manifest changes were needed. The `js-yaml@^4.1.1` entry (used by `@eslint/eslintrc`) is untouched. ### Verification - [x] Lockfile updated â€” `js-yaml@^3.13.1` now resolves to `3.14.2` - [x] `js-yaml@^4.1.1` entry unchanged (`4.1.1`) - [x] `yarn install --frozen-lockfile` passes ðŸ¤– Submitted by langster-patch Co-authored-by: Claude Sonnet 4.6 <noreply@anthropic.com>\\\\\")[#6879](https://github.com/langchain-ai/langgraph/pull/6879)[)](https://github.com/langchain-ai/langgraph/commit/ea20432b9b2be74ca7f33a6ef4c820cb093b12b9 \\\\\"fix: bump js-yaml to 3.14.2 to resolve CVE-2025-64718 (#6879) ## Security Alert Patch Resolves 1 Dependabot security alert (medium severity). ### Package Updated | Package | Old Version | New Version | Strategy | CVE Resolved | |---------|-------------|-------------|----------|--------------| | `js-yaml` | 3.14.1 | 3.14.2 | Lockfile patch (within-range bump) | CVE-2025-64718 | ### CVE Details **CVE-2025-64718** / [GHSA-mh29-5h37-fv8m](https://github.com/advisories/GHSA-mh29-5h37-fv8m) â€” `js-yaml` prototype pollution via YAML merge keys (`<<`). Affects versions < 3.14.2. The vulnerable package is a transitive dev dependency pulled in by `@istanbuljs/load-nyc-config@1.1.0` (a Jest internal). No runtime impact. ### Fix Strategy Lockfile-only patch in `libs/cli/js-examples/yarn.lock`. The `^3.13.1` version range already allows 3.14.2, so no manifest changes were needed. The `js-yaml@^4.1.1` entry (used by `@eslint/eslintrc`) is untouched. ### Verification - [x] Lockfile updated â€” `js-yaml@^3.13.1` now resolves to `3.14.2` - [x] `js-yaml@^4.1.1` entry unchanged (`4.1.1`) - [x] `yarn install --frozen-lockfile` passes ðŸ¤– Submitted by langster-patch Co-authored-by: Claude Sonnet 4.6 <noreply@anthropic.com>\\\\\") | Feb 19, 2026 |\\\\n| [.gitignore](https://github.com/langchain-ai/langgraph/blob/main/.gitignore \\\\\".gitignore\\\\\") | [.gitignore](https://github.com/langchain-ai/langgraph/blob/main/.gitignore \\\\\".gitignore\\\\\") | [chore: more cleanup (](https://github.com/langchain-ai/langgraph/commit/d066a321bc5a6ca08c2532873a2759cdce1c4d0a \\\\\"chore: more cleanup (#6667)\\\\\")[#6667](https://github.com/langchain-ai/langgraph/pull/6667)[)](https://github.com/langchain-ai/langgraph/commit/d066a321bc5a6ca08c2532873a2759cdce1c4d0a \\\\\"chore: more cleanup (#6667)\\\\\") | Jan 9, 2026 |\\\\n| [AGENTS.md](https://github.com/langchain-ai/langgraph/blob/main/AGENTS.md \\\\\"AGENTS.md\\\\\") | [AGENTS.md](https://github.com/langchain-ai/langgraph/blob/main/AGENTS.md \\\\\"AGENTS.md\\\\\") | [chore(infra): update `AGENTS.md` for inline code formatting guidelines (](https://github.com/langchain-ai/langgraph/commit/f688b068e7f06111fc69ee70adc1b66c75fa5a93 \\\\\"chore(infra): update `AGENTS.md` for inline code formatting guidelines (#6752)\\\\\") | Feb 5, 2026 |\\\\n| [CLAUDE.md](https://github.com/langchain-ai/langgraph/blob/main/CLAUDE.md \\\\\"CLAUDE.md\\\\\") | [CLAUDE.md](https://github.com/langchain-ai/langgraph/blob/main/CLAUDE.md \\\\\"CLAUDE.md\\\\\") | [chore(infra): update `AGENTS.md` for inline code formatting guidelines (](https://github.com/langchain-ai/langgraph/commit/f688b068e7f06111fc69ee70adc1b66c75fa5a93 \\\\\"chore(infra): update `AGENTS.md` for inline code formatting guidelines (#6752)\\\\\") | Feb 5, 2026 |\\\\n| [LICENSE](https://github.com/langchain-ai/langgraph/blob/main/LICENSE \\\\\"LICENSE\\\\\") | [LICENSE](https://github.com/langchain-ai/langgraph/blob/main/LICENSE \\\\\"LICENSE\\\\\") | [libs: add cli, sdk-py, sdk-js and move core langgraph](https://github.com/langchain-ai/langgraph/commit/1a06d500d4282cfdb2ae9d7748bb570e8162acdf \\\\\"libs: add cli, sdk-py, sdk-js and move core langgraph\\\\\") | Jun 18, 2024 |\\\\n| [Makefile](https://github.com/langchain-ai/langgraph/blob/main/Makefile \\\\\"Makefile\\\\\") | [Makefile](https://github.com/langchain-ai/langgraph/blob/main/Makefile \\\\\"Makefile\\\\\") | [ci: add automated](https://github.com/langchain-ai/langgraph/commit/b3708bd7f66675f7a04bf9950c33d495c49cd843 \\\\\"ci: add automated `uv lock --upgrade` workflow (#5307)\\\\\")`uv lock --upgrade`[workflow (](https://github.com/langchain-ai/langgraph/commit/b3708bd7f66675f7a04bf9950c33d495c49cd843 \\\\\"ci: add automated `uv lock --upgrade` workflow (#5307)\\\\\")[#5307](https://github.com/langchain-ai/langgraph/pull/5307)[)](https://github.com/langchain-ai/langgraph/commit/b3708bd7f66675f7a04bf9950c33d495c49cd843 \\\\\"ci: add automated `uv lock --upgrade` workflow (#5307)\\\\\") | Jul 2, 2025 |\\\\n| [README.md](https://github.com/langchain-ai/langgraph/blob/main/README.md \\\\\"README.md\\\\\") | [README.md](https://github.com/langchain-ai/langgraph/blob/main/README.md \\\\\"README.md\\\\\") | [chore(docs): Update link for LangGraph guides in README (](https://github.com/langchain-ai/langgraph/commit/8cb87eaf7637ca591adc9715fc412e6661e4a77d \\\\\"chore(docs): Update link for LangGraph guides in README (#6680) https://langchain.slack.com/archives/C04GWPE38LV/p1768251150861949\\\\\")[#6680](https://github.com/langchain-ai/langgraph/pull/6680)[)](https://github.com/langchain-ai/langgraph/commit/8cb87eaf7637ca591adc9715fc412e6661e4a77d \\\\\"chore(docs): Update link for LangGraph guides in README (#6680) https://langchain.slack.com/archives/C04GWPE38LV/p1768251150861949\\\\\") | Jan 13, 2026 |\\\\n| View all files |\\\\n\\\\nRepository files navigation\\\\n---------------------------\\\\n\\\\n*   [README](https://github.com/langchain-ai/langgraph#)\\\\n*   [Code of conduct](https://github.com/langchain-ai/langgraph#)\\\\n*   [Contributing](https://github.com/langchain-ai/langgraph#)\\\\n*   [MIT license](https://github.com/langchain-ai/langgraph#)\\\\n*   [Security](https://github.com/langchain-ai/langgraph#)\\\\n\\\\n![Image 3: LangGraph Logo](https://camo.githubusercontent.com/35c8690644d21b455613e70f617eee193be16e02684824dc60be8ab1216eed6c/68747470733a2f2f6c616e67636861696e2d61692e6769746875622e696f2f6c616e6767726170682f7374617469632f776f72646d61726b5f6461726b2e737667)\\\\n\\\\n[![Image 4: Version](https://camo.githubusercontent.com/28715b4724dc05f3ffb3a0cda4069876d0e8ada606267c944bc83639b3ebe5ff/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c616e6767726170682e737667)](https://pypi.org/project/langgraph/)[![Image 5: Downloads](https://camo.githubusercontent.com/81ef74dbbdd86537708ac52c5757df06b12129ea96d7aa92254c8276fee2b3fa/68747470733a2f2f7374617469632e706570792e746563682f62616467652f6c616e6767726170682f6d6f6e7468)](https://pepy.tech/project/langgraph)[![Image 6: Open Issues](https://camo.githubusercontent.com/bcb848d635a6a73f5c1c2be97875ac1369539d5a025be181e6d1f6125933b2d1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6c616e67636861696e2d61692f6c616e676772617068)](https://github.com/langchain-ai/langgraph/issues)[![Image 7: Docs](https://camo.githubusercontent.com/b98c4ce4549448d09f2217965c7d6f2cf39ee6800b2b4c63dfd62080fb5533d8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c7565)](https://docs.langchain.com/oss/python/langgraph/overview)\\\\n\\\\nTrusted by companies shaping the future of agents â€“ including Klarna, Replit, Elastic, and more â€“ LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents.\\\\n\\\\nGet started\\\\n-----------\\\\n\\\\n[](https://github.com/langchain-ai/langgraph#get-started)\\\\n\\\\nInstall LangGraph:\\\\n\\\\n```\\\\npip install -U langgraph\\\\n```\\\\n\\\\nCreate a simple workflow:\\\\n\\\\nundefinedpython\\\\nfrom langgraph.graph import START, StateGraph\\\\nfrom typing_extensions import TypedDict\\\\n\\\\nclass State(TypedDict):\\\\n    text: str\\\\n\\\\ndef node_a(state: State) -> dict:\\\\n    return {\\\\\"text\\\\\": state[\\\\\"text\\\\\"] + \\\\\"a\\\\\"}\\\\n\\\\ndef node_b(state: State) -> dict:\\\\n    return {\\\\\"text\\\\\": state[\\\\\"text\\\\\"] + \\\\\"b\\\\\"}\\\\n\\\\ngraph = StateGraph(State)\\\\ngraph.add_node(\\\\\"node_a\\\\\", node_a)\\\\ngraph.add_node(\\\\\"node_b\\\\\", node_b)\\\\ngraph.add_edge(START, \\\\\"node_a\\\\\")\\\\ngraph.add_edge(\\\\\"node_a\\\\\", \\\\\"node_b\\\\\")\\\\n\\\\nprint(graph.compile().invoke({\\\\\"text\\\\\": \\\\\"\\\\\"}))\\\\n# {\\'text\\': \\'ab\\'}\\\\nundefined\\\\n\\\\nGet started with the [LangGraph Quickstart](https://docs.langchain.com/oss/python/langgraph/quickstart).\\\\n\\\\nTo quickly build agents with LangChain\\'s `create_agent` (built on LangGraph), see the [LangChain Agents documentation](https://docs.langchain.com/oss/python/langchain/agents).\\\\n\\\\nCore benefits\\\\n-------------\\\\n\\\\n[](https://github.com/langchain-ai/langgraph#core-benefits)\\\\n\\\\nLangGraph provides low-level supporting infrastructure for _any_ long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\\\n\\\\n*   [Durable execution](https://docs.langchain.com/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, automatically resuming from exactly where they left off.\\\\n*   [Human-in-the-loop](https://docs.langchain.com/oss/python/langgraph/interrupts): Seamlessly incorporate human oversight by inspecting and modifying agent state at any point during execution.\\\\n*   [Comprehensive memory](https://docs.langchain.com/oss/python/langgraph/memory): Create truly stateful agents with both short-term working memory for ongoing reasoning and long-term persistent memory across sessions.\\\\n*   [Debugging with LangSmith](http://www.langchain.com/langsmith): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\\\n*   [Production-ready deployment](https://docs.langchain.com/langsmith/app-development): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\\\n\\\\nLangGraphâ€™s ecosystem\\\\n---------------------\\\\n\\\\n[](https://github.com/langchain-ai/langgraph#langgraphs-ecosystem)\\\\n\\\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\\\n\\\\n*   [LangSmith](http://www.langchain.com/langsmith) â€” Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\\\n*   [LangSmith Deployment](https://docs.langchain.com/langsmith/deployments) â€” Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in [LangGraph Studio](https://docs.langchain.com/oss/python/langgraph/studio).\\\\n*   [LangChain](https://docs.langchain.com/oss/python/langchain/overview) â€“ Provides integrations and composable components to streamline LLM application development.\\\\n\\\\nNote\\\\n\\\\nLooking for the JS version of LangGraph? See the [JS repo](https://github.com/langchain-ai/langgraphjs) and the [JS docs](https://docs.langchain.com/oss/javascript/langgraph/overview).\\\\n\\\\nAdditional resources\\\\n--------------------\\\\n\\\\n[](https://github.com/langchain-ai/langgraph#additional-resources)\\\\n\\\\n*   [Guides](https://docs.langchain.com/oss/python/langgraph/overview): Quick, actionable code snippets for topics such as streaming, adding memory & persistence, and design patterns (e.g. branching, subgraphs, etc.).\\\\n*   [Reference](https://reference.langchain.com/python/langgraph/): Detailed reference on core classes, methods, how to use the graph and checkpointing APIs, and higher-level prebuilt components.\\\\n*   [Examples](https://docs.langchain.com/oss/python/langgraph/agentic-rag): Guided examples on getting started with LangGraph.\\\\n*   [LangChain Forum](https://forum.langchain.com/): Connect with the community and share all of your technical questions, ideas, and feedback.\\\\n*   [LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph): Learn the basics of LangGraph in our free, structured course.\\\\n*   [Case studies](https://www.langchain.com/built-with-langgraph): Hear how industry leaders use LangGraph to ship AI applications at scale.\\\\n\\\\nAcknowledgements\\\\n----------------\\\\n\\\\n[](https://github.com/langchain-ai/langgraph#acknowledgements)\\\\n\\\\nLangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\\\n\\\\nAbout\\\\n-----\\\\n\\\\nBuild resilient language agents as graphs.\\\\n\\\\n[docs.langchain.com/oss/python/langgraph/](https://docs.langchain.com/oss/python/langgraph/ \\\\\"https://docs.langchain.com/oss/python/langgraph/\\\\\")\\\\n\\\\n### Topics\\\\n\\\\n[python](https://github.com/topics/python \\\\\"Topic: python\\\\\")[open-source](https://github.com/topics/open-source \\\\\"Topic: open-source\\\\\")[enterprise](https://github.com/topics/enterprise \\\\\"Topic: enterprise\\\\\")[framework](https://github.com/topics/framework \\\\\"Topic: framework\\\\\")[ai](https://github.com/topics/ai \\\\\"Topic: ai\\\\\")[gemini](https://github.com/topics/gemini \\\\\"Topic: gemini\\\\\")[openai](https://github.com/topics/openai \\\\\"Topic: openai\\\\\")[multiagent](https://github.com/topics/multiagent \\\\\"Topic: multiagent\\\\\")[agents](https://github.com/topics/agents \\\\\"Topic: agents\\\\\")[ai-agents](https://github.com/topics/ai-agents \\\\\"Topic: ai-agents\\\\\")[rag](https://github.com/topics/rag \\\\\"Topic: rag\\\\\")[pydantic](https://github.com/topics/pydantic \\\\\"Topic: pydantic\\\\\")[llm](https://github.com/topics/llm \\\\\"Topic: llm\\\\\")[generative-ai](https://github.com/topics/generative-ai \\\\\"Topic: generative-ai\\\\\")[chatgpt](https://github.com/topics/chatgpt \\\\\"Topic: chatgpt\\\\\")[langchain](https://github.com/topics/langchain \\\\\"Topic: langchain\\\\\")[langgraph](https://github.com/topics/langgraph \\\\\"Topic: langgraph\\\\\")[deepagents](https://github.com/topics/deepagents \\\\\"Topic: deepagents\\\\\")\\\\n\\\\n### Resources\\\\n\\\\n[Readme](https://github.com/langchain-ai/langgraph#readme-ov-file)\\\\n\\\\n### License\\\\n\\\\n[MIT license](https://github.com/langchain-ai/langgraph#MIT-1-ov-file)\\\\n\\\\n### Code of conduct\\\\n\\\\n[Code of conduct](https://github.com/langchain-ai/langgraph#coc-ov-file)\\\\n\\\\n### Contributing\\\\n\\\\n[Contributing](https://github.com/langchain-ai/langgraph#contributing-ov-file)\\\\n\\\\n### Security policy\\\\n\\\\n[Security policy](https://github.com/langchain-ai/langgraph#security-ov-file)\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. [Please reload this page](https://github.com/langchain-ai/langgraph).\\\\n\\\\n[Activity](https://github.com/langchain-ai/langgraph/activity)\\\\n\\\\n[Custom properties](https://github.com/langchain-ai/langgraph/custom-properties)\\\\n\\\\n### Stars\\\\n\\\\n[**24.8k** stars](https://github.com/langchain-ai/langgraph/stargazers)\\\\n\\\\n### Watchers\\\\n\\\\n[**142** watching](https://github.com/langchain-ai/langgraph/watchers)\\\\n\\\\n### Forks\\\\n\\\\n[**4.3k** forks](https://github.com/langchain-ai/langgraph/forks)\\\\n\\\\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flanggraph&report=langchain-ai+%28user%29)\\\\n\\\\n[Releases 466](https://github.com/langchain-ai/langgraph/releases)\\\\n------------------------------------------------------------------\\\\n\\\\n[langgraph==1.0.9 Latest Feb 19, 2026](https://github.com/langchain-ai/langgraph/releases/tag/1.0.9)\\\\n\\\\n[+ 465 releases](https://github.com/langchain-ai/langgraph/releases)\\\\n\\\\n[Used by 36k](https://github.com/langchain-ai/langgraph/network/dependents)\\\\n---------------------------------------------------------------------------\\\\n\\\\n[* ![Image 8: @ry86pkqf74-rgb](https://avatars.githubusercontent.com/u/245083510?s=64&v=4) * ![Image 9: @SURESHBEEKHANI](https://avatars.githubusercontent.com/u/107859372?s=64&v=4) * ![Image 10: @scaleapi](https://avatars.githubusercontent.com/u/21693938?s=64&v=4) * ![Image 11: @rahulsharmavishwakarma](https://avatars.githubusercontent.com/u/93391253?s=64&v=4) * ![Image 12: @Konscig](https://avatars.githubusercontent.com/u/71553656?s=64&v=4) * ![Image 13: @asadullah48](https://avatars.githubusercontent.com/u/161634653?s=64&v=4) * ![Image 14: @Ontotext-AD](https://avatars.githubusercontent.com/u/6817639?s=64&v=4) * ![Image 15: @jason-jaemin-lee](https://avatars.githubusercontent.com/u/226609390?s=64&v=4) + 36,021](https://github.com/langchain-ai/langgraph/network/dependents)\\\\n\\\\n[Contributors 286](https://github.com/langchain-ai/langgraph/graphs/contributors)\\\\n---------------------------------------------------------------------------------\\\\n\\\\n*   [![Image 16: @nfcampos](https://avatars.githubusercontent.com/u/56902?s=64&v=4)](https://github.com/nfcampos)\\\\n*   [![Image 17: @hinthornw](https://avatars.githubusercontent.com/u/13333726?s=64&v=4)](https://github.com/hinthornw)\\\\n*   [![Image 18: @dqbd](https://avatars.githubusercontent.com/u/1443449?s=64&v=4)](https://github.com/dqbd)\\\\n*   [![Image 19: @eyurtsev](https://avatars.githubusercontent.com/u/3205522?s=64&v=4)](https://github.com/eyurtsev)\\\\n*   [![Image 20: @sydney-runkle](https://avatars.githubusercontent.com/u/54324534?s=64&v=4)](https://github.com/sydney-runkle)\\\\n*   [![Image 21: @andrewnguonly](https://avatars.githubusercontent.com/u/7654246?s=64&v=4)](https://github.com/andrewnguonly)\\\\n*   [![Image 22: @hwchase17](https://avatars.githubusercontent.com/u/11986836?s=64&v=4)](https://github.com/hwchase17)\\\\n*   [![Image 23: @isahers1](https://avatars.githubusercontent.com/u/78627776?s=64&v=4)](https://github.com/isahers1)\\\\n*   [![Image 24: @lnhsingh](https://avatars.githubusercontent.com/u/15386648?s=64&v=4)](https://github.com/lnhsingh)\\\\n*   [![Image 25: @rlancemartin](https://avatars.githubusercontent.com/u/122662504?s=64&v=4)](https://github.com/rlancemartin)\\\\n*   [![Image 26: @ccurme](https://avatars.githubusercontent.com/u/26529506?s=64&v=4)](https://github.com/ccurme)\\\\n*   [![Image 27: @dependabot[bot]](https://avatars.githubusercontent.com/in/29110?s=64&v=4)](https://github.com/apps/dependabot)\\\\n*   [![Image 28: @bracesproul](https://avatars.githubusercontent.com/u/46789226?s=64&v=4)](https://github.com/bracesproul)\\\\n*   [![Image 29: @lc-arjun](https://avatars.githubusercontent.com/u/185099244?s=64&v=4)](https://github.com/lc-arjun)\\\\n\\\\n[+ 272 contributors](https://github.com/langchain-ai/langgraph/graphs/contributors)\\\\n\\\\nLanguages\\\\n---------\\\\n\\\\n*   [Python 99.3%](https://github.com/langchain-ai/langgraph/search?l=python)\\\\n*   Other 0.7%\\\\n\\\\nFooter\\\\n------\\\\n\\\\n[](https://github.com/) Â© 2026 GitHub,Inc. \\\\n\\\\n### Footer navigation\\\\n\\\\n*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\\\\n*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\\\\n*   [Security](https://github.com/security)\\\\n*   [Status](https://www.githubstatus.com/)\\\\n*   [Community](https://github.community/)\\\\n*   [Docs](https://docs.github.com/)\\\\n*   [Contact](https://support.github.com/?tags=dotcom-footer)\\\\n*    Manage cookies \\\\n*    Do not share my personal information \\\\n\\\\n You canâ€™t perform that action at this time.\"}], \"response_time\": 0.92, \"request_id\": \"c638eed1-e8f2-49ee-8142-786043008a22\"}', name='internet_search', id='92da397c-e083-4863-97ef-25bd2df6f56f', tool_call_id='call_89580e7dd35048f5a3921112'),\n",
              "  AIMessage(content='**LangGraph â€“ A Polished Overview**\\n\\n---\\n\\n### 1. Executive Summary\\nLangGraph is an **openâ€‘source orchestration framework** that lets developers build, deploy, and manage **stateful, longâ€‘running AI agents** as graphâ€‘based workflows. Created by the team behind **LangChain**, it provides lowâ€‘level primitives for durable execution, humanâ€‘inâ€‘theâ€‘loop control, memory, streaming, and debugging. Because it models agent logic as a graph, LangGraph makes complex, multiâ€‘step, and multiâ€‘agent processes transparent, scalable, and easy to reason about.\\n\\n> *â€œLangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows.â€* â€“ IBM Think articleã€1â€ L1-L4ã€‘  \\n\\n---\\n\\n### 2. Core Concepts\\n\\n| Concept | What It Is | Why It Matters |\\n|---------|------------|----------------|\\n| **Nodes** | Individual components or â€œactorsâ€ (e.g., an LLM call, a tool, a data fetcher). | Represent the atomic steps of a workflow; can be reused, swapped, or extended. |\\n| **Edges** | Directed connections that decide **which node runs next** (conditional or fixed). | Enable flexible routing, branching, and loops within the agent. |\\n| **State (Graph Memory)** | A persistent keyâ€‘value store that travels with the workflow, recording past actions, observations, and intermediate results. | Provides â€œmemoryâ€ across steps, supports debugging, and enables reflection (agents reviewing their own outputs). |\\n| **Graph Architecture** | The overall directed graph (often cyclic) that defines the execution topology. | Allows complex control flows (singleâ€‘agent, multiâ€‘agent, hierarchical, sequential) while keeping the system **stateful and durable**. |\\n| **Durable Execution** | Execution that survives crashes, restarts, or backâ€‘pressure; the runtime can resume exactly where it left off. | Guarantees reliability for longâ€‘running tasks (e.g., multiâ€‘day data pipelines). |\\n| **Humanâ€‘inâ€‘theâ€‘Loop (HITL)** | Points in the graph where a human can inspect, approve, or modify the agentâ€™s state. | Adds safety and oversight for highâ€‘stakes decisions. |\\n| **Streaming** | Tokenâ€‘byâ€‘token or stepâ€‘byâ€‘step output that can be shown to users in real time. | Improves UX for interactive agents (chatbots, planning assistants). |\\n| **Memory** | Builtâ€‘in mechanisms for shortâ€‘term working memory and longâ€‘term persistent memory. | Enables contextâ€‘aware agents that remember past interactions across sessions. |\\n\\n> *â€œNodes represent individual components or agents within an AI workflowâ€¦ Edges are a function within Python that determines which node to execute next based on the current state.â€* â€“ LangChain documentationã€2â€ L1-L4ã€‘  \\n\\n---\\n\\n### 3. Key Benefits\\n\\n| Benefit | Description |\\n|---------|-------------|\\n| **Transparency & Observability** | The graph visualizes every step; state can be inspected at any point. |\\n| **Scalability** | Graphâ€‘based design allows horizontal scaling and parallel execution of independent branches. |\\n| **Modularity** | Nodes and edges are interchangeable; developers can plug in new models, tools, or data sources without rewriting the whole workflow. |\\n| **Productionâ€‘Ready** | Integrates with **LangSmith** for tracing, evaluation, and deployment; supports durable execution and checkpointing. |\\n| **Humanâ€‘Centric Control** | Builtâ€‘in HITL checkpoints let humans intervene, approve, or redirect agents. |\\n| **Openâ€‘Source & Free** | MITâ€‘licensed; no runtime overhead; can be used standalone or alongside LangChain. |\\n| **Multiâ€‘Agent Support** | Facilitates creation of networks of specialized agents that collaborate on complex problems. |\\n\\n> *â€œLangGraph provides lowâ€‘level supporting infrastructure for any longâ€‘running, stateful workflow or agentâ€¦ It does not abstract prompts or architecture, and provides the following central benefits: durable execution, humanâ€‘inâ€‘theâ€‘loop, comprehensive memory, debugging with LangSmith, productionâ€‘ready deployment.â€* â€“ LangChain overviewã€2â€ L1-L5ã€‘  \\n\\n---\\n\\n### 4. Typical Use Cases\\n\\n| Use Case | Example |\\n|----------|---------|\\n| **Chatbots & Virtual Assistants** | Build a vacationâ€‘planning bot that remembers user preferences, loops for clarification, and streams responses tokenâ€‘byâ€‘token. |\\n| **Agent Systems** | Deploy autonomous agents for robotics, autonomous vehicles, or game NPCs that can pause for human review. |\\n| **Retrievalâ€‘Augmented Generation (RAG)** | Combine external document retrieval with LLM reasoning inside a graph where retrieval nodes feed into generation nodes. |\\n| **Multiâ€‘Agent Collaboration** | Orchestrate teams of specialized agents (e.g., â€œresearcherâ€, â€œwriterâ€, â€œeditorâ€) that pass artifacts back and forth. |\\n| **Enterprise Workflow Automation** | Automate multiâ€‘step business processes (e.g., invoice approval, onboarding) with persistent state and audit trails. |\\n| **Experimentation & Prototyping** | Quickly prototype new agent topologies in a visual IDE (LangGraph Studio) before moving to production. |\\n\\n> *â€œLangGraph is also built on several key technologies, including LangChainâ€¦ It provides a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agentâ€‘based systems.â€* â€“ IBM Think articleã€1â€ L1-L3ã€‘  \\n\\n---\\n\\n### 5. Relationship to LangChain & the Broader Ecosystem\\n\\n| Component | Role |\\n|-----------|------|\\n| **LangChain** | Highâ€‘level library that supplies readyâ€‘made agents, toolâ€‘wrappers, and prompt templates. LangGraph sits **underneath** LangChain, offering the durable execution and stateâ€‘management backbone. |\\n| **LangSmith** | Observability platform for tracing, evaluating, and monitoring LangGraphâ€‘based agents in production. |\\n| **LangGraph Studio** | Visual, dragâ€‘andâ€‘drop IDE that lets users design graphs without writing code; useful for rapid prototyping and collaboration. |\\n| **Community Integrations** | Projects like **CrewAI**, **AutoGen**, and **MetaGPT** can be built on top of LangGraph, leveraging its graph primitives for multiâ€‘agent orchestration. |\\n| **Deployment Options** | Can be run locally, in a container, or as a managed service via LangSmith; supports scaling on Kubernetes, cloud VMs, or edge devices. |\\n\\n> *â€œLangGraph can be used standalone, but it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents.â€* â€“ LangChain docsã€2â€ L1-L2ã€‘  \\n\\n---\\n\\n### 6. Getting Started (Practical Steps)\\n\\n1. **Install**  \\n   ```bash\\n   pip install -U langgraph\\n   ```\\n2. **Create a Simple Graph** (from LangChain docs)  \\n   ```python\\n   from langgraph.graph import StateGraph, MessagesState, START, END\\n\\n   class State(MessagesState):\\n       pass\\n\\n   def mock_llm(state: State):\\n       return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\n   graph = StateGraph(State)\\n   graph.add_node(mock_llm)\\n   graph.add_edge(START, \"mock_llm\")\\n   graph.add_edge(\"mock_llm\", END)\\n   app = graph.compile()\\n\\n   print(app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]))\\n   ```\\n3. **Add Memory & Persistence** â€“ Use builtâ€‘in checkpointing or integrate with a database (e.g., SQLite, Redis).  \\n4. **Add Humanâ€‘inâ€‘theâ€‘Loop** â€“ Insert `interrupt()` calls to pause and present the current state to a user for approval.  \\n5. **Deploy** â€“ Wrap the compiled graph in a FastAPI/Flask endpoint, or deploy via **LangSmith** for managed scaling and observability.  \\n\\n> *The official LangGraph Quickstart guide provides stepâ€‘byâ€‘step tutorials, example notebooks, and a â€œHelloâ€‘Worldâ€ template.* â€“ LangChain documentationã€2â€ L1-L3ã€‘\\n\\n---\\n\\n### 7. Future Roadmap (as of earlyâ€‘2026)\\n\\n| Planned Enhancement | Expected Impact |\\n|---------------------|-----------------|\\n| **Advanced NLP** â€“ richer language understanding and generation capabilities. | More natural, contextâ€‘aware interactions. |\\n| **Improved ML Integration** â€“ autoâ€‘learning loops that refine agent policies over time. | Continuous performance gains without manual retraining. |\\n| **New Platform Support** â€“ mobile, edge, and IoT runtimes. | Bring agent workflows to constrained environments. |\\n| **Enhanced Debugging & Visualization** â€“ richer stateâ€‘travel and â€œtimeâ€‘machineâ€ debugging. | Faster issue resolution and higher reliability. |\\n| **Standardized Multiâ€‘Agent Protocols** â€“ builtâ€‘in patterns for coordination, negotiation, and conflict resolution. | Simplify creation of largeâ€‘scale multiâ€‘agent systems. |\\n\\n> *â€œFuture developments include enhanced NLP, improved machine learning, and support for new platforms such as mobile and edge computing.â€* â€“ IBM Think articleã€1â€ L1-L5ã€‘\\n\\n---\\n\\n### 8. Key Takeaways\\n\\n* **LangGraph = Graphâ€‘based orchestration for AI agents** â€“ it turns complex, stateful workflows into visual, durable graphs.  \\n* **Built for production** â€“ offers durable execution, humanâ€‘inâ€‘theâ€‘loop checkpoints, persistent memory, and seamless integration with LangSmith for observability.  \\n* **Openâ€‘source & flexible** â€“ MITâ€‘licensed, can be used alone or as the execution engine behind LangChainâ€™s higherâ€‘level agents.  \\n* **Wide adoption** â€“ Companies such as Klarna, Replit, Elastic, Norwegian Cruise Line, and IBMâ€™s own watsonx.ai use LangGraph to power chatbots, multiâ€‘agent systems, and enterprise automation.  \\n* **Getting started is easy** â€“ a few `pip install` commands and a short Python snippet get you a working graph; visual tools (LangGraph Studio) make design accessible to nonâ€‘programmers.  \\n\\n---\\n\\n### 9. References\\n\\n1. **IBM Think â€“ â€œWhat is LangGraph?â€** â€“ Overview of LangGraphâ€™s purpose, architecture, and use cases.  \\n2. **LangChain Documentation â€“ â€œLangGraph overviewâ€** â€“ Core concepts, installation, and productionâ€‘ready features.  \\n3. **Reddit â€“ r/AI_Agents â€œWhy use LangGraph?â€** â€“ Community perspective on graphâ€‘based workflow advantages.  \\n4. **LangChain.com â€“ Product page for LangGraph** â€“ Marketing overview, customer stories, and FAQ.  \\n5. **GitHub â€“ langchain-ai/langgraph** â€“ Source repository, release notes, and contribution activity.  \\n\\n--- \\n\\n*Prepared as a concise, structured briefing for anyone seeking a clear understanding of what LangGraph is, how it works, and why it matters in the modern AIâ€‘agent landscape.*', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2592, 'prompt_tokens': 31532, 'total_tokens': 34124, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 243, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'nvidia/nemotron-3-nano-30b-a3b:free', 'system_fingerprint': None, 'id': 'gen-1771589005-ywGQGvGvLv8XbqFplapK', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c7aee-f0c1-7c50-927d-d827373b12a0-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 31532, 'output_tokens': 2592, 'total_tokens': 34124, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 243}})]}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "eff95675",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**LangGraph â€“ A Polished Overview**\n",
            "\n",
            "---\n",
            "\n",
            "### 1. Executive Summary\n",
            "LangGraph is an **openâ€‘source orchestration framework** that lets developers build, deploy, and manage **stateful, longâ€‘running AI agents** as graphâ€‘based workflows. Created by the team behind **LangChain**, it provides lowâ€‘level primitives for durable execution, humanâ€‘inâ€‘theâ€‘loop control, memory, streaming, and debugging. Because it models agent logic as a graph, LangGraph makes complex, multiâ€‘step, and multiâ€‘agent processes transparent, scalable, and easy to reason about.\n",
            "\n",
            "> *â€œLangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows.â€* â€“ IBM Think articleã€1â€ L1-L4ã€‘  \n",
            "\n",
            "---\n",
            "\n",
            "### 2. Core Concepts\n",
            "\n",
            "| Concept | What It Is | Why It Matters |\n",
            "|---------|------------|----------------|\n",
            "| **Nodes** | Individual components or â€œactorsâ€ (e.g., an LLM call, a tool, a data fetcher). | Represent the atomic steps of a workflow; can be reused, swapped, or extended. |\n",
            "| **Edges** | Directed connections that decide **which node runs next** (conditional or fixed). | Enable flexible routing, branching, and loops within the agent. |\n",
            "| **State (Graph Memory)** | A persistent keyâ€‘value store that travels with the workflow, recording past actions, observations, and intermediate results. | Provides â€œmemoryâ€ across steps, supports debugging, and enables reflection (agents reviewing their own outputs). |\n",
            "| **Graph Architecture** | The overall directed graph (often cyclic) that defines the execution topology. | Allows complex control flows (singleâ€‘agent, multiâ€‘agent, hierarchical, sequential) while keeping the system **stateful and durable**. |\n",
            "| **Durable Execution** | Execution that survives crashes, restarts, or backâ€‘pressure; the runtime can resume exactly where it left off. | Guarantees reliability for longâ€‘running tasks (e.g., multiâ€‘day data pipelines). |\n",
            "| **Humanâ€‘inâ€‘theâ€‘Loop (HITL)** | Points in the graph where a human can inspect, approve, or modify the agentâ€™s state. | Adds safety and oversight for highâ€‘stakes decisions. |\n",
            "| **Streaming** | Tokenâ€‘byâ€‘token or stepâ€‘byâ€‘step output that can be shown to users in real time. | Improves UX for interactive agents (chatbots, planning assistants). |\n",
            "| **Memory** | Builtâ€‘in mechanisms for shortâ€‘term working memory and longâ€‘term persistent memory. | Enables contextâ€‘aware agents that remember past interactions across sessions. |\n",
            "\n",
            "> *â€œNodes represent individual components or agents within an AI workflowâ€¦ Edges are a function within Python that determines which node to execute next based on the current state.â€* â€“ LangChain documentationã€2â€ L1-L4ã€‘  \n",
            "\n",
            "---\n",
            "\n",
            "### 3. Key Benefits\n",
            "\n",
            "| Benefit | Description |\n",
            "|---------|-------------|\n",
            "| **Transparency & Observability** | The graph visualizes every step; state can be inspected at any point. |\n",
            "| **Scalability** | Graphâ€‘based design allows horizontal scaling and parallel execution of independent branches. |\n",
            "| **Modularity** | Nodes and edges are interchangeable; developers can plug in new models, tools, or data sources without rewriting the whole workflow. |\n",
            "| **Productionâ€‘Ready** | Integrates with **LangSmith** for tracing, evaluation, and deployment; supports durable execution and checkpointing. |\n",
            "| **Humanâ€‘Centric Control** | Builtâ€‘in HITL checkpoints let humans intervene, approve, or redirect agents. |\n",
            "| **Openâ€‘Source & Free** | MITâ€‘licensed; no runtime overhead; can be used standalone or alongside LangChain. |\n",
            "| **Multiâ€‘Agent Support** | Facilitates creation of networks of specialized agents that collaborate on complex problems. |\n",
            "\n",
            "> *â€œLangGraph provides lowâ€‘level supporting infrastructure for any longâ€‘running, stateful workflow or agentâ€¦ It does not abstract prompts or architecture, and provides the following central benefits: durable execution, humanâ€‘inâ€‘theâ€‘loop, comprehensive memory, debugging with LangSmith, productionâ€‘ready deployment.â€* â€“ LangChain overviewã€2â€ L1-L5ã€‘  \n",
            "\n",
            "---\n",
            "\n",
            "### 4. Typical Use Cases\n",
            "\n",
            "| Use Case | Example |\n",
            "|----------|---------|\n",
            "| **Chatbots & Virtual Assistants** | Build a vacationâ€‘planning bot that remembers user preferences, loops for clarification, and streams responses tokenâ€‘byâ€‘token. |\n",
            "| **Agent Systems** | Deploy autonomous agents for robotics, autonomous vehicles, or game NPCs that can pause for human review. |\n",
            "| **Retrievalâ€‘Augmented Generation (RAG)** | Combine external document retrieval with LLM reasoning inside a graph where retrieval nodes feed into generation nodes. |\n",
            "| **Multiâ€‘Agent Collaboration** | Orchestrate teams of specialized agents (e.g., â€œresearcherâ€, â€œwriterâ€, â€œeditorâ€) that pass artifacts back and forth. |\n",
            "| **Enterprise Workflow Automation** | Automate multiâ€‘step business processes (e.g., invoice approval, onboarding) with persistent state and audit trails. |\n",
            "| **Experimentation & Prototyping** | Quickly prototype new agent topologies in a visual IDE (LangGraph Studio) before moving to production. |\n",
            "\n",
            "> *â€œLangGraph is also built on several key technologies, including LangChainâ€¦ It provides a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agentâ€‘based systems.â€* â€“ IBM Think articleã€1â€ L1-L3ã€‘  \n",
            "\n",
            "---\n",
            "\n",
            "### 5. Relationship to LangChain & the Broader Ecosystem\n",
            "\n",
            "| Component | Role |\n",
            "|-----------|------|\n",
            "| **LangChain** | Highâ€‘level library that supplies readyâ€‘made agents, toolâ€‘wrappers, and prompt templates. LangGraph sits **underneath** LangChain, offering the durable execution and stateâ€‘management backbone. |\n",
            "| **LangSmith** | Observability platform for tracing, evaluating, and monitoring LangGraphâ€‘based agents in production. |\n",
            "| **LangGraph Studio** | Visual, dragâ€‘andâ€‘drop IDE that lets users design graphs without writing code; useful for rapid prototyping and collaboration. |\n",
            "| **Community Integrations** | Projects like **CrewAI**, **AutoGen**, and **MetaGPT** can be built on top of LangGraph, leveraging its graph primitives for multiâ€‘agent orchestration. |\n",
            "| **Deployment Options** | Can be run locally, in a container, or as a managed service via LangSmith; supports scaling on Kubernetes, cloud VMs, or edge devices. |\n",
            "\n",
            "> *â€œLangGraph can be used standalone, but it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents.â€* â€“ LangChain docsã€2â€ L1-L2ã€‘  \n",
            "\n",
            "---\n",
            "\n",
            "### 6. Getting Started (Practical Steps)\n",
            "\n",
            "1. **Install**  \n",
            "   ```bash\n",
            "   pip install -U langgraph\n",
            "   ```\n",
            "2. **Create a Simple Graph** (from LangChain docs)  \n",
            "   ```python\n",
            "   from langgraph.graph import StateGraph, MessagesState, START, END\n",
            "\n",
            "   class State(MessagesState):\n",
            "       pass\n",
            "\n",
            "   def mock_llm(state: State):\n",
            "       return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n",
            "\n",
            "   graph = StateGraph(State)\n",
            "   graph.add_node(mock_llm)\n",
            "   graph.add_edge(START, \"mock_llm\")\n",
            "   graph.add_edge(\"mock_llm\", END)\n",
            "   app = graph.compile()\n",
            "\n",
            "   print(app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]))\n",
            "   ```\n",
            "3. **Add Memory & Persistence** â€“ Use builtâ€‘in checkpointing or integrate with a database (e.g., SQLite, Redis).  \n",
            "4. **Add Humanâ€‘inâ€‘theâ€‘Loop** â€“ Insert `interrupt()` calls to pause and present the current state to a user for approval.  \n",
            "5. **Deploy** â€“ Wrap the compiled graph in a FastAPI/Flask endpoint, or deploy via **LangSmith** for managed scaling and observability.  \n",
            "\n",
            "> *The official LangGraph Quickstart guide provides stepâ€‘byâ€‘step tutorials, example notebooks, and a â€œHelloâ€‘Worldâ€ template.* â€“ LangChain documentationã€2â€ L1-L3ã€‘\n",
            "\n",
            "---\n",
            "\n",
            "### 7. Future Roadmap (as of earlyâ€‘2026)\n",
            "\n",
            "| Planned Enhancement | Expected Impact |\n",
            "|---------------------|-----------------|\n",
            "| **Advanced NLP** â€“ richer language understanding and generation capabilities. | More natural, contextâ€‘aware interactions. |\n",
            "| **Improved ML Integration** â€“ autoâ€‘learning loops that refine agent policies over time. | Continuous performance gains without manual retraining. |\n",
            "| **New Platform Support** â€“ mobile, edge, and IoT runtimes. | Bring agent workflows to constrained environments. |\n",
            "| **Enhanced Debugging & Visualization** â€“ richer stateâ€‘travel and â€œtimeâ€‘machineâ€ debugging. | Faster issue resolution and higher reliability. |\n",
            "| **Standardized Multiâ€‘Agent Protocols** â€“ builtâ€‘in patterns for coordination, negotiation, and conflict resolution. | Simplify creation of largeâ€‘scale multiâ€‘agent systems. |\n",
            "\n",
            "> *â€œFuture developments include enhanced NLP, improved machine learning, and support for new platforms such as mobile and edge computing.â€* â€“ IBM Think articleã€1â€ L1-L5ã€‘\n",
            "\n",
            "---\n",
            "\n",
            "### 8. Key Takeaways\n",
            "\n",
            "* **LangGraph = Graphâ€‘based orchestration for AI agents** â€“ it turns complex, stateful workflows into visual, durable graphs.  \n",
            "* **Built for production** â€“ offers durable execution, humanâ€‘inâ€‘theâ€‘loop checkpoints, persistent memory, and seamless integration with LangSmith for observability.  \n",
            "* **Openâ€‘source & flexible** â€“ MITâ€‘licensed, can be used alone or as the execution engine behind LangChainâ€™s higherâ€‘level agents.  \n",
            "* **Wide adoption** â€“ Companies such as Klarna, Replit, Elastic, Norwegian Cruise Line, and IBMâ€™s own watsonx.ai use LangGraph to power chatbots, multiâ€‘agent systems, and enterprise automation.  \n",
            "* **Getting started is easy** â€“ a few `pip install` commands and a short Python snippet get you a working graph; visual tools (LangGraph Studio) make design accessible to nonâ€‘programmers.  \n",
            "\n",
            "---\n",
            "\n",
            "### 9. References\n",
            "\n",
            "1. **IBM Think â€“ â€œWhat is LangGraph?â€** â€“ Overview of LangGraphâ€™s purpose, architecture, and use cases.  \n",
            "2. **LangChain Documentation â€“ â€œLangGraph overviewâ€** â€“ Core concepts, installation, and productionâ€‘ready features.  \n",
            "3. **Reddit â€“ r/AI_Agents â€œWhy use LangGraph?â€** â€“ Community perspective on graphâ€‘based workflow advantages.  \n",
            "4. **LangChain.com â€“ Product page for LangGraph** â€“ Marketing overview, customer stories, and FAQ.  \n",
            "5. **GitHub â€“ langchain-ai/langgraph** â€“ Source repository, release notes, and contribution activity.  \n",
            "\n",
            "--- \n",
            "\n",
            "*Prepared as a concise, structured briefing for anyone seeking a clear understanding of what LangGraph is, how it works, and why it matters in the modern AIâ€‘agent landscape.*\n"
          ]
        }
      ],
      "source": [
        "# Print the agent's response\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c970914",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "- Three key methods for models: invoke, stream, and batch.\n",
        "- LLMs can be configured to responsd in a structured format\n",
        "- Agent = Model + Tools\n",
        "- Models (LLMs) are the brain-power of agents\n",
        "- Tools are simply names and agruments of defined Python functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8837b352",
      "metadata": {},
      "source": [
        "## Activity\n",
        "\n",
        "**Over to you:** create an Agent that is able to answer questions, with an added internet search capability."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
